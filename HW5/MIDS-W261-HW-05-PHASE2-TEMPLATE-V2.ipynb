{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MIDS - w261 Machine Learning At Scale\n",
    "__Course Lead:__ Dr James G. Shanahan (__email__ Jimi via  James.Shanahan _AT_ gmail.com)\n",
    "\n",
    "## Assignment - HW5 Phase 2\n",
    "\n",
    "\n",
    "---\n",
    "__Name:__  Leslie Teo, Stan..., Sam Heung\n",
    "__Class:__ MIDS w261 Winter 2018, Section 1     \n",
    "__Email:__  lteo@iSchool.Berkeley.edu; lteo01@berkeley.edu     \n",
    "__StudentId__  303218617    __End of StudentId__   \n",
    "__Week:__   5.5\n",
    "\n",
    "__NOTE:__ please replace `1234567` with your student id above      \n",
    "__Due Time:__ HW is due the Thursday of the following week by 8AM (West coast time).\n",
    "\n",
    "* __HW5 Phase 1__ \n",
    "This can be done on a local machine (with a unit test on the cloud such as Altiscale's PaaS or on AWS) and is due Thursday, Week 6 by 8AM (West coast time). It will primarily focus on building a unit/systems and for pairwise similarity calculations pipeline (for stripe documents)\n",
    "\n",
    "* __HW5 Phase 2__ \n",
    "This will require the Altiscale cluster and will be due Thursday of the following week by 8AM (West coast time). \n",
    "The focus of  HW5 Phase 2  will be to scale up the unit/systems tests to the Google 5 gram corpus. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Datasets\n",
    "\n",
    "For Phase 2 you will first use the small datasets from phase 1 to systems test your code in the cloud. Then you will test your code on 1 file and then 20 files before running the full (191 file) Google n-gram dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Small data for systems tests__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting atlas-boon-systems-test.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile atlas-boon-systems-test.txt\n",
    "atlas boon\t50\t50\t50\n",
    "boon cava dipped\t10\t10\t10\n",
    "atlas dipped\t15\t15\t15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting googlebooks-eng-all-5gram-20090715-0-filtered-first-10-lines.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile googlebooks-eng-all-5gram-20090715-0-filtered-first-10-lines.txt\n",
    "A BILL FOR ESTABLISHING RELIGIOUS\t59\t59\t54\n",
    "A Biography of General George\t92\t90\t74\n",
    "A Case Study in Government\t102\t102\t78\n",
    "A Case Study of Female\t447\t447\t327\n",
    "A Case Study of Limited\t55\t55\t43\n",
    "A Child's Christmas in Wales\t1099\t1061\t866\n",
    "A Circumstantial Narrative of the\t62\t62\t50\n",
    "A City by the Sea\t62\t60\t49\n",
    "A Collection of Fairy Tales\t123\t117\t80\n",
    "A Collection of Forms of\t116\t103\t82"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SETUP: __Paths to Main data in HDFS on Altiscale AND OTHER SETTINGS__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_1 = \"/user/winegarj/data/1_test\"\n",
    "TEST_20 = \"/user/winegarj/data/20_test\"\n",
    "FULL_DATA = \"/user/winegarj/data/full\" \n",
    "import os\n",
    "USER = !whoami\n",
    "USER = USER[0]\n",
    "OUTPUT_PATH_BASE = '/user/{USER}'.format(USER=USER)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set - Up for Phase 2\n",
    "Before you can run your simlarity analysis on the full Google n-gram dataset you should confirm that the code your wrote in Phase 1 works on the cloud. In the space below, copy the code for your three jobs from Phase 1 (`buildStripes.py`, `invertedIndex.py`, `similarity.py`) and rerun your  atlas-boon systems tests on Altiscale (i.e. ** the cloud**). NOTE: _you may end up modifying this code when you get to 5.7, that's fine._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `buildStripes.py` Note: changed to `buildStripes_v2.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting buildStripes_v2.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile buildStripes_v2.py\n",
    "#!~/opt/anaconda2/bin/python\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "from __future__ import division\n",
    "import re\n",
    "import mrjob\n",
    "import json\n",
    "from mrjob.protocol import RawProtocol\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "import itertools\n",
    "\n",
    "class MRbuildStripes(MRJob):\n",
    "    SORT_VALUES = True\n",
    "    \n",
    "    def mapper(self, _, line):\n",
    "        fields = line.lower().strip(\"\\n\").split(\"\\t\")\n",
    "        words = fields[0].split(\" \")\n",
    "        occurrence_count = int(fields[1])\n",
    "        for subset in itertools.combinations(sorted(set(words)), 2):\n",
    "            yield subset[0], (subset[1], occurrence_count)\n",
    "            yield subset[1], (subset[0], occurrence_count)\n",
    "    \n",
    "    def reducer(self, word, occurrence_counts):\n",
    "        stripe = {}\n",
    "        for other_word, occurrence_count in occurrence_counts:\n",
    "            stripe[other_word] = stripe.get(other_word,0)+occurrence_count\n",
    "        yield word, stripe\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    MRbuildStripes.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `invertedIndex.py` Note: changed to `invertedIndex_v2.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting invertedIndex_v2.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile invertedIndex_v2.py\n",
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "from __future__ import division\n",
    "import collections\n",
    "import sys\n",
    "import re\n",
    "import json\n",
    "import math\n",
    "import numpy as np\n",
    "import itertools\n",
    "import mrjob\n",
    "from mrjob.protocol import RawProtocol\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "\n",
    "class MRinvertedIndex(MRJob):\n",
    "    \n",
    "    #START SUDENT CODE531_INV_INDEX\n",
    "    \n",
    "    SORT_VALUES = True\n",
    "    \n",
    "    def steps(self):\n",
    "        \n",
    "        JOBCONF_STEP = {\n",
    "                'mapreduce.job.output.key.comparator.class': 'org.apache.hadoop.mapred.lib.KeyFieldBasedComparator',\n",
    "                'mapreduce.partition.keycomparator.options': '-k1'\n",
    "                }  \n",
    "        return [\n",
    "        MRStep(jobconf=JOBCONF_STEP, \n",
    "               mapper=self.mapper,\n",
    "               reducer=self.reducer)\n",
    "        ]\n",
    "    \n",
    "    def mapper(self, _, line):\n",
    "        sys.stderr.write(\"reporter:counter:Mapper Counters,Calls,1\\n\")\n",
    "        \n",
    "        tokens = line.strip().split('\\t')\n",
    "        value_dict = json.loads(tokens[1])\n",
    "        term_len = len(value_dict)\n",
    "        \n",
    "        for key in value_dict.keys():\n",
    "            yield key, [tokens[0], term_len]\n",
    "\n",
    "            \n",
    "    def reducer(self, key, values):\n",
    "        sys.stderr.write(\"reporter:counter:Reducer Counters,Calls,1\\n\")\n",
    "        out = []\n",
    "        for value_dict in values:\n",
    "            value_dict[0] = value_dict[0].replace('\"','')\n",
    "            out.append(value_dict)\n",
    "        \n",
    "        yield key, out\n",
    "\n",
    "\n",
    "  #END SUDENT CODE531_INV_INDEX\n",
    "        \n",
    "if __name__ == '__main__':\n",
    "    MRinvertedIndex.run() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `similarity.py` Note: changed to `similarity_v2.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting similarity_v2.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile similarity_v2.py\n",
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "from __future__ import division\n",
    "import sys\n",
    "import collections\n",
    "import re\n",
    "import json\n",
    "import math\n",
    "import numpy as np\n",
    "import itertools\n",
    "import mrjob\n",
    "from mrjob.protocol import RawProtocol\n",
    "from mrjob.protocol import JSONProtocol\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "\n",
    "class MRsimilarity(MRJob):\n",
    "    SORT_VALUES = True\n",
    "    OUTPUT_PROTOCOL = RawProtocol\n",
    "    \n",
    "    def steps(self):\n",
    "        \n",
    "        JOBCONF_STEP_2 = {\n",
    "                'mapreduce.job.output.key.comparator.class': 'org.apache.hadoop.mapred.lib.KeyFieldBasedComparator',\n",
    "                'mapreduce.partition.keycomparator.options': '-k1,1nr',\n",
    "                \"mapreduce.job.reduces\": \"1\",\n",
    "                \"SORT_VALUES\":True\n",
    "                }  \n",
    "        JOBCONF_STEP_1 = {\n",
    "                \"mapreduce.job.reduces\": \"64\",\n",
    "                \"mapreduce.job.maps\": \"64\",\n",
    "                } \n",
    "        return [\n",
    "        MRStep(jobconf=JOBCONF_STEP_1,\n",
    "               mapper=self.mapper_pair_sim,\n",
    "               reducer=self.reducer_pair_sim\n",
    "            ),\n",
    "        MRStep(jobconf=JOBCONF_STEP_2, \n",
    "               reducer=self.reducer_pair_sim2\n",
    "            )\n",
    "        ]\n",
    "\n",
    "    def mapper_pair_sim(self, _, line):     \n",
    "        sys.stderr.write(\"reporter:counter:Mapper Counters,Calls,1\\n\")\n",
    "        line = line.strip()\n",
    "        index, posting = line.split('\\t')\n",
    "        posting = json.loads(posting)\n",
    "        posting = dict(posting)\n",
    "        for docs in itertools.combinations(sorted(posting.keys()), 2):\n",
    "            yield (docs, posting[docs[0]], posting[docs[1]]), 1\n",
    "        \n",
    "\n",
    "    def reducer_pair_sim(self, key, values):\n",
    "        sys.stderr.write(\"reporter:counter:Reducer Counters,Calls,1\\n\")\n",
    "        total = sum(values)\n",
    "        cosine = total/(np.sqrt(key[1])*np.sqrt(key[2]))\n",
    "        jacard = total/(key[1]+key[2]-total)\n",
    "        overlap = total/min(key[1],key[2])\n",
    "        dice = 2*total/(key[1]+key[2])\n",
    "        yield np.mean([cosine, jacard, overlap, dice]), (key[0][0]+' - '+key[0][1],cosine,jacard,overlap,dice)\n",
    "\n",
    "    def reducer_pair_sim2(self, key, values):\n",
    "        sys.stderr.write(\"reporter:counter:Intermediate Reducer Counters,Calls,1\\n\")\n",
    "        for value in values:\n",
    "            yield str(key), json.dumps(value)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    MRsimilarity.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### atlas-boon systems test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python buildStripes_v2.py \\\n",
    "    -r local atlas-boon-systems-test.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run in Hadoop\n",
    "\n",
    "OUTPUT_PATH = os.path.join(OUTPUT_PATH_BASE,'tests')\n",
    "!hadoop fs -rm -r {OUTPUT_PATH}\n",
    "\n",
    "!python buildStripes_v2.py \\\n",
    "        -r hadoop atlas-boon-systems-test.txt \\\n",
    "        --output-dir={OUTPUT_PATH} \\\n",
    "        --no-output\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!hadoop fs -cat {OUTPUT_PATH}/*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save into file for processing\n",
    "!hadoop fs -cat {OUTPUT_PATH}/* > test_stripes_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing inverted index\n",
    "!python invertedIndex_v2.py \\\n",
    "    -r local test_stripes_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run in Hadoop\n",
    "\n",
    "OUTPUT_PATH = os.path.join(OUTPUT_PATH_BASE,'tests')\n",
    "!hadoop fs -rm -r {OUTPUT_PATH}\n",
    "\n",
    "!python invertedIndex_v2.py \\\n",
    "        -r hadoop test_stripes_1 \\\n",
    "        --output-dir={OUTPUT_PATH} \\\n",
    "        --no-output\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save into file for processing\n",
    "!hadoop fs -cat {OUTPUT_PATH}/* > test_index_1\n",
    "!cat test_index_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################################################\n",
    "# Pretty print systems tests for generating Inverted Index\n",
    "##########################################################\n",
    "\n",
    "import json\n",
    "\n",
    "for i in range(1,2):\n",
    "    print \"—\"*100\n",
    "    print \"Systems test \",i,\" - Inverted Index\"\n",
    "    print \"—\"*100  \n",
    "    with open(\"test_index_\"+str(i),\"r\") as f:\n",
    "        lines = f.readlines()\n",
    "        for line in lines:\n",
    "            line = line.strip()\n",
    "            word,stripe = line.split(\"\\t\")\n",
    "            stripe = json.loads(stripe)\n",
    "            stripe.extend([[\"\",\"\"] for _ in xrange(3 - len(stripe))])\n",
    "\n",
    "            print \"{0:>16} |{1:>16} |{2:>16} |{3:>16}\".format((word), \n",
    "                stripe[0][0]+\" \"+str(stripe[0][1]), stripe[1][0]+\" \"+str(stripe[1][1]), stripe[2][0]+\" \"+str(stripe[2][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing similarity metrics\n",
    "!python similarity_v2.py \\\n",
    "    -r local test_index_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run in Hadoop\n",
    "\n",
    "OUTPUT_PATH = os.path.join(OUTPUT_PATH_BASE,'tests')\n",
    "!hadoop fs -rm -r {OUTPUT_PATH}\n",
    "\n",
    "!python similarity_v2.py \\\n",
    "        -r hadoop test_index_1 \\\n",
    "        --output-dir={OUTPUT_PATH} \\\n",
    "        --no-output\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Save into file for processing\n",
    "!hadoop fs -cat {OUTPUT_PATH}/* > test_similarities_1\n",
    "!cat test_similarities_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################\n",
    "# Pretty print systems tests\n",
    "# Note: adjust print formatting if you need to\n",
    "############################################\n",
    "\n",
    "import json\n",
    "for i in range(1,2):\n",
    "    print '—'*110\n",
    "    print \"Systems test \",i,\" - Similarity measures\"\n",
    "    print '—'*110\n",
    "    print \"{0:>15} |{1:>15} |{2:>15} |{3:>15} |{4:>15} |{5:>15}\".format(\n",
    "    \"average\", \"pair\", \"cosine\", \"jaccard\", \"overlap\", \"dice\")\n",
    "    print '-'*110\n",
    "\n",
    "    with open(\"test_similarities_\"+str(i),\"r\") as f:\n",
    "        lines = f.readlines()\n",
    "        for line in lines:\n",
    "            line = line.strip()\n",
    "            avg,stripe = line.split(\"\\t\")\n",
    "            stripe = json.loads(stripe)\n",
    "            \n",
    "            print \"{0:>15f} |{1:>15} |{2:>15f} |{3:>15f} |{4:>15f} |{5:>15f}\".format(float(avg),\n",
    "                stripe[0], float(stripe[1]), float(stripe[2]), float(stripe[3]), float(stripe[4]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "#### 10-line systems test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build Stripes\n",
    "\n",
    "OUTPUT_PATH = os.path.join(OUTPUT_PATH_BASE,'tests')\n",
    "!hadoop fs -rm -r {OUTPUT_PATH}\n",
    "\n",
    "!time python buildStripes_v2.py \\\n",
    "        -r hadoop googlebooks-eng-all-5gram-20090715-0-filtered-first-10-lines.txt \\\n",
    "        --output-dir={OUTPUT_PATH} \\\n",
    "        --no-output\n",
    "\n",
    "!hadoop fs -cat {OUTPUT_PATH}/* > test_stripes_2\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat test_stripes_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build Inverted Index\n",
    "\n",
    "OUTPUT_PATH = os.path.join(OUTPUT_PATH_BASE,'tests')\n",
    "!hadoop fs -rm -r {OUTPUT_PATH}\n",
    "\n",
    "!time python invertedIndex_v2.py \\\n",
    "        -r hadoop test_stripes_2 \\\n",
    "        --output-dir={OUTPUT_PATH} \\\n",
    "        --no-output\n",
    "        \n",
    "!hadoop fs -cat {OUTPUT_PATH}/* > test_index_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate Similarity\n",
    "\n",
    "OUTPUT_PATH = os.path.join(OUTPUT_PATH_BASE,'tests')\n",
    "!hadoop fs -rm -r {OUTPUT_PATH}\n",
    "\n",
    "!time python similarity_v2.py \\\n",
    "        -r hadoop test_index_2 \\\n",
    "        --output-dir={OUTPUT_PATH} \\\n",
    "        --no-output\n",
    "        \n",
    "!hadoop fs -cat {OUTPUT_PATH}/* > test_similarities_2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "############################################\n",
    "# Pretty print systems tests\n",
    "# Note: adjust print formatting if you need to\n",
    "############################################\n",
    "\n",
    "import json\n",
    "for i in range(1,3):\n",
    "    print '—'*110\n",
    "    print \"Systems test \",i,\" - Similarity measures\"\n",
    "    print '—'*110\n",
    "    print \"{0:>15} |{1:>15} |{2:>15} |{3:>15} |{4:>15} |{5:>15}\".format(\n",
    "    \"average\", \"pair\", \"cosine\", \"jaccard\", \"overlap\", \"dice\")\n",
    "    print '-'*110\n",
    "\n",
    "    with open(\"test_similarities_\"+str(i),\"r\") as f:\n",
    "        lines = f.readlines()\n",
    "        for line in lines:\n",
    "            line = line.strip()\n",
    "            avg,stripe = line.split(\"\\t\")\n",
    "            stripe = json.loads(stripe)\n",
    "            \n",
    "            print \"{0:>15f} |{1:>15} |{2:>15f} |{3:>15f} |{4:>15f} |{5:>15f}\".format(float(avg),\n",
    "                stripe[0], float(stripe[1]), float(stripe[2]), float(stripe[3]), float(stripe[4]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW5.6 -Google n-grams EDA\n",
    "\n",
    "Do some EDA on this dataset using mrjob, e.g., \n",
    "\n",
    "- A. Longest 5-gram (number of characters)\n",
    "- B. Top 10 most frequent words (please use the count information), i.e., unigrams\n",
    "- C. 20 Most/Least densely appearing words (count/pages_count) sorted in decreasing order of relative frequency \n",
    "- D. Distribution of 5-gram sizes (character length).  E.g., count (using the count field) up how many times a 5-gram of 50 characters shows up. Plot the data graphically using a histogram."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HW5.6.1 - A. Longest 5-gram (number of characters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile longest5gram.py\n",
    "#!/opt/anaconda2/bin/python\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import re\n",
    "from datetime import datetime\n",
    "import sys\n",
    "\n",
    "import mrjob\n",
    "from mrjob.protocol import RawProtocol\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "\n",
    "class longest5gram(MRJob):\n",
    "#     SORT_VALUES = True\n",
    "    def mapper(self, _, line):\n",
    "        fields = line.strip(\"\\n\").split(\"\\t\")\n",
    "        yield len(fields[0]), fields[0]\n",
    "    \n",
    "    def reducer_init(self):\n",
    "        self.longest_ngrams = []\n",
    "        self.longest_size = 0\n",
    "        \n",
    "    def reducer(self, key, values):\n",
    "        if int(key)> self.longest_size:\n",
    "            self.longest_size = int(key)\n",
    "            self.longest_ngrams = list(values)\n",
    "        elif int(key) == self.longest_size:\n",
    "            self.longest_ngrams = list(self.longest_ngrams)+list(values)\n",
    "            \n",
    "    def reducer_final(self):\n",
    "        yield self.longest_size, \";\".join(list(self.longest_ngrams))\n",
    "    \n",
    "    def reducer_2_init(self):\n",
    "        self.longest_2_ngrams = []\n",
    "        self.longest_2_size = 0\n",
    "    \n",
    "    def reducer_2(self, key, values):\n",
    "        if int(key)> self.longest_2_size:\n",
    "            self.longest_2_size = int(key)\n",
    "            self.longest_2_ngrams = list(values)\n",
    "        elif int(key) == self.longest_2_size:\n",
    "            self.longest_2_ngrams = list(self.longest_2_ngrams)+list(values)\n",
    "            \n",
    "    def reducer_2_final(self):\n",
    "        yield self.longest_2_size, \";\".join(list(self.longest_2_ngrams))\n",
    "        \n",
    "    def steps(self):\n",
    "        return [\n",
    "            MRStep(\n",
    "                mapper = self.mapper,\n",
    "                reducer_init = self.reducer_init,\n",
    "                reducer_final = self.reducer_final,\n",
    "                reducer = self.reducer,\n",
    "                jobconf={\n",
    "                    \"mapreduce.job.reduces\": \"32\",\n",
    "                    \"stream.num.map.output.key.fields\": 1,\n",
    "                    \"mapreduce.job.output.key.comparator.class\" : \"org.apache.hadoop.mapred.lib.KeyFieldBasedComparator\",\n",
    "                    \"mapreduce.partition.keycomparator.options\":\"-k1,1nr\",\n",
    "                }\n",
    "            ),\n",
    "            MRStep(\n",
    "                reducer_init = self.reducer_2_init,\n",
    "                reducer_final = self.reducer_2_final,\n",
    "                reducer = self.reducer_2,\n",
    "                jobconf={\n",
    "                    \"mapreduce.job.reduces\": \"1\"\n",
    "                }\n",
    "            )\n",
    "        ]\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    start_time = datetime.now()\n",
    "    longest5gram.run()\n",
    "    end_time = datetime.now()\n",
    "    elapsed_time = end_time - start_time\n",
    "    sys.stderr.write(str(elapsed_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__On test data set:__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_PATH = os.path.join(OUTPUT_PATH_BASE,'longest_ngram_10lines')\n",
    "!hadoop fs -rm -r {OUTPUT_PATH}\n",
    "!time python longest5gram.py \\\n",
    "        -r hadoop googlebooks-eng-all-5gram-20090715-0-filtered-first-10-lines.txt \\\n",
    "        --output-dir={OUTPUT_PATH} \\\n",
    "        --no-output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!hadoop fs -cat {OUTPUT_PATH}/*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_PATH = os.path.join(OUTPUT_PATH_BASE,'longest_ngram_test1')\n",
    "!hadoop fs -rm -r {OUTPUT_PATH}\n",
    "!time python longest5gram.py \\\n",
    "        -r hadoop hdfs://{TEST_1} \\\n",
    "        --output-dir={OUTPUT_PATH} \\\n",
    "        --no-output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!hdfs dfs -cat {OUTPUT_PATH}/*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__ On the 20 files dataset: __"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_PATH = os.path.join(OUTPUT_PATH_BASE,'longest_ngram_test20')\n",
    "!hadoop fs -rm -r {OUTPUT_PATH}\n",
    "!time python longest5gram.py \\\n",
    "        -r hadoop hdfs://{TEST_20} \\\n",
    "        --output-dir={OUTPUT_PATH} \\\n",
    "        --no-output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!hdfs dfs -cat {OUTPUT_PATH}/*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__On full data set:__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_PATH = os.path.join(OUTPUT_PATH_BASE,'longest_full')\n",
    "!hadoop fs -rm -r {OUTPUT_PATH}\n",
    "!time python longest5gram.py \\\n",
    "        -r hadoop hdfs://{FULL_DATA} \\\n",
    "        --output-dir={OUTPUT_PATH} \\\n",
    "        --no-output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!hadoop fs -cat {OUTPUT_PATH}/*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Longest 5grams MR stats\n",
    "\n",
    "    ec2_instance_type: m3.xlarge\n",
    "    num_ec2_instances: 15\n",
    "\n",
    "__Step 1:__  \n",
    "\n",
    "    RUNNING for 107.0s ~= 2 minutes  \n",
    "    Reduce tasks = 16 \n",
    "    \n",
    "__Step 2:__   \n",
    "\n",
    "    RUNNING for 108.8s ~= 2 minutes\n",
    "    Reduce tasks = 1  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HW5.6.1 - B. Top 10 most frequent words "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile mostFrequentWords.py\n",
    "#!~/anaconda2/bin/python\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import re\n",
    "\n",
    "import mrjob\n",
    "from mrjob.protocol import RawProtocol\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "\n",
    "class mostFrequentWords(MRJob):\n",
    "        \n",
    "    # START STUDENT CODE 5.6.1.B\n",
    "    \n",
    "    SORT_VALUES = True\n",
    "    \n",
    "    def steps(self):\n",
    "        \n",
    "        \n",
    "        JOBCONF_STEP1 = {'mapreduce.job.reduces': '10',\n",
    "                        }\n",
    "        \n",
    "        JOBCONF_STEP2 = {\n",
    "                'mapreduce.job.output.key.comparator.class': 'org.apache.hadoop.mapred.lib.KeyFieldBasedComparator',\n",
    "                'stream.num.map.output.key.fields':'2',\n",
    "                'stream.map.output.field.separator':'\\t',    \n",
    "                'mapreduce.partition.keycomparator.options': '-k1,1nr',\n",
    "                'mapreduce.job.reduces': '1', \n",
    "                }  \n",
    "        return [\n",
    "        MRStep(#jobconf=JOBCONF_STEP1, \n",
    "               mapper=self.mapper,\n",
    "               combiner=self.combiner,\n",
    "               reducer=self.reducer,\n",
    "              ),\n",
    "        MRStep(jobconf=JOBCONF_STEP2,\n",
    "              mapper=self.mapper2,\n",
    "              ),\n",
    "        ]\n",
    "\n",
    "    def mapper(self, _, line):\n",
    "        words = re.findall(r'[a-z\\']+', line.lower())\n",
    "        for word in words:\n",
    "            yield word, 1\n",
    "                         \n",
    "    def combiner(self, word, counts):\n",
    "        yield word, sum(counts)\n",
    "            \n",
    "    def reducer(self, word, counts):\n",
    "        yield word, sum(counts)\n",
    "        \n",
    "    def mapper2(self, word, counts):\n",
    "        yield counts, word\n",
    "    \n",
    "    # END STUDENT CODE 5.6.1.B\n",
    "        \n",
    "        \n",
    "if __name__ == '__main__':\n",
    "    mostFrequentWords.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__On the test data set:__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!python mostFrequentWords.py \\\n",
    "    -r local googlebooks-eng-all-5gram-20090715-0-filtered-first-10-lines.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find top 10 most frequent words\n",
    "OUTPUT_PATH = os.path.join(OUTPUT_PATH_BASE,'tests')\n",
    "!hadoop fs -rm -r {OUTPUT_PATH}\n",
    "\n",
    "!python mostFrequentWords.py \\\n",
    "        -r hadoop hdfs://{TEST_1}/* \\\n",
    "        --output-dir={OUTPUT_PATH} \\\n",
    "        --no-output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!hadoop fs -cat {OUTPUT_PATH}/* | head -n 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__ On the 20 files dataset: __"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find top 10 most frequent word\n",
    "\n",
    "OUTPUT_PATH = os.path.join(OUTPUT_PATH_BASE,'tests')\n",
    "!hadoop fs -rm -r {OUTPUT_PATH}\n",
    "\n",
    "!python mostFrequentWords.py \\\n",
    "        -r hadoop hdfs://{TEST_20}/* \\\n",
    "        --output-dir={OUTPUT_PATH} \\\n",
    "        --no-output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!hadoop fs -cat {OUTPUT_PATH}/* | head -n 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__On the full data set:__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find top 10 most frequent word\n",
    "\n",
    "OUTPUT_PATH = os.path.join(OUTPUT_PATH_BASE,'tests')\n",
    "!hadoop fs -rm -r {OUTPUT_PATH}\n",
    "\n",
    "!python mostFrequentWords.py \\\n",
    "        -r hadoop hdfs://{FULL_DATA}/* \\\n",
    "        --output-dir={OUTPUT_PATH} \\\n",
    "        --no-output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!hadoop fs -cat {OUTPUT_PATH}/* | head -n 10000 > test.output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat test.output | head -n 100\n",
    "!cat test.output | tail -n 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Version that excludes stop words**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile mostFrequentWords_v2.py\n",
    "#!/opt/anaconda2/bin/python\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import re\n",
    "\n",
    "import mrjob\n",
    "from mrjob.protocol import RawProtocol\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "\n",
    "class mostFrequentWords(MRJob):\n",
    "        \n",
    "    # START STUDENT CODE 5.6.1.B\n",
    "    \n",
    "    SORT_VALUES = True\n",
    "    \n",
    "    def steps(self):\n",
    "        \n",
    "        \n",
    "        JOBCONF_STEP1 = {'mapreduce.job.reduces': '10',\n",
    "                        }\n",
    "        \n",
    "        JOBCONF_STEP2 = {\n",
    "                'mapreduce.job.output.key.comparator.class': 'org.apache.hadoop.mapred.lib.KeyFieldBasedComparator',\n",
    "                'stream.num.map.output.key.fields':2,\n",
    "                'stream.map.output.field.separator':'\\t',    \n",
    "                'mapreduce.partition.keycomparator.options': '-k1,1nr',\n",
    "                'mapreduce.job.reduces': '1', \n",
    "                }  \n",
    "        return [\n",
    "        MRStep(jobconf=JOBCONF_STEP1, \n",
    "               mapper_init=self.mapper_init,\n",
    "               mapper=self.mapper,\n",
    "               combiner=self.combiner,\n",
    "               reducer=self.reducer,\n",
    "              ),\n",
    "        MRStep(jobconf=JOBCONF_STEP2,\n",
    "              mapper=self.mapper2,\n",
    "              ),\n",
    "        ]\n",
    "    \n",
    "    def mapper_init(self):\n",
    "        self.stopwords =  ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', \n",
    "              'ourselves', 'you', 'your', 'yours', 'yourself', \n",
    "              'yourselves', 'he', 'him', 'his', 'himself', 'she', \n",
    "              'her', 'hers', 'herself', 'it', 'its', 'itself', \n",
    "              'they', 'them', 'their', 'theirs', 'themselves', \n",
    "              'what', 'which', 'who', 'whom', 'this', 'that', \n",
    "              'these', 'those', 'am', 'is', 'are', 'was', 'were', \n",
    "              'be', 'been', 'being', 'have', 'has', 'had', 'having', \n",
    "              'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', \n",
    "              'but', 'if', 'or', 'because', 'as', 'until', 'while', \n",
    "              'of', 'at', 'by', 'for', 'with', 'about', 'against', \n",
    "              'between', 'into', 'through', 'during', 'before', \n",
    "              'after', 'above', 'below', 'to', 'from', 'up', 'down', \n",
    "              'in', 'out', 'on', 'off', 'over', 'under', 'again', \n",
    "              'further', 'then', 'once', 'here', 'there', 'when', \n",
    "              'where', 'why', 'how', 'all', 'any', 'both', 'each', \n",
    "              'few', 'more', 'most', 'other', 'some', 'such', 'no', \n",
    "              'nor', 'not', 'only', 'own', 'same', 'so', 'than', \n",
    "              'too', 'very', 's', 't', 'can', 'will', 'just', \n",
    "              'don', 'should', 'now']\n",
    "\n",
    "    def mapper(self, _, line):\n",
    "        words = re.findall(r'[a-z\\']+', line.lower())\n",
    "        for word in words:\n",
    "            if word not in self.stopwords:\n",
    "                yield word, 1\n",
    "                         \n",
    "    def combiner(self, word, counts):\n",
    "        yield word, sum(counts)\n",
    "            \n",
    "    def reducer(self, word, counts):\n",
    "        yield word, sum(counts)\n",
    "        \n",
    "    def mapper2(self, word, counts):\n",
    "        yield counts, word\n",
    "    \n",
    "    # END STUDENT CODE 5.6.1.B\n",
    "        \n",
    "        \n",
    "if __name__ == '__main__':\n",
    "    mostFrequentWords.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python mostFrequentWords_v2.py \\\n",
    "    -r local googlebooks-eng-all-5gram-20090715-0-filtered-first-10-lines.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Find top 10 most frequent words\n",
    "OUTPUT_PATH = os.path.join(OUTPUT_PATH_BASE,'tests')\n",
    "!hadoop fs -rm -r {OUTPUT_PATH}\n",
    "\n",
    "!python mostFrequentWords_v2.py \\\n",
    "        -r hadoop hdfs://{TEST_1}/* \\\n",
    "        --output-dir={OUTPUT_PATH} \\\n",
    "        --no-output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!hadoop fs -cat {OUTPUT_PATH}/* | head -n 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Most frequent words MR stats\n",
    "    \n",
    "    ec2_instance_type: m3.xlarge\n",
    "    num_ec2_instances: 15\n",
    "    \n",
    "__Step 1:__   \n",
    "\n",
    "    RUNNING for 590.7s ~= 10 minutes   \n",
    "    Launched map tasks=191  \n",
    "    Launched reduce tasks=57   \n",
    "\n",
    "__Step 2:__  \n",
    "\n",
    "    RUNNING for 76.6s   \n",
    "    Launched map tasks=110\n",
    "    Launched reduce tasks=16  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HW5.6.1 - C. 20 Most/Least densely appearing words "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile mostLeastDenseWords.py\n",
    "#!/opt/anaconda2/bin/python\n",
    "# -*- coding: utf-8 -*-\n",
    "from __future__ import division\n",
    "import re\n",
    "import numpy as np\n",
    "import mrjob\n",
    "import json\n",
    "import sys\n",
    "from mrjob.protocol import RawProtocol\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "\n",
    "class mostLeastDenseWords(MRJob):\n",
    "    \n",
    "    # START STUDENT CODE 5.6.1.C\n",
    "    OUTPUT_PROTOCOL = RawProtocol\n",
    "    SORT_VALUES = True\n",
    "    total_page_count = 0 \n",
    "    \n",
    "    def steps(self):  \n",
    "        JOBCONF_STEP1 = {'mapreduce.job.reduces': '10'}\n",
    "        \n",
    "        JOBCONF_STEP2 = {\n",
    "                'mapreduce.job.output.key.comparator.class': 'org.apache.hadoop.mapred.lib.KeyFieldBasedComparator',\n",
    "                'stream.num.map.output.key.fields':'2',\n",
    "                'stream.map.output.field.separator':'\\t',    \n",
    "                'mapreduce.partition.keycomparator.options': '-k2,2nr',\n",
    "                'mapreduce.job.reduces': '1', \n",
    "                }  \n",
    "        \n",
    "        return [MRStep(jobconf=JOBCONF_STEP1, \n",
    "                mapper=self.mapper,\n",
    "                reducer=self.reducer\n",
    "                ),\n",
    "                MRStep(jobconf=JOBCONF_STEP2,\n",
    "                reducer=self.reducer_output)\n",
    "               ]\n",
    "\n",
    "    def mapper(self, _, line):\n",
    "        data = line.split(\"\\t\")\n",
    "        words = data[0].lower().split()\n",
    "        count = int(data[1])\n",
    "        page_count = int(data[2])\n",
    "        \n",
    "        for w in words:\n",
    "            yield w, count\n",
    "\n",
    "        yield \"!Total\", page_count\n",
    "            \n",
    "    def reducer(self, key, data):\n",
    "        yield key, sum(data)\n",
    "\n",
    "    def reducer_output(self, key, data):\n",
    "        if key == \"!Total\":\n",
    "            self.total_page_count = sum(data)\n",
    "        else:\n",
    "            yield key, str(sum(data)/self.total_page_count)\n",
    "            \n",
    "    # END STUDENT CODE 5.6.1.C\n",
    "        \n",
    "if __name__ == '__main__':\n",
    "    mostLeastDenseWords.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__On the test data set:__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python mostLeastDenseWords.py \\\n",
    "    -r local googlebooks-eng-all-5gram-20090715-0-filtered-first-10-lines.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Density for 1 file\n",
    "\n",
    "OUTPUT_PATH = os.path.join(OUTPUT_PATH_BASE,'tests')\n",
    "!hadoop fs -rm -r {OUTPUT_PATH}\n",
    "\n",
    "\n",
    "!time python mostLeastDenseWords.py \\\n",
    "        -r hadoop hdfs://{TEST_1}/* \\\n",
    "        --output-dir={OUTPUT_PATH} \\\n",
    "        --no-output\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!hdfs dfs -cat {OUTPUT_PATH}/*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__ On the 20 files dataset: __"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__On the full data set:__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word density MR stats\n",
    "\n",
    "    ec2_instance_type: m3.xlarge\n",
    "    num_ec2_instances: 15\n",
    "    \n",
    "__Step 1:__ \n",
    "\n",
    "    RUNNING for 649.2s  ~= 10 minutes      \n",
    "    Launched map tasks=190   \n",
    "    Launched reduce tasks=57     \n",
    "\n",
    "__Step 2:__  \n",
    "\n",
    "    RUNNING for 74.4s  ~= 1 minute    \n",
    "    Launched map tasks=110   \n",
    "    Launched reduce tasks=20   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HW5.6.1 - D. Distribution of 5-gram sizes (character length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile distribution.py\n",
    "#!/opt/anaconda2/bin/python\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import mrjob\n",
    "from mrjob.protocol import RawProtocol\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "\n",
    "class distribution(MRJob):\n",
    "    \n",
    "    #### TODO: divide the counts by 1000s to make the graph more readable\n",
    "    #### TODO: split the lengths into buckets <10, <25, <50, <75, <100\n",
    "    def mapper(self, _, line):\n",
    "        fields = line.strip(\"\\n\").split(\"\\t\")\n",
    "        yield len(fields[0]), int(fields[1])\n",
    "    \n",
    "    def combiner(self,length, counts):\n",
    "        yield length, sum(counts)\n",
    "        \n",
    "    def reducer(self,length, counts):\n",
    "        yield length, sum(counts)\n",
    "    \n",
    "    def reducer_sort(self, key, values):\n",
    "        yield key, list(values)[0]\n",
    "        \n",
    "    def steps(self):\n",
    "        return [\n",
    "            MRStep(\n",
    "                mapper = self.mapper,\n",
    "                combiner = self.combiner,\n",
    "                reducer = self.reducer,\n",
    "                jobconf = {\n",
    "                    \"mapreduce.job.reduces\": \"4\",\n",
    "                }\n",
    "            ),\n",
    "            MRStep(\n",
    "                reducer = self.reducer_sort,\n",
    "                jobconf = {\n",
    "                    \"SORT_VALUES\":True,\n",
    "                    \"mapreduce.job.reduces\": \"1\",\n",
    "                    \"stream.num.map.output.key.fields\": 1,\n",
    "                    \"mapreduce.job.output.key.comparator.class\" : \"org.apache.hadoop.mapred.lib.KeyFieldBasedComparator\",\n",
    "                    \"mapreduce.partition.keycomparator.options\":\"-k1,1nr\",\n",
    "                }\n",
    "            )\n",
    "        ]\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    distribution.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__On the test data set:__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_PATH = os.path.join(OUTPUT_PATH_BASE,'distributions_10lines')\n",
    "!hadoop fs -rm -r {OUTPUT_PATH}\n",
    "!time python distribution.py \\\n",
    "        -r hadoop googlebooks-eng-all-5gram-20090715-0-filtered-first-10-lines.txt \\\n",
    "        --output-dir={OUTPUT_PATH} \\\n",
    "        --no-output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!hadoop fs -cat {OUTPUT_PATH}/* > distributions_10lines.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_PATH = os.path.join(OUTPUT_PATH_BASE,'distributions_1file')\n",
    "!hadoop fs -rm -r {OUTPUT_PATH}\n",
    "!python distribution.py \\\n",
    "        -r hadoop hdfs://{TEST_1} \\\n",
    "        --output-dir={OUTPUT_PATH} \\\n",
    "        --no-output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!hadoop fs -cat {OUTPUT_PATH}/* > distributions_1file.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__ On the 20 files dataset: __"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_PATH = os.path.join(OUTPUT_PATH_BASE,'distributions_20files')\n",
    "!hadoop fs -rm -r {OUTPUT_PATH}\n",
    "!time python distribution.py \\\n",
    "        -r hadoop hdfs://{TEST_20} \\\n",
    "        --output-dir={OUTPUT_PATH} \\\n",
    "        --no-output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!hadoop fs -cat {OUTPUT_PATH}/* > distributions_20files.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__On the full data set:__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_PATH = os.path.join(OUTPUT_PATH_BASE,'distributions_full')\n",
    "!hadoop fs -rm -r {OUTPUT_PATH}\n",
    "!time python distribution.py \\\n",
    "        -r hadoop hdfs://{FULL_DATA} \\\n",
    "        --output-dir={OUTPUT_PATH} \\\n",
    "        --no-output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!hadoop fs -cat {OUTPUT_PATH}/* > distributions_full.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distribution MRJob stats\n",
    "\n",
    "__Step 1:__ \n",
    "\n",
    "    RUNNING for 157.8s ~= 2.6 minutes  \n",
    "    Launched map tasks=191  \n",
    "    Launched reduce tasks=16   \n",
    "    \n",
    "__Step 2:__  \n",
    "\n",
    "    RUNNING for 115.0s ~= 2 minutes   \n",
    "    Launched map tasks=139\n",
    "\tLaunched reduce tasks=1  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pylab as pl\n",
    "\n",
    "results_A = []\n",
    "for line in open(\"distributions_10lines.txt\").readlines():\n",
    "    line = line.strip()\n",
    "    X,Y = line.split(\"\\t\")\n",
    "    results_A.append([int(X),int(Y)])\n",
    "\n",
    "items = (np.array(results_A)[::-1].T)\n",
    "fig = pl.figure(figsize=(17,7))\n",
    "ax = pl.subplot(111)\n",
    "width=0.8\n",
    "ax.bar(range(len(items[0])), items[1], width=width)\n",
    "ax.set_xticks(np.arange(len(items[0])) + width/2)\n",
    "ax.set_xticklabels(items[0], rotation=90)\n",
    "\n",
    "\n",
    "\n",
    "pl.title(\"Distributions of 5 Gram lengths using 10-line sample\")\n",
    "pl.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pylab as pl\n",
    "\n",
    "results_A = []\n",
    "for line in open(\"distributions_1file.txt\").readlines():\n",
    "    line = line.strip()\n",
    "    X,Y = line.split(\"\\t\")\n",
    "    results_A.append([int(X),int(Y)])\n",
    "\n",
    "items = (np.array(results_A)[::-1].T)\n",
    "fig = pl.figure(figsize=(17,7))\n",
    "ax = pl.subplot(111)\n",
    "width=0.8\n",
    "ax.bar(range(len(items[0])), items[1], width=width)\n",
    "ax.set_xticks(np.arange(len(items[0])) + width/2)\n",
    "ax.set_xticklabels(items[0], rotation=90)\n",
    "\n",
    "\n",
    "\n",
    "pl.title(\"Distributions of 5 Gram lengths using 1 file\")\n",
    "pl.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pylab as pl\n",
    "\n",
    "results_A = []\n",
    "for line in open(\"distributions_20files.txt\").readlines():\n",
    "    line = line.strip()\n",
    "    X,Y = line.split(\"\\t\")\n",
    "    results_A.append([int(X),int(Y)])\n",
    "\n",
    "items = (np.array(results_A)[::-1].T)\n",
    "fig = pl.figure(figsize=(17,7))\n",
    "ax = pl.subplot(111)\n",
    "width=0.8\n",
    "ax.bar(range(len(items[0])), items[1], width=width)\n",
    "ax.set_xticks(np.arange(len(items[0])) + width/2)\n",
    "ax.set_xticklabels(items[0], rotation=90)\n",
    "\n",
    "\n",
    "\n",
    "pl.title(\"Distributions of 5 Gram lengths using 20 files\")\n",
    "pl.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pylab as pl\n",
    "\n",
    "results_A = []\n",
    "for line in open(\"distributions_full.txt\").readlines():\n",
    "    line = line.strip()\n",
    "    X,Y = line.split(\"\\t\")\n",
    "    results_A.append([int(X),int(Y)])\n",
    "\n",
    "items = (np.array(results_A)[::-1].T)\n",
    "fig = pl.figure(figsize=(17,7))\n",
    "ax = pl.subplot(111)\n",
    "width=0.8\n",
    "ax.bar(range(len(items[0])), items[1], width=width)\n",
    "ax.set_xticks(np.arange(len(items[0])) + width/2)\n",
    "ax.set_xticklabels(items[0], rotation=90)\n",
    "\n",
    "\n",
    "\n",
    "pl.title(\"Distributions of 5 Gram lengths using all files\")\n",
    "pl.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HW5.6.2 - OPTIONAL: log-log plots (PHASE 2)\n",
    "\n",
    "Plot the log-log plot of the frequency distributuion of unigrams. Does it follow power law distribution?\n",
    "\n",
    "For more background see:\n",
    "- https://en.wikipedia.org/wiki/Log%E2%80%93log_plot\n",
    "- https://en.wikipedia.org/wiki/Power_law"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW5.7 - Synonym detection over 2Gig of Data with extra Preprocessing steps (HW5.3-5 plus some preprocessing)   \n",
    "\n",
    "For the remainder of this assignment please feel free to eliminate stop words from your analysis (see stopWords in the cell below)\n",
    "\n",
    "__A large subset of the Google n-grams dataset as was described above__\n",
    "\n",
    "For each HW 5.6 - 5.7.1 Please unit test and system test your code with respect \n",
    "to SYSTEMS TEST DATASET and show the results. \n",
    "Please compute the expected answer by hand and show your hand calculations for the \n",
    "SYSTEMS TEST DATASET. Then show the results you get with your system.\n",
    "\n",
    "In this part of the assignment we will focus on developing methods for detecting synonyms, using the Google 5-grams dataset. At a high level:\n",
    "\n",
    "\n",
    "1. remove stopwords\n",
    "2. get 10,000 most frequent\n",
    "3. get 1000 (9001-10000) features\n",
    "3. build stripes\n",
    "\n",
    "To accomplish this you must script two main tasks using MRJob:\n",
    "\n",
    "\n",
    "__TASK (1)__ Build stripes for the most frequent 10,000 words using cooccurence information based on\n",
    "the words ranked from 9001,-10,000 as a basis/vocabulary (drop stopword-like terms),\n",
    "and output to a file in your bucket on s3 (bigram analysis, though the words are non-contiguous).\n",
    "\n",
    "\n",
    "__TASK (2)__ Using two (symmetric) comparison methods of your choice \n",
    "(e.g., correlations, distances, similarities), pairwise compare \n",
    "all stripes (vectors), and output to a file in your bucket on s3.\n",
    "\n",
    "\n",
    "For this task you will have to determine a method of comparison.\n",
    "Here are a few that you might consider:\n",
    "\n",
    "- Jaccard\n",
    "- Cosine similarity\n",
    "- Spearman correlation\n",
    "- Euclidean distance\n",
    "- Taxicab (Manhattan) distance\n",
    "- Shortest path graph distance (a graph, because our data is symmetric!)\n",
    "- Pearson correlation\n",
    "- Kendall correlation\n",
    "\n",
    "However, be cautioned that some comparison methods are more difficult to\n",
    "parallelize than others, and do not perform more associations than is necessary, \n",
    "since your choice of association will be symmetric.\n",
    "\n",
    "Please report the size of the cluster used and the amount of time it takes to run for the index construction task and for the synonym calculation task. How many pairs need to be processed (HINT: use the posting list length to calculate directly)? Report your  Cluster configuration!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords =  ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', \n",
    "              'ourselves', 'you', 'your', 'yours', 'yourself', \n",
    "              'yourselves', 'he', 'him', 'his', 'himself', 'she', \n",
    "              'her', 'hers', 'herself', 'it', 'its', 'itself', \n",
    "              'they', 'them', 'their', 'theirs', 'themselves', \n",
    "              'what', 'which', 'who', 'whom', 'this', 'that', \n",
    "              'these', 'those', 'am', 'is', 'are', 'was', 'were', \n",
    "              'be', 'been', 'being', 'have', 'has', 'had', 'having', \n",
    "              'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', \n",
    "              'but', 'if', 'or', 'because', 'as', 'until', 'while', \n",
    "              'of', 'at', 'by', 'for', 'with', 'about', 'against', \n",
    "              'between', 'into', 'through', 'during', 'before', \n",
    "              'after', 'above', 'below', 'to', 'from', 'up', 'down', \n",
    "              'in', 'out', 'on', 'off', 'over', 'under', 'again', \n",
    "              'further', 'then', 'once', 'here', 'there', 'when', \n",
    "              'where', 'why', 'how', 'all', 'any', 'both', 'each', \n",
    "              'few', 'more', 'most', 'other', 'some', 'such', 'no', \n",
    "              'nor', 'not', 'only', 'own', 'same', 'so', 'than', \n",
    "              'too', 'very', 's', 't', 'can', 'will', 'just', \n",
    "              'don', 'should', 'now']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__STEP 1: Code and Steps for Preprocessing__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove stop words, get 10,000 most frequent words\n",
    "\n",
    "# Find top 10,000 most frequent words\n",
    "OUTPUT_PATH = os.path.join(OUTPUT_PATH_BASE,'ten_thousand_1')\n",
    "!hadoop fs -rm -r {OUTPUT_PATH}\n",
    "\n",
    "!python mostFrequentWords_v2.py \\\n",
    "        -r hadoop hdfs://{TEST_1}/* \\\n",
    "        --output-dir={OUTPUT_PATH} \\\n",
    "        --no-output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!hadoop fs -cat {OUTPUT_PATH}/* | head -n 10000 > ten_thousand_1.dat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find top 10,000 most frequent words\n",
    "OUTPUT_PATH = os.path.join(OUTPUT_PATH_BASE,'ten_thousand_20')\n",
    "!hadoop fs -rm -r {OUTPUT_PATH}\n",
    "\n",
    "!python mostFrequentWords_v2.py \\\n",
    "        -r hadoop hdfs://{TEST_20}/* \\\n",
    "        --output-dir={OUTPUT_PATH} \\\n",
    "        --no-output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!hadoop fs -cat {OUTPUT_PATH}/* | head -n 10000 > ten_thousand_20.dat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat ten_thousand_20.dat | head\n",
    "!cat ten_thousand_20.dat | tail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find top 10,000 most frequent words\n",
    "OUTPUT_PATH = os.path.join(OUTPUT_PATH_BASE,'ten_thousand_FULL')\n",
    "!hadoop fs -rm -r {OUTPUT_PATH}\n",
    "\n",
    "!python mostFrequentWords_v2.py \\\n",
    "        -r hadoop hdfs://{FULL_DATA}/* \\\n",
    "        --output-dir={OUTPUT_PATH} \\\n",
    "        --no-output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!hadoop fs -cat {OUTPUT_PATH}/* | head -n 10000 > ten_thousand_FULL.dat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat ten_thousand_FULL.dat | head\n",
    "!cat ten_thousand_FULL.dat | tail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Also create features\n",
    "!cat ten_thousand_1.dat |tail -n 999 > features_1.dat\n",
    "!cat ten_thousand_20.dat |tail -n 999 > features_20.dat\n",
    "!cat ten_thousand_FULL.dat |tail -n 999 > features_FULL.dat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We now have files with 10K words => `ten_thousand_*.dat` AND\n",
    "features with words from 9,001 to 10,000 in => `features_*.dat` **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Some extra preprocessing to load files faster\n",
    "import json\n",
    "files = ['features_1','features_20', 'features_FULL', 'ten_thousand_1', 'ten_thousand_20','ten_thousand_FULL']\n",
    "for fileName in files:\n",
    "    with open(fileName+'.dat') as f:\n",
    "        words = []\n",
    "        for line in f:\n",
    "            word = line.split(\"\\t\")[1].strip('\"')\n",
    "            words.append(word)\n",
    "    with open(fileName+'.json', 'w') as outfile:\n",
    "        json.dump(words, outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__STEP 2: MODIFY BUILD STRIPES TO ONLY INCLUDE 10K words with 1K as FEATURES__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting buildStripes_stopwords_1.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile buildStripes_stopwords_1.py\n",
    "#!~/opt/anaconda2/bin/python\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "from __future__ import division\n",
    "import re\n",
    "import mrjob\n",
    "import json\n",
    "from mrjob.protocol import RawProtocol\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "import itertools\n",
    "\n",
    "class MRbuildStripes(MRJob):\n",
    "    \n",
    "    SORT_VALUES = True\n",
    "    \n",
    "    def steps(self):\n",
    "        return [\n",
    "            MRStep(\n",
    "                mapper_init=self.mapper_init,\n",
    "                mapper=self.mapper,\n",
    "                reducer=self.reducer,\n",
    "                jobconf = {\n",
    "                    \"mapreduce.job.reduces\": \"64\",\n",
    "                    \"mapreduce.job.maps\": \"64\",\n",
    "#                     \"SORT_VALUES\":True\n",
    "                }\n",
    "            ),\n",
    "            MRStep(\n",
    "                reducer=self.reducer_2,\n",
    "                jobconf = {\n",
    "                    \"mapreduce.job.reduces\": \"1\",\n",
    "                    \"SORT_VALUES\":True\n",
    "                }\n",
    "            )\n",
    "            \n",
    "        ]\n",
    "    \n",
    "    def mapper_init(self):\n",
    "        self.idx = 9001  # To define when feature set starts\n",
    "        self.filename = 'ten_thousand_1.json'\n",
    "        \n",
    "        self.top_words = []\n",
    "        self.features = []\n",
    "        #with open('features_20.json', 'r') as infile:\n",
    "        #    self.features = json.loads(infile.read())\n",
    "        with open(self.filename, 'r') as infile:\n",
    "            self.top_words = json.loads(infile.read())\n",
    "            self.features =self.top_words[self.idx:]\n",
    "\n",
    "    def mapper(self, _, line):\n",
    "        fields = line.lower().strip(\"\\n\").split(\"\\t\")\n",
    "        words = fields[0].split(\" \")\n",
    "        occurrence_count = int(fields[1])\n",
    "        filtered_words = [word.decode('utf-8', 'ignore') for word in words if word.decode('utf-8', 'ignore') in self.top_words]\n",
    "        for subset in itertools.combinations(sorted(set(filtered_words)), 2):\n",
    "            if subset[0] in self.top_words and subset[1] in self.features:\n",
    "                yield subset[0], (subset[1], occurrence_count)\n",
    "            if subset[1] in self.top_words and subset[0] in self.features:\n",
    "                yield subset[1], (subset[0], occurrence_count)\n",
    "    \n",
    "    def reducer(self, word, occurrence_counts):\n",
    "        stripe = {}\n",
    "        for other_word, occurrence_count in occurrence_counts:\n",
    "            stripe[other_word] = stripe.get(other_word,0)+occurrence_count\n",
    "        yield word, stripe\n",
    "    \n",
    "    def reducer_2(self, key, values):\n",
    "        yield str(key), list(values)[0]\n",
    "        \n",
    "if __name__ == '__main__':\n",
    "    MRbuildStripes.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HW5.7.1 Running on 1 file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_1 = \"/user/winegarj/data/1_test\"\n",
    "TEST_20 = \"/user/winegarj/data/20_test\"\n",
    "FULL_DATA = \"/user/winegarj/data/full\" \n",
    "\n",
    "import os\n",
    "USER = !whoami\n",
    "USER = USER[0]\n",
    "OUTPUT_PATH_BASE = '/user/{USER}'.format(USER=USER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18/03/06 15:35:59 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 5760 minutes, Emptier interval = 360 minutes.\n",
      "Moved: 'hdfs://nn-ia.s3s.altiscale.com:8020/user/lteo01/tests' to trash at: hdfs://nn-ia.s3s.altiscale.com:8020/user/lteo01/.Trash/Current\n",
      "Using configs in /home/lteo01/.mrjob.conf\n",
      "Looking for hadoop binary in /opt/hadoop/bin...\n",
      "Found hadoop binary: /opt/hadoop/bin/hadoop\n",
      "Using Hadoop version 2.7.3\n",
      "Looking for Hadoop streaming jar in /opt/hadoop...\n",
      "Found Hadoop streaming jar: /opt/hadoop/share/hadoop/tools/lib/hadoop-streaming-2.7.3.jar\n",
      "Creating temp directory /tmp/buildStripes_stopwords_1.lteo01.20180306.153600.339221\n",
      "Copying local files to hdfs:///user/lteo01/tmp/mrjob/buildStripes_stopwords_1.lteo01.20180306.153600.339221/files/...\n",
      "Running step 1 of 2...\n",
      "  packageJobJar: [] [/opt/hadoop-2.7.3/share/hadoop/tools/lib/hadoop-streaming-2.7.3.jar] /tmp/streamjob3079440485959886461.jar tmpDir=null\n",
      "  Timeline service address: http://rm-ia.s3s.altiscale.com:8188/ws/v1/timeline/\n",
      "  Connecting to ResourceManager at rm-ia.s3s.altiscale.com/10.251.255.108:8032\n",
      "  Connecting to Application History server at rm-ia.s3s.altiscale.com/10.251.255.108:10200\n",
      "  Timeline service address: http://rm-ia.s3s.altiscale.com:8188/ws/v1/timeline/\n",
      "  Connecting to ResourceManager at rm-ia.s3s.altiscale.com/10.251.255.108:8032\n",
      "  Connecting to Application History server at rm-ia.s3s.altiscale.com/10.251.255.108:10200\n",
      "  Loaded native gpl library from the embedded binaries\n",
      "  Successfully loaded & initialized native-lzo library [hadoop-lzo rev d62701d4d05dfa6115bbaf8d9dff002df142e62d]\n",
      "  Total input paths to process : 1\n",
      "  number of splits:64\n",
      "  Submitting tokens for job: job_1509050304403_25846\n",
      "  Submitted application application_1509050304403_25846\n",
      "  The url to track the job: http://rm-ia.s3s.altiscale.com:8088/proxy/application_1509050304403_25846/\n",
      "  Running job: job_1509050304403_25846\n",
      "  Job job_1509050304403_25846 running in uber mode : false\n",
      "   map 0% reduce 0%\n",
      "   map 3% reduce 0%\n",
      "   map 5% reduce 0%\n",
      "   map 8% reduce 0%\n",
      "   map 9% reduce 0%\n",
      "   map 12% reduce 0%\n",
      "   map 17% reduce 0%\n",
      "   map 34% reduce 0%\n",
      "   map 47% reduce 0%\n",
      "   map 57% reduce 0%\n",
      "   map 59% reduce 0%\n",
      "   map 61% reduce 0%\n",
      "   map 68% reduce 0%\n",
      "   map 72% reduce 0%\n",
      "   map 86% reduce 0%\n",
      "   map 97% reduce 0%\n",
      "   map 100% reduce 0%\n",
      "   map 100% reduce 5%\n",
      "   map 100% reduce 8%\n",
      "   map 100% reduce 13%\n",
      "   map 100% reduce 16%\n",
      "   map 100% reduce 20%\n",
      "   map 100% reduce 25%\n",
      "   map 100% reduce 30%\n",
      "   map 100% reduce 31%\n",
      "   map 100% reduce 52%\n",
      "   map 100% reduce 58%\n",
      "   map 100% reduce 67%\n",
      "   map 100% reduce 77%\n",
      "   map 100% reduce 84%\n",
      "   map 100% reduce 88%\n",
      "   map 100% reduce 95%\n",
      "   map 100% reduce 100%\n",
      "  Job job_1509050304403_25846 completed successfully\n",
      "  Output directory: hdfs:///user/lteo01/tmp/mrjob/buildStripes_stopwords_1.lteo01.20180306.153600.339221/step-output/0000\n",
      "Counters: 51\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=16677958\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=177036\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=158013\n",
      "\t\tFILE: Number of bytes written=17521251\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=16688390\n",
      "\t\tHDFS: Number of bytes written=177036\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of read operations=384\n",
      "\t\tHDFS: Number of write operations=128\n",
      "\tJob Counters \n",
      "\t\tKilled map tasks=1\n",
      "\t\tKilled reduce tasks=1\n",
      "\t\tLaunched map tasks=65\n",
      "\t\tLaunched reduce tasks=64\n",
      "\t\tRack-local map tasks=65\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=3357037056\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=912051200\n",
      "\t\tTotal time spent by all map tasks (ms)=2185571\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=6556713\n",
      "\t\tTotal time spent by all reduce tasks (ms)=356270\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=1781350\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=2185571\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=356270\n",
      "\tMap-Reduce Framework\n",
      "\t\tCPU time spent (ms)=638780\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tGC time elapsed (ms)=24044\n",
      "\t\tInput split bytes=10432\n",
      "\t\tMap input records=311614\n",
      "\t\tMap output bytes=244420\n",
      "\t\tMap output materialized bytes=311034\n",
      "\t\tMap output records=8922\n",
      "\t\tMerged Map outputs=4096\n",
      "\t\tPhysical memory (bytes) snapshot=70482464768\n",
      "\t\tReduce input groups=8916\n",
      "\t\tReduce input records=8922\n",
      "\t\tReduce output records=3671\n",
      "\t\tReduce shuffle bytes=311034\n",
      "\t\tShuffled Maps =4096\n",
      "\t\tSpilled Records=17844\n",
      "\t\tTotal committed heap usage (bytes)=86851452928\n",
      "\t\tVirtual memory (bytes) snapshot=511292547072\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "Running step 2 of 2...\n",
      "  packageJobJar: [] [/opt/hadoop-2.7.3/share/hadoop/tools/lib/hadoop-streaming-2.7.3.jar] /tmp/streamjob7596518797717696857.jar tmpDir=null\n",
      "  Timeline service address: http://rm-ia.s3s.altiscale.com:8188/ws/v1/timeline/\n",
      "  Connecting to ResourceManager at rm-ia.s3s.altiscale.com/10.251.255.108:8032\n",
      "  Connecting to Application History server at rm-ia.s3s.altiscale.com/10.251.255.108:10200\n",
      "  Timeline service address: http://rm-ia.s3s.altiscale.com:8188/ws/v1/timeline/\n",
      "  Connecting to ResourceManager at rm-ia.s3s.altiscale.com/10.251.255.108:8032\n",
      "  Connecting to Application History server at rm-ia.s3s.altiscale.com/10.251.255.108:10200\n",
      "  Loaded native gpl library from the embedded binaries\n",
      "  Successfully loaded & initialized native-lzo library [hadoop-lzo rev d62701d4d05dfa6115bbaf8d9dff002df142e62d]\n",
      "  Total input paths to process : 64\n",
      "  number of splits:64\n",
      "  Submitting tokens for job: job_1509050304403_25850\n",
      "  Submitted application application_1509050304403_25850\n",
      "  The url to track the job: http://rm-ia.s3s.altiscale.com:8088/proxy/application_1509050304403_25850/\n",
      "  Running job: job_1509050304403_25850\n",
      "  Job job_1509050304403_25850 running in uber mode : false\n",
      "   map 0% reduce 0%\n",
      "   map 28% reduce 0%\n",
      "   map 31% reduce 0%\n",
      "   map 33% reduce 0%\n",
      "   map 38% reduce 0%\n",
      "   map 42% reduce 0%\n",
      "   map 48% reduce 0%\n",
      "   map 81% reduce 0%\n",
      "   map 98% reduce 0%\n",
      "   map 100% reduce 0%\n",
      "   map 100% reduce 100%\n",
      "  Job job_1509050304403_25850 completed successfully\n",
      "  Output directory: hdfs:///user/lteo01/tests\n",
      "Counters: 50\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=177036\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=177036\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=101669\n",
      "\t\tFILE: Number of bytes written=8856683\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=189388\n",
      "\t\tHDFS: Number of bytes written=177036\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of read operations=195\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tKilled map tasks=1\n",
      "\t\tLaunched map tasks=64\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tRack-local map tasks=64\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=2361400320\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=34398720\n",
      "\t\tTotal time spent by all map tasks (ms)=1537370\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=4612110\n",
      "\t\tTotal time spent by all reduce tasks (ms)=13437\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=67185\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=1537370\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=13437\n",
      "\tMap-Reduce Framework\n",
      "\t\tCPU time spent (ms)=71340\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tGC time elapsed (ms)=14366\n",
      "\t\tInput split bytes=12352\n",
      "\t\tMap input records=3671\n",
      "\t\tMap output bytes=180911\n",
      "\t\tMap output materialized bytes=143945\n",
      "\t\tMap output records=3671\n",
      "\t\tMerged Map outputs=64\n",
      "\t\tPhysical memory (bytes) snapshot=51145428992\n",
      "\t\tReduce input groups=3671\n",
      "\t\tReduce input records=3671\n",
      "\t\tReduce output records=3671\n",
      "\t\tReduce shuffle bytes=143945\n",
      "\t\tShuffled Maps =64\n",
      "\t\tSpilled Records=7342\n",
      "\t\tTotal committed heap usage (bytes)=57250152448\n",
      "\t\tVirtual memory (bytes) snapshot=223516569600\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "Removing HDFS temp directory hdfs:///user/lteo01/tmp/mrjob/buildStripes_stopwords_1.lteo01.20180306.153600.339221...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing temp directory /tmp/buildStripes_stopwords_1.lteo01.20180306.153600.339221...\r\n",
      "\r\n",
      "real\t2m49.371s\r\n",
      "user\t0m50.082s\r\n",
      "sys\t0m2.459s\r\n"
     ]
    }
   ],
   "source": [
    "# Run in Hadoop\n",
    "\n",
    "OUTPUT_PATH = os.path.join(OUTPUT_PATH_BASE,'tests')\n",
    "!hadoop fs -rm -r {OUTPUT_PATH}\n",
    "\n",
    "!time python buildStripes_stopwords_1.py \\\n",
    "        -r hadoop hdfs://{TEST_1}/* \\\n",
    "        --file ten_thousand_1.json  \\\n",
    "        --output-dir={OUTPUT_PATH} \\\n",
    "        --no-output\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####NOT USED, BUT KEEP JUST IN CASE\n",
    "import os\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "\n",
    "def totalOrderSort(myPath, outFileName):\n",
    "    wordsFiles = {}\n",
    "    words = []\n",
    "    for f in listdir(myPath):\n",
    "        if isfile(join(myPath,f)) and \"part\" in f:\n",
    "            with open(join(myPath,f)) as openFile:\n",
    "                word = f.readline().split(\"\\t\")[0]\n",
    "                words.append(word)\n",
    "                wordsFiles[word]= join(mypath,f)\n",
    "    print wordsFiles\n",
    "    print words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Look at data\n",
    "# !hadoop fs -cat {OUTPUT_PATH}/*\n",
    "# Save into file for processing\n",
    "!hadoop fs -cat {OUTPUT_PATH}/* > google_stripes_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18/03/06 15:38:53 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 5760 minutes, Emptier interval = 360 minutes.\n",
      "Moved: 'hdfs://nn-ia.s3s.altiscale.com:8020/user/lteo01/tests' to trash at: hdfs://nn-ia.s3s.altiscale.com:8020/user/lteo01/.Trash/Current\n",
      "Using configs in /home/lteo01/.mrjob.conf\n",
      "Looking for hadoop binary in /opt/hadoop/bin...\n",
      "Found hadoop binary: /opt/hadoop/bin/hadoop\n",
      "Using Hadoop version 2.7.3\n",
      "Looking for Hadoop streaming jar in /opt/hadoop...\n",
      "Found Hadoop streaming jar: /opt/hadoop/share/hadoop/tools/lib/hadoop-streaming-2.7.3.jar\n",
      "Creating temp directory /tmp/invertedIndex_s.lteo01.20180306.153854.119645\n",
      "Copying local files to hdfs:///user/lteo01/tmp/mrjob/invertedIndex_s.lteo01.20180306.153854.119645/files/...\n",
      "Running step 1 of 1...\n",
      "  packageJobJar: [] [/opt/hadoop-2.7.3/share/hadoop/tools/lib/hadoop-streaming-2.7.3.jar] /tmp/streamjob198206904659061463.jar tmpDir=null\n",
      "  Timeline service address: http://rm-ia.s3s.altiscale.com:8188/ws/v1/timeline/\n",
      "  Connecting to ResourceManager at rm-ia.s3s.altiscale.com/10.251.255.108:8032\n",
      "  Connecting to Application History server at rm-ia.s3s.altiscale.com/10.251.255.108:10200\n",
      "  Timeline service address: http://rm-ia.s3s.altiscale.com:8188/ws/v1/timeline/\n",
      "  Connecting to ResourceManager at rm-ia.s3s.altiscale.com/10.251.255.108:8032\n",
      "  Connecting to Application History server at rm-ia.s3s.altiscale.com/10.251.255.108:10200\n",
      "  Loaded native gpl library from the embedded binaries\n",
      "  Successfully loaded & initialized native-lzo library [hadoop-lzo rev d62701d4d05dfa6115bbaf8d9dff002df142e62d]\n",
      "  Total input paths to process : 1\n",
      "  number of splits:2\n",
      "  Submitting tokens for job: job_1509050304403_25852\n",
      "  Submitted application application_1509050304403_25852\n",
      "  The url to track the job: http://rm-ia.s3s.altiscale.com:8088/proxy/application_1509050304403_25852/\n",
      "  Running job: job_1509050304403_25852\n",
      "  Job job_1509050304403_25852 running in uber mode : false\n",
      "   map 0% reduce 0%\n",
      "   map 100% reduce 0%\n",
      "   map 100% reduce 100%\n",
      "  Job job_1509050304403_25852 completed successfully\n",
      "  Output directory: hdfs:///user/lteo01/tests\n",
      "Counters: 51\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=219590\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=141300\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=98491\n",
      "\t\tFILE: Number of bytes written=595714\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=219946\n",
      "\t\tHDFS: Number of bytes written=141300\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of read operations=9\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tRack-local map tasks=2\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=18932736\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=8112640\n",
      "\t\tTotal time spent by all map tasks (ms)=12326\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=36978\n",
      "\t\tTotal time spent by all reduce tasks (ms)=3169\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=15845\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=12326\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=3169\n",
      "\tMap-Reduce Framework\n",
      "\t\tCPU time spent (ms)=4790\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tGC time elapsed (ms)=283\n",
      "\t\tInput split bytes=356\n",
      "\t\tMap input records=3671\n",
      "\t\tMap output bytes=255686\n",
      "\t\tMap output materialized bytes=100618\n",
      "\t\tMap output records=8414\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tPhysical memory (bytes) snapshot=1906708480\n",
      "\t\tReduce input groups=8414\n",
      "\t\tReduce input records=8414\n",
      "\t\tReduce output records=999\n",
      "\t\tReduce shuffle bytes=100618\n",
      "\t\tShuffled Maps =2\n",
      "\t\tSpilled Records=16828\n",
      "\t\tTotal committed heap usage (bytes)=2306342912\n",
      "\t\tVirtual memory (bytes) snapshot=11412086784\n",
      "\tMapper Counters\n",
      "\t\tCalls=3671\n",
      "\tReducer Counters\n",
      "\t\tCalls=999\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "Removing HDFS temp directory hdfs:///user/lteo01/tmp/mrjob/invertedIndex_s.lteo01.20180306.153854.119645...\n",
      "Removing temp directory /tmp/invertedIndex_s.lteo01.20180306.153854.119645...\n",
      "\n",
      "real\t0m45.120s\n",
      "user\t0m35.290s\n",
      "sys\t0m1.884s\n"
     ]
    }
   ],
   "source": [
    "# Run in Hadoop\n",
    "\n",
    "OUTPUT_PATH = os.path.join(OUTPUT_PATH_BASE,'tests')\n",
    "!hadoop fs -rm -r {OUTPUT_PATH}\n",
    "\n",
    "!time python invertedIndex_v2.py \\\n",
    "        -r hadoop google_stripes_1 \\\n",
    "        --output-dir={OUTPUT_PATH} \\\n",
    "        --no-output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"ab\"\t[[\"cannot\", 6], [\"cd\", 1], [\"chord\", 2], [\"equal\", 4], [\"lines\", 2], [\"piece\", 3], [\"quantity\", 3], [\"university\", 12]]\r\n",
      "\"abnormalities\"\t[[\"associated\", 3], [\"b\", 3], [\"found\", 17], [\"indicate\", 2], [\"may\", 55], [\"organs\", 1], [\"patients\", 12], [\"reproductive\", 1], [\"result\", 2], [\"show\", 5], [\"usually\", 3], [\"vitamin\", 1]]\r\n",
      "\"absolve\"\t[[\"citizens\", 1], [\"english\", 14], [\"lord\", 7], [\"may\", 55], [\"obligations\", 1], [\"right\", 13], [\"thee\", 2], [\"would\", 47]]\r\n",
      "\"absorbing\"\t[[\"beautiful\", 4], [\"become\", 12], [\"capable\", 5], [\"case\", 11], [\"cost\", 2], [\"far\", 15], [\"france\", 1], [\"interest\", 10], [\"light\", 6], [\"may\", 55], [\"molecules\", 1], [\"still\", 8]]\r\n",
      "\"abundantly\"\t[[\"apparently\", 2], [\"become\", 12], [\"clear\", 1], [\"consumption\", 1], [\"exemplified\", 1], [\"found\", 17], [\"naked\", 1], [\"supplied\", 2]]\r\n",
      "\"abyss\"\t[[\"distress\", 1], [\"lead\", 7], [\"nothing\", 6], [\"occupied\", 1], [\"previous\", 4], [\"separates\", 2], [\"ultimately\", 2], [\"us\", 21]]\r\n",
      "\"ac\"\t[[\"angle\", 4], [\"circuit\", 2], [\"components\", 1], [\"draw\", 4], [\"equal\", 4], [\"equivalent\", 1], [\"fig\", 1], [\"line\", 12], [\"making\", 8], [\"output\", 2], [\"plate\", 3], [\"signal\", 2]]\r\n",
      "\"accusation\"\t[[\"face\", 5], [\"laid\", 3], [\"make\", 28], [\"placing\", 1], [\"serious\", 2]]\r\n",
      "\"accustom\"\t[[\"able\", 8], [\"change\", 7], [\"could\", 25], [\"minds\", 1], [\"never\", 12], [\"obedience\", 1]]\r\n",
      "\"acetic\"\t[[\"acid\", 4], [\"acids\", 2], [\"add\", 2], [\"added\", 2], [\"alcohol\", 1], [\"converted\", 1], [\"drop\", 3], [\"slight\", 2], [\"times\", 6]]\r\n",
      "cat: write error: Broken pipe\r\n"
     ]
    }
   ],
   "source": [
    "# Save into file for processing\n",
    "!hadoop fs -cat {OUTPUT_PATH}/* > google_index_1\n",
    "!cat google_index_1 | head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################################################\n",
    "# Pretty print systems tests for generating Inverted Index\n",
    "##########################################################\n",
    "\n",
    "import json\n",
    "\n",
    "for i in range(1,2):\n",
    "    print \"—\"*100\n",
    "    print \"Systems test \",i,\" - Inverted Index\"\n",
    "    print \"—\"*100  \n",
    "    with open(\"google_index_\"+str(i)+\"\",\"r\") as f:\n",
    "        lines = f.readlines()\n",
    "        for line in lines:\n",
    "            line = line.strip()\n",
    "            word,stripe = line.split(\"\\t\")\n",
    "            stripe = json.loads(stripe)\n",
    "            stripe.extend([[\"\",\"\"] for _ in xrange(3 - len(stripe))])\n",
    "\n",
    "            print \"{0:>16} |{1:>16} |{2:>16} |{3:>16}\".format((word), \n",
    "                stripe[0][0]+\" \"+str(stripe[0][1]), stripe[1][0]+\" \"+str(stripe[1][1]), stripe[2][0]+\" \"+str(stripe[2][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18/03/06 15:45:11 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 5760 minutes, Emptier interval = 360 minutes.\n",
      "Moved: 'hdfs://nn-ia.s3s.altiscale.com:8020/user/lteo01/tests' to trash at: hdfs://nn-ia.s3s.altiscale.com:8020/user/lteo01/.Trash/Current\n",
      "Using configs in /home/lteo01/.mrjob.conf\n",
      "Looking for hadoop binary in /opt/hadoop/bin...\n",
      "Found hadoop binary: /opt/hadoop/bin/hadoop\n",
      "Using Hadoop version 2.7.3\n",
      "Looking for Hadoop streaming jar in /opt/hadoop...\n",
      "Found Hadoop streaming jar: /opt/hadoop/share/hadoop/tools/lib/hadoop-streaming-2.7.3.jar\n",
      "Creating temp directory /tmp/similarity_v2.lteo01.20180306.154512.772830\n",
      "Copying local files to hdfs:///user/lteo01/tmp/mrjob/similarity_v2.lteo01.20180306.154512.772830/files/...\n",
      "Running step 1 of 2...\n",
      "  packageJobJar: [] [/opt/hadoop-2.7.3/share/hadoop/tools/lib/hadoop-streaming-2.7.3.jar] /tmp/streamjob6359915834628725457.jar tmpDir=null\n",
      "  Timeline service address: http://rm-ia.s3s.altiscale.com:8188/ws/v1/timeline/\n",
      "  Connecting to ResourceManager at rm-ia.s3s.altiscale.com/10.251.255.108:8032\n",
      "  Connecting to Application History server at rm-ia.s3s.altiscale.com/10.251.255.108:10200\n",
      "  Timeline service address: http://rm-ia.s3s.altiscale.com:8188/ws/v1/timeline/\n",
      "  Connecting to ResourceManager at rm-ia.s3s.altiscale.com/10.251.255.108:8032\n",
      "  Connecting to Application History server at rm-ia.s3s.altiscale.com/10.251.255.108:10200\n",
      "  Loaded native gpl library from the embedded binaries\n",
      "  Successfully loaded & initialized native-lzo library [hadoop-lzo rev d62701d4d05dfa6115bbaf8d9dff002df142e62d]\n",
      "  Total input paths to process : 1\n",
      "  number of splits:64\n",
      "  Submitting tokens for job: job_1509050304403_25866\n",
      "  Submitted application application_1509050304403_25866\n",
      "  The url to track the job: http://rm-ia.s3s.altiscale.com:8088/proxy/application_1509050304403_25866/\n",
      "  Running job: job_1509050304403_25866\n",
      "  Job job_1509050304403_25866 running in uber mode : false\n",
      "   map 0% reduce 0%\n",
      "   map 8% reduce 0%\n",
      "   map 17% reduce 0%\n",
      "   map 20% reduce 0%\n",
      "   map 41% reduce 0%\n",
      "   map 44% reduce 0%\n",
      "   map 52% reduce 0%\n",
      "   map 55% reduce 0%\n",
      "   map 61% reduce 0%\n",
      "   map 72% reduce 0%\n",
      "   map 100% reduce 0%\n",
      "   map 100% reduce 2%\n",
      "   map 100% reduce 3%\n",
      "   map 100% reduce 8%\n",
      "   map 100% reduce 14%\n",
      "   map 100% reduce 19%\n",
      "   map 100% reduce 27%\n",
      "   map 100% reduce 41%\n",
      "   map 100% reduce 47%\n",
      "   map 100% reduce 50%\n",
      "   map 100% reduce 66%\n",
      "   map 100% reduce 75%\n",
      "   map 100% reduce 81%\n",
      "   map 100% reduce 91%\n",
      "   map 100% reduce 97%\n",
      "   map 100% reduce 100%\n",
      "  Job job_1509050304403_25866 completed successfully\n",
      "  Output directory: hdfs:///user/lteo01/tmp/mrjob/similarity_v2.lteo01.20180306.154512.772830/step-output/0000\n",
      "Counters: 53\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=4563740\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=3176012\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=577604\n",
      "\t\tFILE: Number of bytes written=18581799\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=4574876\n",
      "\t\tHDFS: Number of bytes written=3176012\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of read operations=384\n",
      "\t\tHDFS: Number of write operations=128\n",
      "\tJob Counters \n",
      "\t\tKilled map tasks=1\n",
      "\t\tKilled reduce tasks=1\n",
      "\t\tLaunched map tasks=64\n",
      "\t\tLaunched reduce tasks=64\n",
      "\t\tRack-local map tasks=64\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=2604532224\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=993692160\n",
      "\t\tTotal time spent by all map tasks (ms)=1695659\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=5086977\n",
      "\t\tTotal time spent by all reduce tasks (ms)=388161\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=1940805\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=1695659\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=388161\n",
      "\tMap-Reduce Framework\n",
      "\t\tCPU time spent (ms)=194410\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tGC time elapsed (ms)=29618\n",
      "\t\tInput split bytes=11136\n",
      "\t\tMap input records=999\n",
      "\t\tMap output bytes=1110023\n",
      "\t\tMap output materialized bytes=1027639\n",
      "\t\tMap output records=33859\n",
      "\t\tMerged Map outputs=4096\n",
      "\t\tPhysical memory (bytes) snapshot=70392254464\n",
      "\t\tReduce input groups=33186\n",
      "\t\tReduce input records=33859\n",
      "\t\tReduce output records=33186\n",
      "\t\tReduce shuffle bytes=1027639\n",
      "\t\tShuffled Maps =4096\n",
      "\t\tSpilled Records=67718\n",
      "\t\tTotal committed heap usage (bytes)=87347429376\n",
      "\t\tVirtual memory (bytes) snapshot=511293038592\n",
      "\tMapper Counters\n",
      "\t\tCalls=999\n",
      "\tReducer Counters\n",
      "\t\tCalls=33186\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "Running step 2 of 2...\n",
      "  packageJobJar: [] [/opt/hadoop-2.7.3/share/hadoop/tools/lib/hadoop-streaming-2.7.3.jar] /tmp/streamjob5005793851484577522.jar tmpDir=null\n",
      "  Timeline service address: http://rm-ia.s3s.altiscale.com:8188/ws/v1/timeline/\n",
      "  Connecting to ResourceManager at rm-ia.s3s.altiscale.com/10.251.255.108:8032\n",
      "  Connecting to Application History server at rm-ia.s3s.altiscale.com/10.251.255.108:10200\n",
      "  Timeline service address: http://rm-ia.s3s.altiscale.com:8188/ws/v1/timeline/\n",
      "  Connecting to ResourceManager at rm-ia.s3s.altiscale.com/10.251.255.108:8032\n",
      "  Connecting to Application History server at rm-ia.s3s.altiscale.com/10.251.255.108:10200\n",
      "  Loaded native gpl library from the embedded binaries\n",
      "  Successfully loaded & initialized native-lzo library [hadoop-lzo rev d62701d4d05dfa6115bbaf8d9dff002df142e62d]\n",
      "  Total input paths to process : 64\n",
      "  number of splits:64\n",
      "  Submitting tokens for job: job_1509050304403_25869\n",
      "  Submitted application application_1509050304403_25869\n",
      "  The url to track the job: http://rm-ia.s3s.altiscale.com:8088/proxy/application_1509050304403_25869/\n",
      "  Running job: job_1509050304403_25869\n",
      "  Job job_1509050304403_25869 running in uber mode : false\n",
      "   map 0% reduce 0%\n",
      "   map 8% reduce 0%\n",
      "   map 27% reduce 0%\n",
      "   map 41% reduce 0%\n",
      "   map 42% reduce 0%\n",
      "   map 47% reduce 0%\n",
      "   map 48% reduce 0%\n",
      "   map 56% reduce 0%\n",
      "   map 61% reduce 0%\n",
      "   map 75% reduce 0%\n",
      "   map 100% reduce 0%\n",
      "   map 100% reduce 100%\n",
      "  Job job_1509050304403_25869 completed successfully\n",
      "  Output directory: hdfs:///user/lteo01/tests\n",
      "Counters: 51\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=3176012\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=2994314\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=585477\n",
      "\t\tFILE: Number of bytes written=10234831\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=3187660\n",
      "\t\tHDFS: Number of bytes written=2994314\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of read operations=195\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tIntermediate Reducer Counters\n",
      "\t\tCalls=763\n",
      "\tJob Counters \n",
      "\t\tKilled map tasks=1\n",
      "\t\tLaunched map tasks=64\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tRack-local map tasks=64\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=2209935360\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=17390080\n",
      "\t\tTotal time spent by all map tasks (ms)=1438760\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=4316280\n",
      "\t\tTotal time spent by all reduce tasks (ms)=6793\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=33965\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=1438760\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=6793\n",
      "\tMap-Reduce Framework\n",
      "\t\tCPU time spent (ms)=81410\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tGC time elapsed (ms)=12981\n",
      "\t\tInput split bytes=11648\n",
      "\t\tMap input records=33186\n",
      "\t\tMap output bytes=3209302\n",
      "\t\tMap output materialized bytes=1056355\n",
      "\t\tMap output records=33186\n",
      "\t\tMerged Map outputs=64\n",
      "\t\tPhysical memory (bytes) snapshot=51216035840\n",
      "\t\tReduce input groups=33186\n",
      "\t\tReduce input records=33186\n",
      "\t\tReduce output records=33186\n",
      "\t\tReduce shuffle bytes=1056355\n",
      "\t\tShuffled Maps =64\n",
      "\t\tSpilled Records=66372\n",
      "\t\tTotal committed heap usage (bytes)=56915656704\n",
      "\t\tVirtual memory (bytes) snapshot=223498162176\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "Removing HDFS temp directory hdfs:///user/lteo01/tmp/mrjob/similarity_v2.lteo01.20180306.154512.772830...\n",
      "Removing temp directory /tmp/similarity_v2.lteo01.20180306.154512.772830...\n",
      "\n",
      "real\t2m29.156s\n",
      "user\t0m46.927s\n",
      "sys\t0m2.245s\n"
     ]
    }
   ],
   "source": [
    "# Run in Hadoop\n",
    "\n",
    "OUTPUT_PATH = os.path.join(OUTPUT_PATH_BASE,'tests')\n",
    "!hadoop fs -rm -r {OUTPUT_PATH}\n",
    "\n",
    "!time python similarity_v2.py \\\n",
    "        -r hadoop google_index_1 \\\n",
    "        --output-dir={OUTPUT_PATH} \\\n",
    "        --no-output\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\t[\"conspicuous - temper\", 1.0, 1.0, 1.0, 1.0]\r\n",
      "1.0\t[\"honest - mentioned\", 1.0, 1.0, 1.0, 1.0]\r\n",
      "1.0\t[\"conscience - estate\", 1.0, 1.0, 1.0, 1.0]\r\n",
      "1.0\t[\"susceptible - undertook\", 1.0, 1.0, 1.0, 1.0]\r\n",
      "1.0\t[\"check - migration\", 1.0, 1.0, 1.0, 1.0]\r\n",
      "1.0\t[\"citizens - obligations\", 1.0, 1.0, 1.0, 1.0]\r\n",
      "1.0\t[\"defect - edge\", 1.0, 1.0, 1.0, 1.0]\r\n",
      "1.0\t[\"translation - varied\", 1.0, 1.0, 1.0, 1.0]\r\n",
      "1.0\t[\"retina - stigma\", 1.0, 1.0, 1.0, 1.0]\r\n",
      "1.0\t[\"ad - illinois\", 1.0, 1.0, 1.0, 1.0]\r\n",
      "cat: write error: Broken pipe\r\n"
     ]
    }
   ],
   "source": [
    "# Save into file for processing\n",
    "!hadoop fs -cat {OUTPUT_PATH}/* > google_similarities_1\n",
    "!cat google_similarities_1 | head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################\n",
    "# Pretty print systems tests\n",
    "# Note: adjust print formatting if you need to\n",
    "############################################\n",
    "\n",
    "import json\n",
    "for i in range(1,2):\n",
    "    print '—'*110\n",
    "    print \"Systems test \",i,\" - Similarity measures\"\n",
    "    print '—'*110\n",
    "    print \"{0:>21} | {1:>15} |{2:>15} | {3:>15} | {4:>15} | {5:>15}\".format(\"pair\", \n",
    "        \"cosine\", \"jaccard\", \"overlap\", \"dice\", \"average\")\n",
    "    print '-'*110\n",
    "\n",
    "    with open(\"google_similarities_\"+str(i),\"r\") as f:\n",
    "        lines = f.readlines()\n",
    "        for line in lines:\n",
    "            line = line.strip()\n",
    "            avg,stripe = line.split(\"\\t\")\n",
    "            stripe = json.loads(stripe)\n",
    "            \n",
    "            print \"{0:>21} | {1:>15f} |{2:>15f} |{3:>15f} | {4:>15f} | {5:>15f} \".format(stripe[0], \n",
    "                float(stripe[1]), float(stripe[2]), float(stripe[3]), float(stripe[4]), float(avg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ADD CELLS AS NEEDED"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HW5.7.2 Running on 20 test files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting buildStripes_stopwords_20.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile buildStripes_stopwords_20.py\n",
    "#!~/opt/anaconda2/bin/python\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "from __future__ import division\n",
    "import re\n",
    "import mrjob\n",
    "import json\n",
    "from mrjob.protocol import RawProtocol\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "import itertools\n",
    "\n",
    "class MRbuildStripes(MRJob):\n",
    "    \n",
    "    SORT_VALUES = True\n",
    "    \n",
    "    def steps(self):\n",
    "        return [\n",
    "            MRStep(\n",
    "                mapper_init=self.mapper_init,\n",
    "                mapper=self.mapper,\n",
    "                reducer=self.reducer,\n",
    "                jobconf = {\n",
    "                    \"mapreduce.job.reduces\": \"64\",\n",
    "                    \"mapreduce.job.maps\": \"64\",\n",
    "#                     \"SORT_VALUES\":True\n",
    "                }\n",
    "            ),\n",
    "            MRStep(\n",
    "                reducer=self.reducer_2,\n",
    "                jobconf = {\n",
    "                    \"mapreduce.job.reduces\": \"1\",\n",
    "                    \"SORT_VALUES\":True\n",
    "                }\n",
    "            )\n",
    "            \n",
    "        ]\n",
    "    \n",
    "    def mapper_init(self):\n",
    "        self.idx = 9001  # To define when feature set starts\n",
    "        self.filename = 'ten_thousand_20.json'\n",
    "        \n",
    "        self.top_words = []\n",
    "        self.features = []\n",
    "        #with open('features_20.json', 'r') as infile:\n",
    "        #    self.features = json.loads(infile.read())\n",
    "        with open(self.filename, 'r') as infile:\n",
    "            self.top_words = json.loads(infile.read())\n",
    "            self.features =self.top_words[self.idx:]\n",
    "\n",
    "    def mapper(self, _, line):\n",
    "        fields = line.lower().strip(\"\\n\").split(\"\\t\")\n",
    "        words = fields[0].split(\" \")\n",
    "        occurrence_count = int(fields[1])\n",
    "        filtered_words = [word.decode('utf-8', 'ignore') for word in words if word.decode('utf-8', 'ignore') in self.top_words]\n",
    "        for subset in itertools.combinations(sorted(set(filtered_words)), 2):\n",
    "            if subset[0] in self.top_words and subset[1] in self.features:\n",
    "                yield subset[0], (subset[1], occurrence_count)\n",
    "            if subset[1] in self.top_words and subset[0] in self.features:\n",
    "                yield subset[1], (subset[0], occurrence_count)\n",
    "    \n",
    "    def reducer(self, word, occurrence_counts):\n",
    "        stripe = {}\n",
    "        for other_word, occurrence_count in occurrence_counts:\n",
    "            stripe[other_word] = stripe.get(other_word,0)+occurrence_count\n",
    "        yield word, stripe\n",
    "    \n",
    "    def reducer_2(self, key, values):\n",
    "        yield str(key), list(values)[0]\n",
    "        \n",
    "if __name__ == '__main__':\n",
    "    MRbuildStripes.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18/03/06 15:53:11 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 5760 minutes, Emptier interval = 360 minutes.\n",
      "Moved: 'hdfs://nn-ia.s3s.altiscale.com:8020/user/lteo01/tests' to trash at: hdfs://nn-ia.s3s.altiscale.com:8020/user/lteo01/.Trash/Current\n",
      "Using configs in /home/lteo01/.mrjob.conf\n",
      "Looking for hadoop binary in /opt/hadoop/bin...\n",
      "Found hadoop binary: /opt/hadoop/bin/hadoop\n",
      "Using Hadoop version 2.7.3\n",
      "Looking for Hadoop streaming jar in /opt/hadoop...\n",
      "Found Hadoop streaming jar: /opt/hadoop/share/hadoop/tools/lib/hadoop-streaming-2.7.3.jar\n",
      "Creating temp directory /tmp/buildStripes_stopwords_20.lteo01.20180306.155312.676147\n",
      "Copying local files to hdfs:///user/lteo01/tmp/mrjob/buildStripes_stopwords_20.lteo01.20180306.155312.676147/files/...\n",
      "Running step 1 of 2...\n",
      "  packageJobJar: [] [/opt/hadoop-2.7.3/share/hadoop/tools/lib/hadoop-streaming-2.7.3.jar] /tmp/streamjob7797187335133637485.jar tmpDir=null\n",
      "  Timeline service address: http://rm-ia.s3s.altiscale.com:8188/ws/v1/timeline/\n",
      "  Connecting to ResourceManager at rm-ia.s3s.altiscale.com/10.251.255.108:8032\n",
      "  Connecting to Application History server at rm-ia.s3s.altiscale.com/10.251.255.108:10200\n",
      "  Timeline service address: http://rm-ia.s3s.altiscale.com:8188/ws/v1/timeline/\n",
      "  Connecting to ResourceManager at rm-ia.s3s.altiscale.com/10.251.255.108:8032\n",
      "  Connecting to Application History server at rm-ia.s3s.altiscale.com/10.251.255.108:10200\n",
      "  Loaded native gpl library from the embedded binaries\n",
      "  Successfully loaded & initialized native-lzo library [hadoop-lzo rev d62701d4d05dfa6115bbaf8d9dff002df142e62d]\n",
      "  Total input paths to process : 20\n",
      "  number of splits:77\n",
      "  Submitting tokens for job: job_1509050304403_25883\n",
      "  Submitted application application_1509050304403_25883\n",
      "  The url to track the job: http://rm-ia.s3s.altiscale.com:8088/proxy/application_1509050304403_25883/\n",
      "  Running job: job_1509050304403_25883\n",
      "  Job job_1509050304403_25883 running in uber mode : false\n",
      "   map 0% reduce 0%\n",
      "   map 1% reduce 0%\n",
      "   map 2% reduce 0%\n",
      "   map 3% reduce 0%\n",
      "   map 4% reduce 0%\n",
      "   map 5% reduce 0%\n",
      "   map 6% reduce 0%\n",
      "   map 7% reduce 0%\n",
      "   map 8% reduce 0%\n",
      "   map 9% reduce 0%\n",
      "   map 10% reduce 0%\n",
      "   map 11% reduce 0%\n",
      "   map 12% reduce 0%\n",
      "   map 13% reduce 0%\n",
      "   map 14% reduce 0%\n",
      "   map 15% reduce 0%\n",
      "   map 16% reduce 0%\n",
      "   map 17% reduce 0%\n",
      "   map 18% reduce 0%\n",
      "   map 19% reduce 0%\n",
      "   map 20% reduce 0%\n",
      "   map 21% reduce 0%\n",
      "   map 22% reduce 0%\n",
      "   map 23% reduce 0%\n",
      "   map 24% reduce 0%\n",
      "   map 25% reduce 0%\n",
      "   map 26% reduce 0%\n",
      "   map 27% reduce 0%\n",
      "   map 28% reduce 0%\n",
      "   map 29% reduce 0%\n",
      "   map 30% reduce 0%\n",
      "   map 31% reduce 0%\n",
      "   map 32% reduce 0%\n",
      "   map 33% reduce 0%\n",
      "   map 34% reduce 0%\n",
      "   map 35% reduce 0%\n",
      "   map 36% reduce 0%\n",
      "   map 37% reduce 0%\n",
      "   map 38% reduce 0%\n",
      "   map 39% reduce 0%\n",
      "   map 40% reduce 0%\n",
      "   map 41% reduce 0%\n",
      "   map 42% reduce 0%\n",
      "   map 44% reduce 0%\n",
      "   map 45% reduce 0%\n",
      "   map 46% reduce 0%\n",
      "   map 47% reduce 0%\n",
      "   map 48% reduce 0%\n",
      "   map 49% reduce 0%\n",
      "   map 50% reduce 0%\n",
      "   map 51% reduce 0%\n",
      "   map 52% reduce 0%\n",
      "   map 53% reduce 0%\n",
      "   map 54% reduce 0%\n",
      "   map 55% reduce 0%\n",
      "   map 56% reduce 0%\n",
      "   map 57% reduce 0%\n",
      "   map 58% reduce 0%\n",
      "   map 59% reduce 0%\n",
      "   map 60% reduce 0%\n",
      "   map 61% reduce 0%\n",
      "   map 62% reduce 0%\n",
      "   map 63% reduce 0%\n",
      "   map 64% reduce 0%\n",
      "   map 65% reduce 0%\n",
      "   map 66% reduce 0%\n",
      "   map 67% reduce 0%\n",
      "   map 68% reduce 0%\n",
      "   map 69% reduce 0%\n",
      "   map 70% reduce 0%\n",
      "   map 71% reduce 0%\n",
      "   map 72% reduce 0%\n",
      "   map 73% reduce 0%\n",
      "   map 74% reduce 0%\n",
      "   map 75% reduce 0%\n",
      "   map 76% reduce 0%\n",
      "   map 77% reduce 0%\n",
      "   map 78% reduce 0%\n",
      "   map 79% reduce 0%\n",
      "   map 80% reduce 0%\n",
      "   map 81% reduce 0%\n",
      "   map 82% reduce 0%\n",
      "   map 83% reduce 0%\n",
      "   map 84% reduce 0%\n",
      "   map 85% reduce 0%\n",
      "   map 86% reduce 0%\n",
      "   map 87% reduce 0%\n",
      "   map 88% reduce 0%\n",
      "   map 90% reduce 0%\n",
      "   map 91% reduce 0%\n",
      "   map 92% reduce 0%\n",
      "   map 93% reduce 0%\n",
      "   map 94% reduce 0%\n",
      "   map 95% reduce 0%\n",
      "   map 97% reduce 0%\n",
      "   map 98% reduce 0%\n",
      "   map 99% reduce 0%\n",
      "   map 100% reduce 0%\n",
      "   map 100% reduce 3%\n",
      "   map 100% reduce 6%\n",
      "   map 100% reduce 13%\n",
      "   map 100% reduce 14%\n",
      "   map 100% reduce 17%\n",
      "   map 100% reduce 25%\n",
      "   map 100% reduce 33%\n",
      "   map 100% reduce 36%\n",
      "   map 100% reduce 44%\n",
      "   map 100% reduce 50%\n",
      "   map 100% reduce 58%\n",
      "   map 100% reduce 66%\n",
      "   map 100% reduce 81%\n",
      "   map 100% reduce 94%\n",
      "   map 100% reduce 97%\n",
      "   map 100% reduce 100%\n",
      "  Job job_1509050304403_25883 completed successfully\n",
      "  Output directory: hdfs:///user/lteo01/tmp/mrjob/buildStripes_stopwords_20.lteo01.20180306.155312.676147/step-output/0000\n",
      "Counters: 52\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=218073461\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=1918345\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=1753225\n",
      "\t\tFILE: Number of bytes written=24057250\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=218086129\n",
      "\t\tHDFS: Number of bytes written=1918345\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of read operations=423\n",
      "\t\tHDFS: Number of write operations=128\n",
      "\tJob Counters \n",
      "\t\tKilled map tasks=16\n",
      "\t\tKilled reduce tasks=1\n",
      "\t\tLaunched map tasks=93\n",
      "\t\tLaunched reduce tasks=64\n",
      "\t\tOther local map tasks=1\n",
      "\t\tRack-local map tasks=92\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=31726268928\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=892556800\n",
      "\t\tTotal time spent by all map tasks (ms)=20655123\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=61965369\n",
      "\t\tTotal time spent by all reduce tasks (ms)=348655\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=1743275\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=20655123\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=348655\n",
      "\tMap-Reduce Framework\n",
      "\t\tCPU time spent (ms)=10313340\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tGC time elapsed (ms)=36016\n",
      "\t\tInput split bytes=12668\n",
      "\t\tMap input records=5931307\n",
      "\t\tMap output bytes=4386717\n",
      "\t\tMap output materialized bytes=3506526\n",
      "\t\tMap output records=161180\n",
      "\t\tMerged Map outputs=4928\n",
      "\t\tPhysical memory (bytes) snapshot=81559470080\n",
      "\t\tReduce input groups=158959\n",
      "\t\tReduce input records=161180\n",
      "\t\tReduce output records=9396\n",
      "\t\tReduce shuffle bytes=3506526\n",
      "\t\tShuffled Maps =4928\n",
      "\t\tSpilled Records=322360\n",
      "\t\tTotal committed heap usage (bytes)=99076800512\n",
      "\t\tVirtual memory (bytes) snapshot=555853099008\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "Running step 2 of 2...\n",
      "  packageJobJar: [] [/opt/hadoop-2.7.3/share/hadoop/tools/lib/hadoop-streaming-2.7.3.jar] /tmp/streamjob5720480750309435003.jar tmpDir=null\n",
      "  Timeline service address: http://rm-ia.s3s.altiscale.com:8188/ws/v1/timeline/\n",
      "  Connecting to ResourceManager at rm-ia.s3s.altiscale.com/10.251.255.108:8032\n",
      "  Connecting to Application History server at rm-ia.s3s.altiscale.com/10.251.255.108:10200\n",
      "  Timeline service address: http://rm-ia.s3s.altiscale.com:8188/ws/v1/timeline/\n",
      "  Connecting to ResourceManager at rm-ia.s3s.altiscale.com/10.251.255.108:8032\n",
      "  Connecting to Application History server at rm-ia.s3s.altiscale.com/10.251.255.108:10200\n",
      "  Loaded native gpl library from the embedded binaries\n",
      "  Successfully loaded & initialized native-lzo library [hadoop-lzo rev d62701d4d05dfa6115bbaf8d9dff002df142e62d]\n",
      "  Total input paths to process : 64\n",
      "  number of splits:64\n",
      "  Submitting tokens for job: job_1509050304403_25892\n",
      "  Submitted application application_1509050304403_25892\n",
      "  The url to track the job: http://rm-ia.s3s.altiscale.com:8088/proxy/application_1509050304403_25892/\n",
      "  Running job: job_1509050304403_25892\n",
      "  Job job_1509050304403_25892 running in uber mode : false\n",
      "   map 0% reduce 0%\n",
      "   map 2% reduce 0%\n",
      "   map 5% reduce 0%\n",
      "   map 25% reduce 0%\n",
      "   map 48% reduce 0%\n",
      "   map 98% reduce 0%\n",
      "   map 100% reduce 0%\n",
      "   map 100% reduce 100%\n",
      "  Job job_1509050304403_25892 completed successfully\n",
      "  Output directory: hdfs:///user/lteo01/tests\n",
      "Counters: 50\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=1918345\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=1918345\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=971100\n",
      "\t\tFILE: Number of bytes written=10667416\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=1930761\n",
      "\t\tHDFS: Number of bytes written=1918345\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of read operations=195\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tKilled map tasks=1\n",
      "\t\tLaunched map tasks=64\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tRack-local map tasks=64\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=1729265664\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=9607680\n",
      "\t\tTotal time spent by all map tasks (ms)=1125824\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=3377472\n",
      "\t\tTotal time spent by all reduce tasks (ms)=3753\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=18765\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=1125824\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=3753\n",
      "\tMap-Reduce Framework\n",
      "\t\tCPU time spent (ms)=71840\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tGC time elapsed (ms)=13247\n",
      "\t\tInput split bytes=12416\n",
      "\t\tMap input records=9396\n",
      "\t\tMap output bytes=1932973\n",
      "\t\tMap output materialized bytes=1083947\n",
      "\t\tMap output records=9396\n",
      "\t\tMerged Map outputs=64\n",
      "\t\tPhysical memory (bytes) snapshot=51164381184\n",
      "\t\tReduce input groups=9396\n",
      "\t\tReduce input records=9396\n",
      "\t\tReduce output records=9396\n",
      "\t\tReduce shuffle bytes=1083947\n",
      "\t\tShuffled Maps =64\n",
      "\t\tSpilled Records=18792\n",
      "\t\tTotal committed heap usage (bytes)=56856936448\n",
      "\t\tVirtual memory (bytes) snapshot=223487868928\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "Removing HDFS temp directory hdfs:///user/lteo01/tmp/mrjob/buildStripes_stopwords_20.lteo01.20180306.155312.676147...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing temp directory /tmp/buildStripes_stopwords_20.lteo01.20180306.155312.676147...\r\n",
      "\r\n",
      "real\t7m27.141s\r\n",
      "user\t0m51.520s\r\n",
      "sys\t0m2.541s\r\n"
     ]
    }
   ],
   "source": [
    "# Run in Hadoop\n",
    "\n",
    "OUTPUT_PATH = os.path.join(OUTPUT_PATH_BASE,'tests')\n",
    "!hadoop fs -rm -r {OUTPUT_PATH}\n",
    "\n",
    "!time python buildStripes_stopwords_20.py \\\n",
    "        -r hadoop hdfs://{TEST_20}/* \\\n",
    "        --file ten_thousand_20.json \\\n",
    "        --output-dir={OUTPUT_PATH} \\\n",
    "        --no-output\n",
    "            \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "!hadoop fs -cat {OUTPUT_PATH}/* > google_stripes_20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18/03/06 16:00:43 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 5760 minutes, Emptier interval = 360 minutes.\n",
      "Moved: 'hdfs://nn-ia.s3s.altiscale.com:8020/user/lteo01/tests' to trash at: hdfs://nn-ia.s3s.altiscale.com:8020/user/lteo01/.Trash/Current\n",
      "Using configs in /home/lteo01/.mrjob.conf\n",
      "Looking for hadoop binary in /opt/hadoop/bin...\n",
      "Found hadoop binary: /opt/hadoop/bin/hadoop\n",
      "Using Hadoop version 2.7.3\n",
      "Looking for Hadoop streaming jar in /opt/hadoop...\n",
      "Found Hadoop streaming jar: /opt/hadoop/share/hadoop/tools/lib/hadoop-streaming-2.7.3.jar\n",
      "Creating temp directory /tmp/invertedIndex_v2.lteo01.20180306.160044.358309\n",
      "Copying local files to hdfs:///user/lteo01/tmp/mrjob/invertedIndex_v2.lteo01.20180306.160044.358309/files/...\n",
      "Running step 1 of 1...\n",
      "  packageJobJar: [] [/opt/hadoop-2.7.3/share/hadoop/tools/lib/hadoop-streaming-2.7.3.jar] /tmp/streamjob9036766339060416359.jar tmpDir=null\n",
      "  Timeline service address: http://rm-ia.s3s.altiscale.com:8188/ws/v1/timeline/\n",
      "  Connecting to ResourceManager at rm-ia.s3s.altiscale.com/10.251.255.108:8032\n",
      "  Connecting to Application History server at rm-ia.s3s.altiscale.com/10.251.255.108:10200\n",
      "  Timeline service address: http://rm-ia.s3s.altiscale.com:8188/ws/v1/timeline/\n",
      "  Connecting to ResourceManager at rm-ia.s3s.altiscale.com/10.251.255.108:8032\n",
      "  Connecting to Application History server at rm-ia.s3s.altiscale.com/10.251.255.108:10200\n",
      "  Loaded native gpl library from the embedded binaries\n",
      "  Successfully loaded & initialized native-lzo library [hadoop-lzo rev d62701d4d05dfa6115bbaf8d9dff002df142e62d]\n",
      "  Total input paths to process : 1\n",
      "  number of splits:2\n",
      "  Submitting tokens for job: job_1509050304403_25895\n",
      "  Submitted application application_1509050304403_25895\n",
      "  The url to track the job: http://rm-ia.s3s.altiscale.com:8088/proxy/application_1509050304403_25895/\n",
      "  Running job: job_1509050304403_25895\n",
      "  Job job_1509050304403_25895 running in uber mode : false\n",
      "   map 0% reduce 0%\n",
      "   map 100% reduce 0%\n",
      "   map 100% reduce 100%\n",
      "  Job job_1509050304403_25895 completed successfully\n",
      "  Output directory: hdfs:///user/lteo01/tests\n",
      "Counters: 51\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=2007749\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=1825200\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=1215199\n",
      "\t\tFILE: Number of bytes written=2796078\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=2008111\n",
      "\t\tHDFS: Number of bytes written=1825200\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of read operations=9\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tRack-local map tasks=2\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=21660672\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=18731520\n",
      "\t\tTotal time spent by all map tasks (ms)=14102\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=42306\n",
      "\t\tTotal time spent by all reduce tasks (ms)=7317\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=36585\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=14102\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=7317\n",
      "\tMap-Reduce Framework\n",
      "\t\tCPU time spent (ms)=8600\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tGC time elapsed (ms)=250\n",
      "\t\tInput split bytes=362\n",
      "\t\tMap input records=9396\n",
      "\t\tMap output bytes=3472723\n",
      "\t\tMap output materialized bytes=1184226\n",
      "\t\tMap output records=111602\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tPhysical memory (bytes) snapshot=1942818816\n",
      "\t\tReduce input groups=111602\n",
      "\t\tReduce input records=111602\n",
      "\t\tReduce output records=999\n",
      "\t\tReduce shuffle bytes=1184226\n",
      "\t\tShuffled Maps =2\n",
      "\t\tSpilled Records=223204\n",
      "\t\tTotal committed heap usage (bytes)=2225602560\n",
      "\t\tVirtual memory (bytes) snapshot=11415015424\n",
      "\tMapper Counters\n",
      "\t\tCalls=9396\n",
      "\tReducer Counters\n",
      "\t\tCalls=999\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "Removing HDFS temp directory hdfs:///user/lteo01/tmp/mrjob/invertedIndex_v2.lteo01.20180306.160044.358309...\n",
      "Removing temp directory /tmp/invertedIndex_v2.lteo01.20180306.160044.358309...\n",
      "\n",
      "real\t0m53.397s\n",
      "user\t0m34.685s\n",
      "sys\t0m1.875s\n"
     ]
    }
   ],
   "source": [
    "# Run in Hadoop\n",
    "\n",
    "OUTPUT_PATH = os.path.join(OUTPUT_PATH_BASE,'tests')\n",
    "!hadoop fs -rm -r {OUTPUT_PATH}\n",
    "\n",
    "!time python invertedIndex_v2.py \\\n",
    "        -r hadoop google_stripes_20 \\\n",
    "        --output-dir={OUTPUT_PATH} \\\n",
    "        --no-output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "!hadoop fs -cat {OUTPUT_PATH}/* > google_index_20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18/03/06 16:01:41 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 5760 minutes, Emptier interval = 360 minutes.\n",
      "Moved: 'hdfs://nn-ia.s3s.altiscale.com:8020/user/lteo01/tests' to trash at: hdfs://nn-ia.s3s.altiscale.com:8020/user/lteo01/.Trash/Current\n",
      "Using configs in /home/lteo01/.mrjob.conf\n",
      "Looking for hadoop binary in /opt/hadoop/bin...\n",
      "Found hadoop binary: /opt/hadoop/bin/hadoop\n",
      "Using Hadoop version 2.7.3\n",
      "Looking for Hadoop streaming jar in /opt/hadoop...\n",
      "Found Hadoop streaming jar: /opt/hadoop/share/hadoop/tools/lib/hadoop-streaming-2.7.3.jar\n",
      "Creating temp directory /tmp/similarity_v2.lteo01.20180306.160142.297158\n",
      "Copying local files to hdfs:///user/lteo01/tmp/mrjob/similarity_v2.lteo01.20180306.160142.297158/files/...\n",
      "Running step 1 of 2...\n",
      "  packageJobJar: [] [/opt/hadoop-2.7.3/share/hadoop/tools/lib/hadoop-streaming-2.7.3.jar] /tmp/streamjob2570539209473584308.jar tmpDir=null\n",
      "  Timeline service address: http://rm-ia.s3s.altiscale.com:8188/ws/v1/timeline/\n",
      "  Connecting to ResourceManager at rm-ia.s3s.altiscale.com/10.251.255.108:8032\n",
      "  Connecting to Application History server at rm-ia.s3s.altiscale.com/10.251.255.108:10200\n",
      "  Timeline service address: http://rm-ia.s3s.altiscale.com:8188/ws/v1/timeline/\n",
      "  Connecting to ResourceManager at rm-ia.s3s.altiscale.com/10.251.255.108:8032\n",
      "  Connecting to Application History server at rm-ia.s3s.altiscale.com/10.251.255.108:10200\n",
      "  Loaded native gpl library from the embedded binaries\n",
      "  Successfully loaded & initialized native-lzo library [hadoop-lzo rev d62701d4d05dfa6115bbaf8d9dff002df142e62d]\n",
      "  Total input paths to process : 1\n",
      "  number of splits:64\n",
      "  Submitting tokens for job: job_1509050304403_25897\n",
      "  Submitted application application_1509050304403_25897\n",
      "  The url to track the job: http://rm-ia.s3s.altiscale.com:8088/proxy/application_1509050304403_25897/\n",
      "  Running job: job_1509050304403_25897\n",
      "  Job job_1509050304403_25897 running in uber mode : false\n",
      "   map 0% reduce 0%\n",
      "   map 2% reduce 0%\n",
      "   map 3% reduce 0%\n",
      "   map 5% reduce 0%\n",
      "   map 6% reduce 0%\n",
      "   map 8% reduce 0%\n",
      "   map 9% reduce 0%\n",
      "   map 11% reduce 0%\n",
      "   map 22% reduce 0%\n",
      "   map 34% reduce 0%\n",
      "   map 36% reduce 0%\n",
      "   map 37% reduce 0%\n",
      "   map 38% reduce 0%\n",
      "   map 40% reduce 0%\n",
      "   map 43% reduce 0%\n",
      "   map 48% reduce 0%\n",
      "   map 56% reduce 0%\n",
      "   map 66% reduce 0%\n",
      "   map 80% reduce 0%\n",
      "   map 83% reduce 0%\n",
      "   map 85% reduce 0%\n",
      "   map 86% reduce 0%\n",
      "   map 88% reduce 0%\n",
      "   map 92% reduce 0%\n",
      "   map 100% reduce 0%\n",
      "   map 100% reduce 1%\n",
      "   map 100% reduce 7%\n",
      "   map 100% reduce 11%\n",
      "   map 100% reduce 18%\n",
      "   map 100% reduce 24%\n",
      "   map 100% reduce 29%\n",
      "   map 100% reduce 35%\n",
      "   map 100% reduce 40%\n",
      "   map 100% reduce 43%\n",
      "   map 100% reduce 47%\n",
      "   map 100% reduce 52%\n",
      "   map 100% reduce 58%\n",
      "   map 100% reduce 63%\n",
      "   map 100% reduce 67%\n",
      "   map 100% reduce 71%\n",
      "   map 100% reduce 80%\n",
      "   map 100% reduce 89%\n",
      "   map 100% reduce 94%\n",
      "   map 100% reduce 97%\n",
      "   map 100% reduce 98%\n",
      "   map 100% reduce 99%\n",
      "   map 100% reduce 100%\n",
      "  Job job_1509050304403_25897 completed successfully\n",
      "  Output directory: hdfs:///user/lteo01/tmp/mrjob/similarity_v2.lteo01.20180306.160142.297158/step-output/0000\n",
      "Counters: 53\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=8134016\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=447956582\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=62978707\n",
      "\t\tFILE: Number of bytes written=179602432\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=8145216\n",
      "\t\tHDFS: Number of bytes written=447956582\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of read operations=384\n",
      "\t\tHDFS: Number of write operations=128\n",
      "\tJob Counters \n",
      "\t\tKilled map tasks=2\n",
      "\t\tKilled reduce tasks=1\n",
      "\t\tLaunched map tasks=65\n",
      "\t\tLaunched reduce tasks=65\n",
      "\t\tRack-local map tasks=65\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=3014295552\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=2499348480\n",
      "\t\tTotal time spent by all map tasks (ms)=1962432\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=5887296\n",
      "\t\tTotal time spent by all reduce tasks (ms)=976308\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=4881540\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=1962432\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=976308\n",
      "\tMap-Reduce Framework\n",
      "\t\tCPU time spent (ms)=1255420\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tGC time elapsed (ms)=29929\n",
      "\t\tInput split bytes=11200\n",
      "\t\tMap input records=999\n",
      "\t\tMap output bytes=218931010\n",
      "\t\tMap output materialized bytes=99647041\n",
      "\t\tMap output records=6341523\n",
      "\t\tMerged Map outputs=4096\n",
      "\t\tPhysical memory (bytes) snapshot=74688458752\n",
      "\t\tReduce input groups=3830170\n",
      "\t\tReduce input records=6341523\n",
      "\t\tReduce output records=3830170\n",
      "\t\tReduce shuffle bytes=99647041\n",
      "\t\tShuffled Maps =4096\n",
      "\t\tSpilled Records=12683046\n",
      "\t\tTotal committed heap usage (bytes)=88349868032\n",
      "\t\tVirtual memory (bytes) snapshot=511779401728\n",
      "\tMapper Counters\n",
      "\t\tCalls=999\n",
      "\tReducer Counters\n",
      "\t\tCalls=3830170\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "Running step 2 of 2...\n",
      "  packageJobJar: [] [/opt/hadoop-2.7.3/share/hadoop/tools/lib/hadoop-streaming-2.7.3.jar] /tmp/streamjob2173917651311599920.jar tmpDir=null\n",
      "  Timeline service address: http://rm-ia.s3s.altiscale.com:8188/ws/v1/timeline/\n",
      "  Connecting to ResourceManager at rm-ia.s3s.altiscale.com/10.251.255.108:8032\n",
      "  Connecting to Application History server at rm-ia.s3s.altiscale.com/10.251.255.108:10200\n",
      "  Timeline service address: http://rm-ia.s3s.altiscale.com:8188/ws/v1/timeline/\n",
      "  Connecting to ResourceManager at rm-ia.s3s.altiscale.com/10.251.255.108:8032\n",
      "  Connecting to Application History server at rm-ia.s3s.altiscale.com/10.251.255.108:10200\n",
      "  Loaded native gpl library from the embedded binaries\n",
      "  Successfully loaded & initialized native-lzo library [hadoop-lzo rev d62701d4d05dfa6115bbaf8d9dff002df142e62d]\n",
      "  Total input paths to process : 64\n",
      "  number of splits:64\n",
      "  Submitting tokens for job: job_1509050304403_25900\n",
      "  Submitted application application_1509050304403_25900\n",
      "  The url to track the job: http://rm-ia.s3s.altiscale.com:8088/proxy/application_1509050304403_25900/\n",
      "  Running job: job_1509050304403_25900\n",
      "  Job job_1509050304403_25900 running in uber mode : false\n",
      "   map 0% reduce 0%\n",
      "   map 5% reduce 0%\n",
      "   map 8% reduce 0%\n",
      "   map 22% reduce 0%\n",
      "   map 31% reduce 0%\n",
      "   map 33% reduce 0%\n",
      "   map 34% reduce 0%\n",
      "   map 36% reduce 0%\n",
      "   map 38% reduce 0%\n",
      "   map 43% reduce 0%\n",
      "   map 60% reduce 0%\n",
      "   map 80% reduce 0%\n",
      "   map 100% reduce 0%\n",
      "   map 100% reduce 46%\n",
      "   map 100% reduce 55%\n",
      "   map 100% reduce 63%\n",
      "   map 100% reduce 67%\n",
      "   map 100% reduce 68%\n",
      "   map 100% reduce 69%\n",
      "   map 100% reduce 70%\n",
      "   map 100% reduce 71%\n",
      "   map 100% reduce 72%\n",
      "   map 100% reduce 73%\n",
      "   map 100% reduce 74%\n",
      "   map 100% reduce 75%\n",
      "   map 100% reduce 76%\n",
      "   map 100% reduce 77%\n",
      "   map 100% reduce 78%\n",
      "   map 100% reduce 79%\n",
      "   map 100% reduce 80%\n",
      "   map 100% reduce 81%\n",
      "   map 100% reduce 82%\n",
      "   map 100% reduce 83%\n",
      "   map 100% reduce 84%\n",
      "   map 100% reduce 85%\n",
      "   map 100% reduce 86%\n",
      "   map 100% reduce 87%\n",
      "   map 100% reduce 88%\n",
      "   map 100% reduce 89%\n",
      "   map 100% reduce 90%\n",
      "   map 100% reduce 91%\n",
      "   map 100% reduce 92%\n",
      "   map 100% reduce 93%\n",
      "   map 100% reduce 94%\n",
      "   map 100% reduce 95%\n",
      "   map 100% reduce 96%\n",
      "   map 100% reduce 97%\n",
      "   map 100% reduce 98%\n",
      "   map 100% reduce 99%\n",
      "   map 100% reduce 100%\n",
      "  Job job_1509050304403_25900 completed successfully\n",
      "  Output directory: hdfs:///user/lteo01/tests\n",
      "Counters: 51\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=447956582\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=426232864\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=72269900\n",
      "\t\tFILE: Number of bytes written=207659422\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=447968230\n",
      "\t\tHDFS: Number of bytes written=426232864\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of read operations=195\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tIntermediate Reducer Counters\n",
      "\t\tCalls=80545\n",
      "\tJob Counters \n",
      "\t\tKilled map tasks=2\n",
      "\t\tLaunched map tasks=66\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tRack-local map tasks=66\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=2917498368\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=334182400\n",
      "\t\tTotal time spent by all map tasks (ms)=1899413\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=5698239\n",
      "\t\tTotal time spent by all reduce tasks (ms)=130540\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=652700\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=1899413\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=130540\n",
      "\tMap-Reduce Framework\n",
      "\t\tCPU time spent (ms)=414310\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tGC time elapsed (ms)=19275\n",
      "\t\tInput split bytes=11648\n",
      "\t\tMap input records=3830170\n",
      "\t\tMap output bytes=452357572\n",
      "\t\tMap output materialized bytes=126796523\n",
      "\t\tMap output records=3830170\n",
      "\t\tMerged Map outputs=64\n",
      "\t\tPhysical memory (bytes) snapshot=52568535040\n",
      "\t\tReduce input groups=3830170\n",
      "\t\tReduce input records=3830170\n",
      "\t\tReduce output records=3830170\n",
      "\t\tReduce shuffle bytes=126796523\n",
      "\t\tShuffled Maps =64\n",
      "\t\tSpilled Records=7660340\n",
      "\t\tTotal committed heap usage (bytes)=57847840768\n",
      "\t\tVirtual memory (bytes) snapshot=223555067904\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "Removing HDFS temp directory hdfs:///user/lteo01/tmp/mrjob/similarity_v2.lteo01.20180306.160142.297158...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing temp directory /tmp/similarity_v2.lteo01.20180306.160142.297158...\r\n",
      "\r\n",
      "real\t5m2.989s\r\n",
      "user\t0m48.174s\r\n",
      "sys\t0m2.341s\r\n"
     ]
    }
   ],
   "source": [
    "# Run in Hadoop\n",
    "\n",
    "OUTPUT_PATH = os.path.join(OUTPUT_PATH_BASE,'tests')\n",
    "!hadoop fs -rm -r {OUTPUT_PATH}\n",
    "\n",
    "!time python similarity_v2.py \\\n",
    "        -r hadoop google_index_20 \\\n",
    "        --output-dir={OUTPUT_PATH} \\\n",
    "        --no-output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\t[\"inspect - traders\", 1.0, 1.0, 1.0, 1.0]\r\n",
      "1.0\t[\"ceremonies - petitions\", 1.0, 1.0, 1.0, 1.0]\r\n",
      "1.0\t[\"settlements - stressed\", 1.0, 1.0, 1.0, 1.0]\r\n",
      "1.0\t[\"conveying - interfering\", 1.0, 1.0, 1.0, 1.0]\r\n",
      "1.0\t[\"shareholders - stockholders\", 1.0, 1.0, 1.0, 1.0]\r\n",
      "1.0\t[\"recognizing - waged\", 1.0, 1.0, 1.0, 1.0]\r\n",
      "1.0\t[\"believer - populace\", 1.0, 1.0, 1.0, 1.0]\r\n",
      "1.0\t[\"constructing - irrigation\", 1.0, 1.0, 1.0, 1.0]\r\n",
      "1.0\t[\"letting - ornament\", 1.0, 1.0, 1.0, 1.0]\r\n",
      "1.0\t[\"applicability - disadvantages\", 1.0, 1.0, 1.0, 1.0]\r\n",
      "cat: write error: Broken pipe\r\n"
     ]
    }
   ],
   "source": [
    "!hadoop fs -cat {OUTPUT_PATH}/* > google_similarities_2\n",
    "!cat google_similarities_2 | head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "############################################\n",
    "# Pretty print systems tests\n",
    "# Note: adjust print formatting if you need to\n",
    "############################################\n",
    "\n",
    "import json\n",
    "for i in range(2,3):\n",
    "    print '—'*110\n",
    "    print \"Systems test \",i,\" - Similarity measures\"\n",
    "    print '—'*110\n",
    "    print \"{0:>15} |{1:>15} |{2:>15} |{3:>15} |{4:>15} |{5:>15}\".format(\n",
    "    \"average\", \"pair\", \"cosine\", \"jaccard\", \"overlap\", \"dice\")\n",
    "    print '-'*110\n",
    "\n",
    "    with open(\"google_similarities_\"+str(i),\"r\") as f:\n",
    "        lines = f.readlines()\n",
    "        for line in lines:\n",
    "            line = line.strip()\n",
    "            avg,stripe = line.split(\"\\t\")\n",
    "            stripe = json.loads(stripe)\n",
    "            \n",
    "            print \"{0:>15f} |{1:>15} |{2:>15f} |{3:>15f} |{4:>15f} |{5:>15f}\".format(float(avg),\n",
    "                stripe[0], float(stripe[1]), float(stripe[2]), float(stripe[3]), float(stripe[4]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HW5.7.3 Running the full dataset on Altiscale\n",
    "\n",
    "Please contact the TAs for approval after obtaining results from 5.7.2. We have ran into issues in the past where the clusters froze because people did not test their code on a smaller dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting buildStripes_stopwords.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile buildStripes_stopwords.py\n",
    "#!~/opt/anaconda2/bin/python\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "from __future__ import division\n",
    "import re\n",
    "import mrjob\n",
    "import json\n",
    "from mrjob.protocol import RawProtocol\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "import itertools\n",
    "\n",
    "class MRbuildStripes(MRJob):\n",
    "    \n",
    "    SORT_VALUES = True\n",
    "    \n",
    "    def steps(self):\n",
    "        return [\n",
    "            MRStep(\n",
    "                mapper_init=self.mapper_init,\n",
    "                mapper=self.mapper,\n",
    "                reducer=self.reducer,\n",
    "                jobconf = {\n",
    "                    \"mapreduce.job.reduces\": \"64\",\n",
    "                    \"mapreduce.job.maps\": \"64\",\n",
    "#                     \"SORT_VALUES\":True\n",
    "                }\n",
    "            ),\n",
    "            MRStep(\n",
    "                reducer=self.reducer_2,\n",
    "                jobconf = {\n",
    "                    \"mapreduce.job.reduces\": \"1\",\n",
    "                    \"SORT_VALUES\":True\n",
    "                }\n",
    "            )\n",
    "            \n",
    "        ]\n",
    "    \n",
    "    def mapper_init(self):\n",
    "        self.idx = 9001  # To define when feature set starts\n",
    "        self.filename = 'ten_thousand_FULL.json'\n",
    "        \n",
    "        self.top_words = []\n",
    "        self.features = []\n",
    "        #with open('features_20.json', 'r') as infile:\n",
    "        #    self.features = json.loads(infile.read())\n",
    "        with open(self.filename, 'r') as infile:\n",
    "            self.top_words = json.loads(infile.read())\n",
    "            self.features =self.top_words[self.idx:]\n",
    "\n",
    "    def mapper(self, _, line):\n",
    "        fields = line.lower().strip(\"\\n\").split(\"\\t\")\n",
    "        words = fields[0].split(\" \")\n",
    "        occurrence_count = int(fields[1])\n",
    "        filtered_words = [word.decode('utf-8', 'ignore') for word in words if word.decode('utf-8', 'ignore') in self.top_words]\n",
    "        for subset in itertools.combinations(sorted(set(filtered_words)), 2):\n",
    "            if subset[0] in self.top_words and subset[1] in self.features:\n",
    "                yield subset[0], (subset[1], occurrence_count)\n",
    "            if subset[1] in self.top_words and subset[0] in self.features:\n",
    "                yield subset[1], (subset[0], occurrence_count)\n",
    "    \n",
    "    def reducer(self, word, occurrence_counts):\n",
    "        stripe = {}\n",
    "        for other_word, occurrence_count in occurrence_counts:\n",
    "            stripe[other_word] = stripe.get(other_word,0)+occurrence_count\n",
    "        yield word, stripe\n",
    "    \n",
    "    def reducer_2(self, key, values):\n",
    "        yield str(key), list(values)[0]\n",
    "        \n",
    "if __name__ == '__main__':\n",
    "    MRbuildStripes.run()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18/03/06 16:09:25 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 5760 minutes, Emptier interval = 360 minutes.\n",
      "Moved: 'hdfs://nn-ia.s3s.altiscale.com:8020/user/lteo01/tests' to trash at: hdfs://nn-ia.s3s.altiscale.com:8020/user/lteo01/.Trash/Current\n",
      "Using configs in /home/lteo01/.mrjob.conf\n",
      "Looking for hadoop binary in /opt/hadoop/bin...\n",
      "Found hadoop binary: /opt/hadoop/bin/hadoop\n",
      "Using Hadoop version 2.7.3\n",
      "Looking for Hadoop streaming jar in /opt/hadoop...\n",
      "Found Hadoop streaming jar: /opt/hadoop/share/hadoop/tools/lib/hadoop-streaming-2.7.3.jar\n",
      "Creating temp directory /tmp/buildStripes_stopwords.lteo01.20180306.160925.905278\n",
      "Copying local files to hdfs:///user/lteo01/tmp/mrjob/buildStripes_stopwords.lteo01.20180306.160925.905278/files/...\n",
      "Running step 1 of 2...\n",
      "  packageJobJar: [] [/opt/hadoop-2.7.3/share/hadoop/tools/lib/hadoop-streaming-2.7.3.jar] /tmp/streamjob3862403407545034719.jar tmpDir=null\n",
      "  Timeline service address: http://rm-ia.s3s.altiscale.com:8188/ws/v1/timeline/\n",
      "  Connecting to ResourceManager at rm-ia.s3s.altiscale.com/10.251.255.108:8032\n",
      "  Connecting to Application History server at rm-ia.s3s.altiscale.com/10.251.255.108:10200\n",
      "  Timeline service address: http://rm-ia.s3s.altiscale.com:8188/ws/v1/timeline/\n",
      "  Connecting to ResourceManager at rm-ia.s3s.altiscale.com/10.251.255.108:8032\n",
      "  Connecting to Application History server at rm-ia.s3s.altiscale.com/10.251.255.108:10200\n",
      "  Loaded native gpl library from the embedded binaries\n",
      "  Successfully loaded & initialized native-lzo library [hadoop-lzo rev d62701d4d05dfa6115bbaf8d9dff002df142e62d]\n",
      "  Total input paths to process : 190\n",
      "  number of splits:190\n",
      "  Submitting tokens for job: job_1509050304403_25913\n",
      "  Submitted application application_1509050304403_25913\n",
      "  The url to track the job: http://rm-ia.s3s.altiscale.com:8088/proxy/application_1509050304403_25913/\n",
      "  Running job: job_1509050304403_25913\n",
      "  Job job_1509050304403_25913 running in uber mode : false\n",
      "   map 0% reduce 0%\n",
      "   map 1% reduce 0%\n",
      "   map 2% reduce 0%\n",
      "   map 3% reduce 0%\n",
      "   map 4% reduce 0%\n",
      "   map 5% reduce 0%\n",
      "   map 6% reduce 0%\n",
      "   map 7% reduce 0%\n",
      "   map 8% reduce 0%\n",
      "   map 9% reduce 0%\n",
      "   map 10% reduce 0%\n",
      "   map 11% reduce 0%\n",
      "   map 12% reduce 0%\n",
      "   map 13% reduce 0%\n",
      "   map 14% reduce 0%\n",
      "   map 15% reduce 0%\n",
      "   map 16% reduce 0%\n",
      "   map 17% reduce 0%\n",
      "   map 18% reduce 0%\n",
      "   map 19% reduce 0%\n",
      "   map 20% reduce 0%\n",
      "   map 21% reduce 0%\n",
      "   map 22% reduce 0%\n",
      "   map 23% reduce 0%\n",
      "   map 24% reduce 0%\n",
      "   map 25% reduce 0%\n",
      "   map 26% reduce 0%\n",
      "   map 27% reduce 0%\n",
      "   map 28% reduce 0%\n",
      "   map 29% reduce 0%\n",
      "   map 30% reduce 0%\n",
      "   map 31% reduce 0%\n",
      "   map 32% reduce 0%\n",
      "   map 33% reduce 0%\n",
      "   map 34% reduce 0%\n",
      "   map 35% reduce 0%\n",
      "   map 36% reduce 0%\n",
      "   map 37% reduce 0%\n",
      "   map 38% reduce 0%\n",
      "   map 39% reduce 0%\n",
      "   map 40% reduce 0%\n",
      "   map 41% reduce 0%\n",
      "   map 42% reduce 0%\n",
      "   map 43% reduce 0%\n",
      "   map 44% reduce 0%\n",
      "   map 45% reduce 0%\n",
      "   map 46% reduce 0%\n",
      "   map 47% reduce 0%\n",
      "   map 48% reduce 0%\n",
      "   map 49% reduce 0%\n",
      "   map 50% reduce 0%\n",
      "   map 51% reduce 0%\n",
      "   map 52% reduce 0%\n",
      "   map 53% reduce 0%\n",
      "   map 54% reduce 0%\n",
      "   map 55% reduce 0%\n",
      "   map 56% reduce 0%\n",
      "   map 57% reduce 0%\n",
      "   map 58% reduce 0%\n",
      "   map 59% reduce 0%\n",
      "   map 60% reduce 0%\n",
      "   map 61% reduce 0%\n",
      "   map 62% reduce 0%\n",
      "   map 63% reduce 0%\n",
      "   map 64% reduce 0%\n",
      "   map 65% reduce 0%\n",
      "   map 66% reduce 0%\n",
      "   map 67% reduce 0%\n",
      "   map 68% reduce 0%\n",
      "   map 69% reduce 0%\n",
      "   map 70% reduce 0%\n",
      "   map 71% reduce 0%\n",
      "   map 72% reduce 0%\n",
      "   map 73% reduce 0%\n",
      "   map 74% reduce 0%\n",
      "   map 75% reduce 0%\n",
      "   map 76% reduce 0%\n",
      "   map 77% reduce 0%\n",
      "   map 78% reduce 0%\n",
      "   map 79% reduce 0%\n",
      "   map 80% reduce 0%\n",
      "   map 81% reduce 0%\n",
      "   map 82% reduce 0%\n",
      "   map 83% reduce 0%\n",
      "   map 84% reduce 0%\n",
      "   map 85% reduce 0%\n",
      "   map 86% reduce 0%\n",
      "   map 87% reduce 0%\n",
      "   map 88% reduce 0%\n",
      "   map 89% reduce 0%\n",
      "   map 90% reduce 0%\n",
      "   map 91% reduce 0%\n",
      "   map 92% reduce 0%\n",
      "   map 93% reduce 0%\n",
      "   map 94% reduce 0%\n",
      "   map 95% reduce 0%\n",
      "   map 96% reduce 0%\n",
      "   map 97% reduce 0%\n",
      "   map 98% reduce 0%\n",
      "   map 99% reduce 0%\n",
      "   map 100% reduce 0%\n",
      "   map 100% reduce 3%\n",
      "   map 100% reduce 6%\n",
      "   map 100% reduce 9%\n",
      "   map 100% reduce 11%\n",
      "   map 100% reduce 16%\n",
      "   map 100% reduce 21%\n",
      "   map 100% reduce 28%\n",
      "   map 100% reduce 31%\n",
      "   map 100% reduce 34%\n",
      "   map 100% reduce 38%\n",
      "   map 100% reduce 45%\n",
      "   map 100% reduce 52%\n",
      "   map 100% reduce 58%\n",
      "   map 100% reduce 65%\n",
      "   map 100% reduce 70%\n",
      "   map 100% reduce 76%\n",
      "   map 100% reduce 83%\n",
      "   map 100% reduce 89%\n",
      "   map 100% reduce 100%\n",
      "  Job job_1509050304403_25913 completed successfully\n",
      "  Output directory: hdfs:///user/lteo01/tmp/mrjob/buildStripes_stopwords.lteo01.20180306.160925.905278/step-output/0000\n",
      "Counters: 52\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=2156069116\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=9258628\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=12917939\n",
      "\t\tFILE: Number of bytes written=75064037\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=2156099976\n",
      "\t\tHDFS: Number of bytes written=9258628\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of read operations=762\n",
      "\t\tHDFS: Number of write operations=128\n",
      "\tJob Counters \n",
      "\t\tKilled map tasks=13\n",
      "\t\tKilled reduce tasks=1\n",
      "\t\tLaunched map tasks=203\n",
      "\t\tLaunched reduce tasks=64\n",
      "\t\tOther local map tasks=2\n",
      "\t\tRack-local map tasks=201\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=286394612736\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=2048793600\n",
      "\t\tTotal time spent by all map tasks (ms)=186454826\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=559364478\n",
      "\t\tTotal time spent by all reduce tasks (ms)=800310\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=4001550\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=186454826\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=800310\n",
      "\tMap-Reduce Framework\n",
      "\t\tCPU time spent (ms)=104577240\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tGC time elapsed (ms)=156496\n",
      "\t\tInput split bytes=30860\n",
      "\t\tMap input records=58682266\n",
      "\t\tMap output bytes=43619664\n",
      "\t\tMap output materialized bytes=28213364\n",
      "\t\tMap output records=1596658\n",
      "\t\tMerged Map outputs=12160\n",
      "\t\tPhysical memory (bytes) snapshot=175376351232\n",
      "\t\tReduce input groups=1453930\n",
      "\t\tReduce input records=1596658\n",
      "\t\tReduce output records=9998\n",
      "\t\tReduce shuffle bytes=28213364\n",
      "\t\tShuffled Maps =12160\n",
      "\t\tSpilled Records=3193316\n",
      "\t\tTotal committed heap usage (bytes)=193317568512\n",
      "\t\tVirtual memory (bytes) snapshot=942978039808\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "Running step 2 of 2...\n",
      "  packageJobJar: [] [/opt/hadoop-2.7.3/share/hadoop/tools/lib/hadoop-streaming-2.7.3.jar] /tmp/streamjob3379446092713327852.jar tmpDir=null\n",
      "  Timeline service address: http://rm-ia.s3s.altiscale.com:8188/ws/v1/timeline/\n",
      "  Connecting to ResourceManager at rm-ia.s3s.altiscale.com/10.251.255.108:8032\n",
      "  Connecting to Application History server at rm-ia.s3s.altiscale.com/10.251.255.108:10200\n",
      "  Timeline service address: http://rm-ia.s3s.altiscale.com:8188/ws/v1/timeline/\n",
      "  Connecting to ResourceManager at rm-ia.s3s.altiscale.com/10.251.255.108:8032\n",
      "  Connecting to Application History server at rm-ia.s3s.altiscale.com/10.251.255.108:10200\n",
      "  Loaded native gpl library from the embedded binaries\n",
      "  Successfully loaded & initialized native-lzo library [hadoop-lzo rev d62701d4d05dfa6115bbaf8d9dff002df142e62d]\n",
      "  Total input paths to process : 64\n",
      "  number of splits:64\n",
      "  Submitting tokens for job: job_1509050304403_25936\n",
      "  Submitted application application_1509050304403_25936\n",
      "  The url to track the job: http://rm-ia.s3s.altiscale.com:8088/proxy/application_1509050304403_25936/\n",
      "  Running job: job_1509050304403_25936\n",
      "  Job job_1509050304403_25936 running in uber mode : false\n",
      "   map 0% reduce 0%\n",
      "   map 3% reduce 0%\n",
      "   map 27% reduce 0%\n",
      "   map 33% reduce 0%\n",
      "   map 38% reduce 0%\n",
      "   map 52% reduce 0%\n",
      "   map 63% reduce 0%\n",
      "   map 100% reduce 0%\n",
      "   map 100% reduce 100%\n",
      "  Job job_1509050304403_25936 completed successfully\n",
      "  Output directory: hdfs:///user/lteo01/tests\n",
      "Counters: 50\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=9258628\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=9258628\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=4480433\n",
      "\t\tFILE: Number of bytes written=17655246\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=9270852\n",
      "\t\tHDFS: Number of bytes written=9258628\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of read operations=195\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tKilled map tasks=1\n",
      "\t\tLaunched map tasks=65\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tRack-local map tasks=65\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=2165245440\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=17280000\n",
      "\t\tTotal time spent by all map tasks (ms)=1409665\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=4228995\n",
      "\t\tTotal time spent by all reduce tasks (ms)=6750\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=33750\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=1409665\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=6750\n",
      "\tMap-Reduce Framework\n",
      "\t\tCPU time spent (ms)=75660\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tGC time elapsed (ms)=13527\n",
      "\t\tInput split bytes=12224\n",
      "\t\tMap input records=9998\n",
      "\t\tMap output bytes=9285772\n",
      "\t\tMap output materialized bytes=4564719\n",
      "\t\tMap output records=9998\n",
      "\t\tMerged Map outputs=64\n",
      "\t\tPhysical memory (bytes) snapshot=51156398080\n",
      "\t\tReduce input groups=9998\n",
      "\t\tReduce input records=9998\n",
      "\t\tReduce output records=9998\n",
      "\t\tReduce shuffle bytes=4564719\n",
      "\t\tShuffled Maps =64\n",
      "\t\tSpilled Records=19996\n",
      "\t\tTotal committed heap usage (bytes)=57151586304\n",
      "\t\tVirtual memory (bytes) snapshot=223489622016\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "Removing HDFS temp directory hdfs:///user/lteo01/tmp/mrjob/buildStripes_stopwords.lteo01.20180306.160925.905278...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing temp directory /tmp/buildStripes_stopwords.lteo01.20180306.160925.905278...\r\n",
      "\r\n",
      "real\t30m3.979s\r\n",
      "user\t0m55.912s\r\n",
      "sys\t0m2.750s\r\n"
     ]
    }
   ],
   "source": [
    "# Run in Hadoop\n",
    "\n",
    "OUTPUT_PATH = os.path.join(OUTPUT_PATH_BASE,'tests')\n",
    "!hadoop fs -rm -r {OUTPUT_PATH}\n",
    "\n",
    "!time python buildStripes_stopwords.py \\\n",
    "        -r hadoop hdfs://{FULL_DATA}/* \\\n",
    "        --file ten_thousand_FULL.json \\\n",
    "        --output-dir={OUTPUT_PATH} \\\n",
    "        --no-output\n",
    "            \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "!hadoop fs -cat {OUTPUT_PATH}/* > google_stripes_FULL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18/03/06 16:43:40 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 5760 minutes, Emptier interval = 360 minutes.\n",
      "Moved: 'hdfs://nn-ia.s3s.altiscale.com:8020/user/lteo01/tests' to trash at: hdfs://nn-ia.s3s.altiscale.com:8020/user/lteo01/.Trash/Current\n",
      "Using configs in /home/lteo01/.mrjob.conf\n",
      "Looking for hadoop binary in /opt/hadoop/bin...\n",
      "Found hadoop binary: /opt/hadoop/bin/hadoop\n",
      "Using Hadoop version 2.7.3\n",
      "Looking for Hadoop streaming jar in /opt/hadoop...\n",
      "Found Hadoop streaming jar: /opt/hadoop/share/hadoop/tools/lib/hadoop-streaming-2.7.3.jar\n",
      "Creating temp directory /tmp/invertedIndex_v2.lteo01.20180306.164340.912983\n",
      "Copying local files to hdfs:///user/lteo01/tmp/mrjob/invertedIndex_v2.lteo01.20180306.164340.912983/files/...\n",
      "Running step 1 of 1...\n",
      "  packageJobJar: [] [/opt/hadoop-2.7.3/share/hadoop/tools/lib/hadoop-streaming-2.7.3.jar] /tmp/streamjob6369264445573276562.jar tmpDir=null\n",
      "  Timeline service address: http://rm-ia.s3s.altiscale.com:8188/ws/v1/timeline/\n",
      "  Connecting to ResourceManager at rm-ia.s3s.altiscale.com/10.251.255.108:8032\n",
      "  Connecting to Application History server at rm-ia.s3s.altiscale.com/10.251.255.108:10200\n",
      "  Timeline service address: http://rm-ia.s3s.altiscale.com:8188/ws/v1/timeline/\n",
      "  Connecting to ResourceManager at rm-ia.s3s.altiscale.com/10.251.255.108:8032\n",
      "  Connecting to Application History server at rm-ia.s3s.altiscale.com/10.251.255.108:10200\n",
      "  Loaded native gpl library from the embedded binaries\n",
      "  Successfully loaded & initialized native-lzo library [hadoop-lzo rev d62701d4d05dfa6115bbaf8d9dff002df142e62d]\n",
      "  Total input paths to process : 1\n",
      "  number of splits:2\n",
      "  Submitting tokens for job: job_1509050304403_25944\n",
      "  Submitted application application_1509050304403_25944\n",
      "  The url to track the job: http://rm-ia.s3s.altiscale.com:8088/proxy/application_1509050304403_25944/\n",
      "  Running job: job_1509050304403_25944\n",
      "  Job job_1509050304403_25944 running in uber mode : false\n",
      "   map 0% reduce 0%\n",
      "   map 67% reduce 0%\n",
      "   map 100% reduce 0%\n",
      "   map 100% reduce 93%\n",
      "   map 100% reduce 100%\n",
      "  Job job_1509050304403_25944 completed successfully\n",
      "  Output directory: hdfs:///user/lteo01/tests\n",
      "Counters: 51\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=9347906\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=9483978\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=6175125\n",
      "\t\tFILE: Number of bytes written=12516020\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=9348272\n",
      "\t\tHDFS: Number of bytes written=9483978\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of read operations=9\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tRack-local map tasks=2\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=37146624\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=34897920\n",
      "\t\tTotal time spent by all map tasks (ms)=24184\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=72552\n",
      "\t\tTotal time spent by all reduce tasks (ms)=13632\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=68160\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=24184\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=13632\n",
      "\tMap-Reduce Framework\n",
      "\t\tCPU time spent (ms)=19090\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tGC time elapsed (ms)=435\n",
      "\t\tInput split bytes=366\n",
      "\t\tMap input records=9998\n",
      "\t\tMap output bytes=17762835\n",
      "\t\tMap output materialized bytes=5944236\n",
      "\t\tMap output records=555336\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tPhysical memory (bytes) snapshot=2009055232\n",
      "\t\tReduce input groups=555336\n",
      "\t\tReduce input records=555336\n",
      "\t\tReduce output records=999\n",
      "\t\tReduce shuffle bytes=5944236\n",
      "\t\tShuffled Maps =2\n",
      "\t\tSpilled Records=1110672\n",
      "\t\tTotal committed heap usage (bytes)=2192048128\n",
      "\t\tVirtual memory (bytes) snapshot=11412049920\n",
      "\tMapper Counters\n",
      "\t\tCalls=9998\n",
      "\tReducer Counters\n",
      "\t\tCalls=999\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "Removing HDFS temp directory hdfs:///user/lteo01/tmp/mrjob/invertedIndex_v2.lteo01.20180306.164340.912983...\n",
      "Removing temp directory /tmp/invertedIndex_v2.lteo01.20180306.164340.912983...\n",
      "\n",
      "real\t1m3.077s\n",
      "user\t0m35.738s\n",
      "sys\t0m1.908s\n"
     ]
    }
   ],
   "source": [
    "# Run in Hadoop\n",
    "\n",
    "OUTPUT_PATH = os.path.join(OUTPUT_PATH_BASE,'tests')\n",
    "!hadoop fs -rm -r {OUTPUT_PATH}\n",
    "\n",
    "!time python invertedIndex_v2.py \\\n",
    "        -r hadoop google_stripes_FULL \\\n",
    "        --output-dir={OUTPUT_PATH} \\\n",
    "        --no-output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "!hadoop fs -cat {OUTPUT_PATH}/* > google_index_FULL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18/03/06 16:44:47 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 5760 minutes, Emptier interval = 360 minutes.\n",
      "Moved: 'hdfs://nn-ia.s3s.altiscale.com:8020/user/lteo01/tests' to trash at: hdfs://nn-ia.s3s.altiscale.com:8020/user/lteo01/.Trash/Current\n",
      "Using configs in /home/lteo01/.mrjob.conf\n",
      "Looking for hadoop binary in /opt/hadoop/bin...\n",
      "Found hadoop binary: /opt/hadoop/bin/hadoop\n",
      "Using Hadoop version 2.7.3\n",
      "Looking for Hadoop streaming jar in /opt/hadoop...\n",
      "Found Hadoop streaming jar: /opt/hadoop/share/hadoop/tools/lib/hadoop-streaming-2.7.3.jar\n",
      "Creating temp directory /tmp/similarity_v2.lteo01.20180306.164448.602492\n",
      "Copying local files to hdfs:///user/lteo01/tmp/mrjob/similarity_v2.lteo01.20180306.164448.602492/files/...\n",
      "Running step 1 of 2...\n",
      "  packageJobJar: [] [/opt/hadoop-2.7.3/share/hadoop/tools/lib/hadoop-streaming-2.7.3.jar] /tmp/streamjob6219423789189061735.jar tmpDir=null\n",
      "  Timeline service address: http://rm-ia.s3s.altiscale.com:8188/ws/v1/timeline/\n",
      "  Connecting to ResourceManager at rm-ia.s3s.altiscale.com/10.251.255.108:8032\n",
      "  Connecting to Application History server at rm-ia.s3s.altiscale.com/10.251.255.108:10200\n",
      "  Timeline service address: http://rm-ia.s3s.altiscale.com:8188/ws/v1/timeline/\n",
      "  Connecting to ResourceManager at rm-ia.s3s.altiscale.com/10.251.255.108:8032\n",
      "  Connecting to Application History server at rm-ia.s3s.altiscale.com/10.251.255.108:10200\n",
      "  Loaded native gpl library from the embedded binaries\n",
      "  Successfully loaded & initialized native-lzo library [hadoop-lzo rev d62701d4d05dfa6115bbaf8d9dff002df142e62d]\n",
      "  Total input paths to process : 1\n",
      "  number of splits:64\n",
      "  Submitting tokens for job: job_1509050304403_25948\n",
      "  Submitted application application_1509050304403_25948\n",
      "  The url to track the job: http://rm-ia.s3s.altiscale.com:8088/proxy/application_1509050304403_25948/\n",
      "  Running job: job_1509050304403_25948\n",
      "  Job job_1509050304403_25948 running in uber mode : false\n",
      "   map 0% reduce 0%\n",
      "   map 1% reduce 0%\n",
      "   map 2% reduce 0%\n",
      "   map 5% reduce 0%\n",
      "   map 6% reduce 0%\n",
      "   map 14% reduce 0%\n",
      "   map 21% reduce 0%\n",
      "   map 22% reduce 0%\n",
      "   map 23% reduce 0%\n",
      "   map 24% reduce 0%\n",
      "   map 26% reduce 0%\n",
      "   map 32% reduce 0%\n",
      "   map 35% reduce 0%\n",
      "   map 41% reduce 0%\n",
      "   map 59% reduce 0%\n",
      "   map 63% reduce 0%\n",
      "   map 64% reduce 0%\n",
      "   map 65% reduce 0%\n",
      "   map 66% reduce 0%\n",
      "   map 67% reduce 0%\n",
      "   map 68% reduce 0%\n",
      "   map 70% reduce 0%\n",
      "   map 71% reduce 0%\n",
      "   map 72% reduce 0%\n",
      "   map 73% reduce 0%\n",
      "   map 74% reduce 0%\n",
      "   map 75% reduce 0%\n",
      "   map 76% reduce 0%\n",
      "   map 77% reduce 0%\n",
      "   map 78% reduce 0%\n",
      "   map 79% reduce 0%\n",
      "   map 80% reduce 0%\n",
      "   map 81% reduce 0%\n",
      "   map 82% reduce 0%\n",
      "   map 83% reduce 0%\n",
      "   map 84% reduce 0%\n",
      "   map 85% reduce 0%\n",
      "   map 86% reduce 0%\n",
      "   map 87% reduce 0%\n",
      "   map 88% reduce 0%\n",
      "   map 89% reduce 0%\n",
      "   map 90% reduce 0%\n",
      "   map 91% reduce 0%\n",
      "   map 92% reduce 0%\n",
      "   map 93% reduce 0%\n",
      "   map 95% reduce 0%\n",
      "   map 96% reduce 0%\n",
      "   map 98% reduce 0%\n",
      "   map 99% reduce 0%\n",
      "   map 100% reduce 0%\n",
      "   map 100% reduce 2%\n",
      "   map 100% reduce 4%\n",
      "   map 100% reduce 5%\n",
      "   map 100% reduce 12%\n",
      "   map 100% reduce 19%\n",
      "   map 100% reduce 21%\n",
      "   map 100% reduce 26%\n",
      "   map 100% reduce 29%\n",
      "   map 100% reduce 33%\n",
      "   map 100% reduce 36%\n",
      "   map 100% reduce 41%\n",
      "   map 100% reduce 44%\n",
      "   map 100% reduce 45%\n",
      "   map 100% reduce 47%\n",
      "   map 100% reduce 52%\n",
      "   map 100% reduce 55%\n",
      "   map 100% reduce 57%\n",
      "   map 100% reduce 62%\n",
      "   map 100% reduce 63%\n",
      "   map 100% reduce 64%\n",
      "   map 100% reduce 65%\n",
      "   map 100% reduce 67%\n",
      "   map 100% reduce 69%\n",
      "   map 100% reduce 70%\n",
      "   map 100% reduce 71%\n",
      "   map 100% reduce 72%\n",
      "   map 100% reduce 73%\n",
      "   map 100% reduce 74%\n",
      "   map 100% reduce 75%\n",
      "   map 100% reduce 76%\n",
      "   map 100% reduce 77%\n",
      "   map 100% reduce 78%\n",
      "   map 100% reduce 79%\n",
      "   map 100% reduce 80%\n",
      "   map 100% reduce 81%\n",
      "   map 100% reduce 82%\n",
      "   map 100% reduce 83%\n",
      "   map 100% reduce 84%\n",
      "   map 100% reduce 85%\n",
      "   map 100% reduce 86%\n",
      "   map 100% reduce 87%\n",
      "   map 100% reduce 88%\n",
      "   map 100% reduce 89%\n",
      "   map 100% reduce 90%\n",
      "   map 100% reduce 91%\n",
      "   map 100% reduce 92%\n",
      "   map 100% reduce 93%\n",
      "   map 100% reduce 94%\n",
      "   map 100% reduce 95%\n",
      "   map 100% reduce 96%\n",
      "   map 100% reduce 97%\n",
      "   map 100% reduce 98%\n",
      "   map 100% reduce 99%\n",
      "   map 100% reduce 100%\n",
      "  Job job_1509050304403_25948 completed successfully\n",
      "  Output directory: hdfs:///user/lteo01/tmp/mrjob/similarity_v2.lteo01.20180306.164448.602492/step-output/0000\n",
      "Counters: 51\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=16646186\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=3568988772\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=713095396\n",
      "\t\tFILE: Number of bytes written=2846828005\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=16657514\n",
      "\t\tHDFS: Number of bytes written=3568988772\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of read operations=384\n",
      "\t\tHDFS: Number of write operations=128\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=64\n",
      "\t\tLaunched reduce tasks=64\n",
      "\t\tRack-local map tasks=64\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=19193599488\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=20034265600\n",
      "\t\tTotal time spent by all map tasks (ms)=12495833\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=37487499\n",
      "\t\tTotal time spent by all reduce tasks (ms)=7825885\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=39129425\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=12495833\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=7825885\n",
      "\tMap-Reduce Framework\n",
      "\t\tCPU time spent (ms)=14309520\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tGC time elapsed (ms)=95221\n",
      "\t\tInput split bytes=11328\n",
      "\t\tMap input records=999\n",
      "\t\tMap output bytes=5758697974\n",
      "\t\tMap output materialized bytes=2116755669\n",
      "\t\tMap output records=159413716\n",
      "\t\tMerged Map outputs=4096\n",
      "\t\tPhysical memory (bytes) snapshot=83392819200\n",
      "\t\tReduce input groups=28881712\n",
      "\t\tReduce input records=159413716\n",
      "\t\tReduce output records=28881712\n",
      "\t\tReduce shuffle bytes=2116755669\n",
      "\t\tShuffled Maps =4096\n",
      "\t\tSpilled Records=318827432\n",
      "\t\tTotal committed heap usage (bytes)=91265957888\n",
      "\t\tVirtual memory (bytes) snapshot=511691108352\n",
      "\tMapper Counters\n",
      "\t\tCalls=999\n",
      "\tReducer Counters\n",
      "\t\tCalls=28881712\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "Running step 2 of 2...\n",
      "  packageJobJar: [] [/opt/hadoop-2.7.3/share/hadoop/tools/lib/hadoop-streaming-2.7.3.jar] /tmp/streamjob936296176585572327.jar tmpDir=null\n",
      "  Timeline service address: http://rm-ia.s3s.altiscale.com:8188/ws/v1/timeline/\n",
      "  Connecting to ResourceManager at rm-ia.s3s.altiscale.com/10.251.255.108:8032\n",
      "  Connecting to Application History server at rm-ia.s3s.altiscale.com/10.251.255.108:10200\n",
      "  Timeline service address: http://rm-ia.s3s.altiscale.com:8188/ws/v1/timeline/\n",
      "  Connecting to ResourceManager at rm-ia.s3s.altiscale.com/10.251.255.108:8032\n",
      "  Connecting to Application History server at rm-ia.s3s.altiscale.com/10.251.255.108:10200\n",
      "  Loaded native gpl library from the embedded binaries\n",
      "  Successfully loaded & initialized native-lzo library [hadoop-lzo rev d62701d4d05dfa6115bbaf8d9dff002df142e62d]\n",
      "  Total input paths to process : 64\n",
      "  number of splits:64\n",
      "  Submitting tokens for job: job_1509050304403_25958\n",
      "  Submitted application application_1509050304403_25958\n",
      "  The url to track the job: http://rm-ia.s3s.altiscale.com:8088/proxy/application_1509050304403_25958/\n",
      "  Running job: job_1509050304403_25958\n",
      "  Job job_1509050304403_25958 running in uber mode : false\n",
      "   map 0% reduce 0%\n",
      "   map 2% reduce 0%\n",
      "   map 3% reduce 0%\n",
      "   map 6% reduce 0%\n",
      "   map 8% reduce 0%\n",
      "   map 13% reduce 0%\n",
      "   map 21% reduce 0%\n",
      "   map 25% reduce 0%\n",
      "   map 31% reduce 0%\n",
      "   map 52% reduce 0%\n",
      "   map 64% reduce 0%\n",
      "   map 65% reduce 0%\n",
      "   map 66% reduce 0%\n",
      "   map 68% reduce 0%\n",
      "   map 69% reduce 0%\n",
      "   map 70% reduce 0%\n",
      "   map 71% reduce 0%\n",
      "   map 73% reduce 0%\n",
      "   map 80% reduce 0%\n",
      "   map 82% reduce 0%\n",
      "   map 86% reduce 0%\n",
      "   map 90% reduce 0%\n",
      "   map 92% reduce 0%\n",
      "   map 98% reduce 0%\n",
      "   map 100% reduce 0%\n",
      "   map 100% reduce 17%\n",
      "  Task Id : attempt_1509050304403_25958_r_000000_0, Status : FAILED\n",
      "Container [pid=30654,containerID=container_1509050304403_25958_01_000067] is running beyond physical memory limits. Current usage: 2.5 GB of 2.5 GB physical memory used; 4.3 GB of 5.3 GB virtual memory used. Killing container.\n",
      "Dump of the process-tree for container_1509050304403_25958_01_000067 :\n",
      "\t|- PID PPID PGRPID SESSID CMD_NAME USER_MODE_TIME(MILLIS) SYSTEM_TIME(MILLIS) VMEM_USAGE(BYTES) RSSMEM_USAGE(PAGES) FULL_CMD_LINE\n",
      "\t|- 30661 30654 30654 30654 (java) 7178 769 4570681344 665440 /usr/java/default/bin/java -XX:NewRatio=8 -Djava.net.preferIPv4Stack=true -Xmx2540m -Djava.io.tmpdir=/var/storage/nm-sdj1/nm-local/usercache/lteo01/appcache/application_1509050304403_25958/container_1509050304403_25958_01_000067/tmp -Dlog4j.configuration=container-log4j.properties -Dyarn.app.container.log.dir=/var/storage/nm-sdi1/nm-logs/application_1509050304403_25958/container_1509050304403_25958_01_000067 -Dyarn.app.container.log.filesize=0 -Dhadoop.root.logger=INFO,CLA -Dhadoop.root.logfile=syslog -Dyarn.app.mapreduce.shuffle.logger=INFO,shuffleCLA -Dyarn.app.mapreduce.shuffle.logfile=syslog.shuffle -Dyarn.app.mapreduce.shuffle.log.filesize=0 -Dyarn.app.mapreduce.shuffle.log.backups=0 org.apache.hadoop.mapred.YarnChild 10.251.240.185 36105 attempt_1509050304403_25958_r_000000_0 67 \n",
      "\t|- 30654 30652 30654 30654 (bash) 0 0 9490432 522 /bin/bash -c /usr/java/default/bin/java -XX:NewRatio=8 -Djava.net.preferIPv4Stack=true -Xmx2540m -Djava.io.tmpdir=/var/storage/nm-sdj1/nm-local/usercache/lteo01/appcache/application_1509050304403_25958/container_1509050304403_25958_01_000067/tmp -Dlog4j.configuration=container-log4j.properties -Dyarn.app.container.log.dir=/var/storage/nm-sdi1/nm-logs/application_1509050304403_25958/container_1509050304403_25958_01_000067 -Dyarn.app.container.log.filesize=0 -Dhadoop.root.logger=INFO,CLA -Dhadoop.root.logfile=syslog -Dyarn.app.mapreduce.shuffle.logger=INFO,shuffleCLA -Dyarn.app.mapreduce.shuffle.logfile=syslog.shuffle -Dyarn.app.mapreduce.shuffle.log.filesize=0 -Dyarn.app.mapreduce.shuffle.log.backups=0 org.apache.hadoop.mapred.YarnChild 10.251.240.185 36105 attempt_1509050304403_25958_r_000000_0 67 1>/var/storage/nm-sdi1/nm-logs/application_1509050304403_25958/container_1509050304403_25958_01_000067/stdout 2>/var/storage/nm-sdi1/nm-logs/application_1509050304403_25958/container_1509050304403_25958_01_000067/stderr  \n",
      "\n",
      "Container killed on request. Exit code is 143\n",
      "Container exited with a non-zero exit code 143\n",
      "\n",
      "   map 100% reduce 0%\n",
      "   map 100% reduce 17%\n",
      "   map 100% reduce 23%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Task Id : attempt_1509050304403_25958_r_000000_1, Status : FAILED\r\n",
      "Container [pid=6567,containerID=container_1509050304403_25958_01_000068] is running beyond physical memory limits. Current usage: 2.5 GB of 2.5 GB physical memory used; 4.3 GB of 5.3 GB virtual memory used. Killing container.\r\n",
      "Dump of the process-tree for container_1509050304403_25958_01_000068 :\r\n",
      "\t|- PID PPID PGRPID SESSID CMD_NAME USER_MODE_TIME(MILLIS) SYSTEM_TIME(MILLIS) VMEM_USAGE(BYTES) RSSMEM_USAGE(PAGES) FULL_CMD_LINE\r\n",
      "\t|- 6574 6567 6567 6567 (java) 6903 561 4572450816 664683 /usr/java/default/bin/java -XX:NewRatio=8 -Djava.net.preferIPv4Stack=true -Xmx2540m -Djava.io.tmpdir=/var/storage/nm-sdh1/nm-local/usercache/lteo01/appcache/application_1509050304403_25958/container_1509050304403_25958_01_000068/tmp -Dlog4j.configuration=container-log4j.properties -Dyarn.app.container.log.dir=/var/storage/nm-sdd1/nm-logs/application_1509050304403_25958/container_1509050304403_25958_01_000068 -Dyarn.app.container.log.filesize=0 -Dhadoop.root.logger=INFO,CLA -Dhadoop.root.logfile=syslog -Dyarn.app.mapreduce.shuffle.logger=INFO,shuffleCLA -Dyarn.app.mapreduce.shuffle.logfile=syslog.shuffle -Dyarn.app.mapreduce.shuffle.log.filesize=0 -Dyarn.app.mapreduce.shuffle.log.backups=0 org.apache.hadoop.mapred.YarnChild 10.251.240.185 36105 attempt_1509050304403_25958_r_000000_1 68 \r\n",
      "\t|- 6567 6565 6567 6567 (bash) 0 1 9490432 517 /bin/bash -c /usr/java/default/bin/java -XX:NewRatio=8 -Djava.net.preferIPv4Stack=true -Xmx2540m -Djava.io.tmpdir=/var/storage/nm-sdh1/nm-local/usercache/lteo01/appcache/application_1509050304403_25958/container_1509050304403_25958_01_000068/tmp -Dlog4j.configuration=container-log4j.properties -Dyarn.app.container.log.dir=/var/storage/nm-sdd1/nm-logs/application_1509050304403_25958/container_1509050304403_25958_01_000068 -Dyarn.app.container.log.filesize=0 -Dhadoop.root.logger=INFO,CLA -Dhadoop.root.logfile=syslog -Dyarn.app.mapreduce.shuffle.logger=INFO,shuffleCLA -Dyarn.app.mapreduce.shuffle.logfile=syslog.shuffle -Dyarn.app.mapreduce.shuffle.log.filesize=0 -Dyarn.app.mapreduce.shuffle.log.backups=0 org.apache.hadoop.mapred.YarnChild 10.251.240.185 36105 attempt_1509050304403_25958_r_000000_1 68 1>/var/storage/nm-sdd1/nm-logs/application_1509050304403_25958/container_1509050304403_25958_01_000068/stdout 2>/var/storage/nm-sdd1/nm-logs/application_1509050304403_25958/container_1509050304403_25958_01_000068/stderr  \r\n",
      "\r\n",
      "Container killed on request. Exit code is 143\r\n",
      "Container exited with a non-zero exit code 143\r\n",
      "\r\n",
      "   map 100% reduce 0%\r\n",
      "   map 100% reduce 17%\r\n",
      "  Task Id : attempt_1509050304403_25958_r_000000_2, Status : FAILED\r\n",
      "Container [pid=6827,containerID=container_1509050304403_25958_01_000069] is running beyond physical memory limits. Current usage: 2.5 GB of 2.5 GB physical memory used; 4.3 GB of 5.3 GB virtual memory used. Killing container.\r\n",
      "Dump of the process-tree for container_1509050304403_25958_01_000069 :\r\n",
      "\t|- PID PPID PGRPID SESSID CMD_NAME USER_MODE_TIME(MILLIS) SYSTEM_TIME(MILLIS) VMEM_USAGE(BYTES) RSSMEM_USAGE(PAGES) FULL_CMD_LINE\r\n",
      "\t|- 6834 6827 6827 6827 (java) 10194 719 4571537408 667773 /usr/java/default/bin/java -XX:NewRatio=8 -Djava.net.preferIPv4Stack=true -Xmx2540m -Djava.io.tmpdir=/var/storage/nm-sdl1/nm-local/usercache/lteo01/appcache/application_1509050304403_25958/container_1509050304403_25958_01_000069/tmp -Dlog4j.configuration=container-log4j.properties -Dyarn.app.container.log.dir=/var/storage/nm-sdj1/nm-logs/application_1509050304403_25958/container_1509050304403_25958_01_000069 -Dyarn.app.container.log.filesize=0 -Dhadoop.root.logger=INFO,CLA -Dhadoop.root.logfile=syslog -Dyarn.app.mapreduce.shuffle.logger=INFO,shuffleCLA -Dyarn.app.mapreduce.shuffle.logfile=syslog.shuffle -Dyarn.app.mapreduce.shuffle.log.filesize=0 -Dyarn.app.mapreduce.shuffle.log.backups=0 org.apache.hadoop.mapred.YarnChild 10.251.240.185 36105 attempt_1509050304403_25958_r_000000_2 69 \r\n",
      "\t|- 6827 6825 6827 6827 (bash) 0 1 9490432 518 /bin/bash -c /usr/java/default/bin/java -XX:NewRatio=8 -Djava.net.preferIPv4Stack=true -Xmx2540m -Djava.io.tmpdir=/var/storage/nm-sdl1/nm-local/usercache/lteo01/appcache/application_1509050304403_25958/container_1509050304403_25958_01_000069/tmp -Dlog4j.configuration=container-log4j.properties -Dyarn.app.container.log.dir=/var/storage/nm-sdj1/nm-logs/application_1509050304403_25958/container_1509050304403_25958_01_000069 -Dyarn.app.container.log.filesize=0 -Dhadoop.root.logger=INFO,CLA -Dhadoop.root.logfile=syslog -Dyarn.app.mapreduce.shuffle.logger=INFO,shuffleCLA -Dyarn.app.mapreduce.shuffle.logfile=syslog.shuffle -Dyarn.app.mapreduce.shuffle.log.filesize=0 -Dyarn.app.mapreduce.shuffle.log.backups=0 org.apache.hadoop.mapred.YarnChild 10.251.240.185 36105 attempt_1509050304403_25958_r_000000_2 69 1>/var/storage/nm-sdj1/nm-logs/application_1509050304403_25958/container_1509050304403_25958_01_000069/stdout 2>/var/storage/nm-sdj1/nm-logs/application_1509050304403_25958/container_1509050304403_25958_01_000069/stderr  \r\n",
      "\r\n",
      "Container killed on request. Exit code is 143\r\n",
      "Container exited with a non-zero exit code 143\r\n",
      "\r\n",
      "   map 100% reduce 0%\r\n",
      "   map 100% reduce 17%\r\n",
      "   map 100% reduce 100%\r\n",
      "  Job job_1509050304403_25958 failed with state FAILED due to: Task failed task_1509050304403_25958_r_000000\r\n",
      "Job failed as tasks failed. failedMaps:0 failedReduces:1\r\n",
      "\r\n",
      "  Job not successful!\r\n",
      "  Streaming Command Failed!\r\n",
      "Counters: 37\r\n",
      "\tFile Input Format Counters \r\n",
      "\t\tBytes Read=3568988772\r\n",
      "\tFile System Counters\r\n",
      "\t\tFILE: Number of bytes read=0\r\n",
      "\t\tFILE: Number of bytes written=1155657349\r\n",
      "\t\tFILE: Number of large read operations=0\r\n",
      "\t\tFILE: Number of read operations=0\r\n",
      "\t\tFILE: Number of write operations=0\r\n",
      "\t\tHDFS: Number of bytes read=3569000420\r\n",
      "\t\tHDFS: Number of bytes written=0\r\n",
      "\t\tHDFS: Number of large read operations=0\r\n",
      "\t\tHDFS: Number of read operations=192\r\n",
      "\t\tHDFS: Number of write operations=0\r\n",
      "\tJob Counters \r\n",
      "\t\tFailed reduce tasks=4\r\n",
      "\t\tLaunched map tasks=64\r\n",
      "\t\tLaunched reduce tasks=4\r\n",
      "\t\tRack-local map tasks=64\r\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=4097983488\r\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=392279040\r\n",
      "\t\tTotal time spent by all map tasks (ms)=2667958\r\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=8003874\r\n",
      "\t\tTotal time spent by all reduce tasks (ms)=153234\r\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=766170\r\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=2667958\r\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=153234\r\n",
      "\tMap-Reduce Framework\r\n",
      "\t\tCPU time spent (ms)=1077410\r\n",
      "\t\tCombine input records=0\r\n",
      "\t\tFailed Shuffles=0\r\n",
      "\t\tGC time elapsed (ms)=50475\r\n",
      "\t\tInput split bytes=11648\r\n",
      "\t\tMap input records=28881712\r\n",
      "\t\tMap output bytes=3607507469\r\n",
      "\t\tMap output materialized bytes=1147196559\r\n",
      "\t\tMap output records=28881712\r\n",
      "\t\tMerged Map outputs=0\r\n",
      "\t\tPhysical memory (bytes) snapshot=51693977600\r\n",
      "\t\tSpilled Records=28881712\r\n",
      "\t\tTotal committed heap usage (bytes)=57156304896\r\n",
      "\t\tVirtual memory (bytes) snapshot=218945347584\r\n",
      "Scanning logs for probable cause of failure...\r\n",
      "Can't fetch history log; missing job ID\r\n",
      "Can't fetch task logs; missing application ID\r\n",
      "Step 2 of 2 failed: Command '['/opt/hadoop/bin/hadoop', 'jar', '/opt/hadoop/share/hadoop/tools/lib/hadoop-streaming-2.7.3.jar', '-files', 'hdfs:///user/lteo01/tmp/mrjob/similarity_v2.lteo01.20180306.164448.602492/files/mrjob.zip#mrjob.zip,hdfs:///user/lteo01/virtualenv/py27.zip#py27.zip,hdfs:///user/lteo01/tmp/mrjob/similarity_v2.lteo01.20180306.164448.602492/files/setup-wrapper.sh#setup-wrapper.sh,hdfs:///user/lteo01/tmp/mrjob/similarity_v2.lteo01.20180306.164448.602492/files/similarity_v2.py#similarity_v2.py', '-D', u'SORT_VALUES=True', '-D', u'mapreduce.job.output.key.comparator.class=org.apache.hadoop.mapred.lib.KeyFieldBasedComparator', '-D', u'mapreduce.job.reduces=1', '-D', u'mapreduce.partition.keycomparator.options=-k1,1nr', '-D', 'mapreduce.partition.keypartitioner.options=-k1,1', '-D', 'stream.num.map.output.key.fields=2', '-partitioner', 'org.apache.hadoop.mapred.lib.KeyFieldBasedPartitioner', '-cmdenv', 'LD_LIBRARY_PATH=/opt/rh/python27/root/usr/lib64', '-input', 'hdfs:///user/lteo01/tmp/mrjob/similarity_v2.lteo01.20180306.164448.602492/step-output/0000', '-output', 'hdfs:///user/lteo01/tests', '-mapper', 'cat', '-reducer', 'sh -ex setup-wrapper.sh /opt/rh/python27/root/usr/bin/python2.7 similarity_v2.py --step-num=1 --reducer']' returned non-zero exit status 256\r\n",
      "Removing HDFS temp directory hdfs:///user/lteo01/tmp/mrjob/similarity_v2.lteo01.20180306.164448.602492...\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing temp directory /tmp/similarity_v2.lteo01.20180306.164448.602492...\n",
      "\n",
      "real\t11m33.738s\n",
      "user\t0m49.698s\n",
      "sys\t0m2.419s\n"
     ]
    }
   ],
   "source": [
    "# Run in Hadoop\n",
    "\n",
    "OUTPUT_PATH = os.path.join(OUTPUT_PATH_BASE,'tests')\n",
    "!hadoop fs -rm -r {OUTPUT_PATH}\n",
    "\n",
    "!time python similarity_v2.py \\\n",
    "        -r hadoop google_index_FULL \\\n",
    "        --output-dir={OUTPUT_PATH} \\\n",
    "        --no-output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cat: `/user/lteo01/tests/*': No such file or directory\r\n"
     ]
    }
   ],
   "source": [
    "!hadoop fs -cat {OUTPUT_PATH}/* > google_similarities_3\n",
    "!cat google_similarities_3 | head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "############################################\n",
    "# Pretty print systems tests\n",
    "# Note: adjust print formatting if you need to\n",
    "############################################\n",
    "\n",
    "import json\n",
    "for i in range(2,3):\n",
    "    print '—'*110\n",
    "    print \"Systems test \",i,\" - Similarity measures\"\n",
    "    print '—'*110\n",
    "    print \"{0:>15} |{1:>15} |{2:>15} |{3:>15} |{4:>15} |{5:>15}\".format(\n",
    "    \"average\", \"pair\", \"cosine\", \"jaccard\", \"overlap\", \"dice\")\n",
    "    print '-'*110\n",
    "\n",
    "    with open(\"google_similarities_\"+str(i),\"r\") as f:\n",
    "        lines = f.readlines()\n",
    "        for line in lines:\n",
    "            line = line.strip()\n",
    "            avg,stripe = line.split(\"\\t\")\n",
    "            stripe = json.loads(stripe)\n",
    "            \n",
    "            print \"{0:>15f} |{1:>15} |{2:>15f} |{3:>15f} |{4:>15f} |{5:>15f}\".format(float(avg),\n",
    "                stripe[0], float(stripe[1]), float(stripe[2]), float(stripe[3]), float(stripe[4]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pretty print results\n",
    "NOTE: depending on how you processed the stop words your results may differ from the table provided. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print \"\\nTop/Bottom 20 results - Similarity measures - sorted by cosine\"\n",
    "print \"(From the entire data set)\"\n",
    "print '—'*117\n",
    "print \"{0:>30} |{1:>15} |{2:>15} |{3:>15} |{4:>15} |{5:>15}\".format(\n",
    "        \"pair\", \"cosine\", \"jaccard\", \"overlap\", \"dice\", \"average\")\n",
    "print '-'*117\n",
    "\n",
    "for stripe in sortedSims[:20]:\n",
    "    print \"{0:>30} |{1:>15f} |{2:>15f} |{3:>15f} |{4:>15f} |{5:>15f}\".format(\n",
    "        stripe[0], float(stripe[1]), float(stripe[2]), float(stripe[3]), float(stripe[4]), float(stripe[5]) )\n",
    "\n",
    "print '—'*117\n",
    "\n",
    "for stripe in sortedSims[-20:]:\n",
    "    print \"{0:>30} |{1:>15f} |{2:>15f} |{3:>15f} |{4:>15f} |{5:>15f}\".format(\n",
    "        stripe[0], float(stripe[1]), float(stripe[2]), float(stripe[3]), float(stripe[4]), float(stripe[5]) )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Top/Bottom 20 results - Similarity measures - sorted by cosine\n",
    "(From the entire data set)\n",
    "—————————————————————————————————————————————————————————————————————————————————————————————————————————————————————\n",
    "                          pair |         cosine |        jaccard |        overlap |           dice |        average\n",
    "---------------------------------------------------------------------------------------------------------------------\n",
    "                   cons - pros |       0.894427 |       0.800000 |       1.000000 |       0.888889 |       0.895829\n",
    "            forties - twenties |       0.816497 |       0.666667 |       1.000000 |       0.800000 |       0.820791\n",
    "                    own - time |       0.809510 |       0.670563 |       0.921168 |       0.802799 |       0.801010\n",
    "                 little - time |       0.784197 |       0.630621 |       0.926101 |       0.773473 |       0.778598\n",
    "                  found - time |       0.783434 |       0.636364 |       0.883788 |       0.777778 |       0.770341\n",
    "                 nova - scotia |       0.774597 |       0.600000 |       1.000000 |       0.750000 |       0.781149\n",
    "                   hong - kong |       0.769800 |       0.615385 |       0.888889 |       0.761905 |       0.758995\n",
    "                   life - time |       0.769666 |       0.608789 |       0.925081 |       0.756829 |       0.765091\n",
    "                  time - world |       0.755476 |       0.585049 |       0.937500 |       0.738209 |       0.754058\n",
    "                  means - time |       0.752181 |       0.587117 |       0.902597 |       0.739854 |       0.745437\n",
    "                   form - time |       0.749943 |       0.588418 |       0.876733 |       0.740885 |       0.738995\n",
    "       infarction - myocardial |       0.748331 |       0.560000 |       1.000000 |       0.717949 |       0.756570\n",
    "                 people - time |       0.745788 |       0.573577 |       0.923875 |       0.729010 |       0.743063\n",
    "                 angeles - los |       0.745499 |       0.586207 |       0.850000 |       0.739130 |       0.730209\n",
    "                  little - own |       0.739343 |       0.585834 |       0.767296 |       0.738834 |       0.707827\n",
    "                    life - own |       0.737053 |       0.582217 |       0.778502 |       0.735951 |       0.708430\n",
    "          anterior - posterior |       0.733388 |       0.576471 |       0.790323 |       0.731343 |       0.707881\n",
    "                  power - time |       0.719611 |       0.533623 |       0.933586 |       0.695898 |       0.720680\n",
    "              dearly - install |       0.707107 |       0.500000 |       1.000000 |       0.666667 |       0.718443\n",
    "                   found - own |       0.704802 |       0.544134 |       0.710949 |       0.704776 |       0.666165\n",
    "—————————————————————————————————————————————————————————————————————————————————————————————————————————————————————\n",
    "           arrival - essential |       0.008258 |       0.004098 |       0.009615 |       0.008163 |       0.007534\n",
    "         governments - surface |       0.008251 |       0.003534 |       0.014706 |       0.007042 |       0.008383\n",
    "                king - lesions |       0.008178 |       0.003106 |       0.017857 |       0.006192 |       0.008833\n",
    "              clinical - stood |       0.008178 |       0.003831 |       0.011905 |       0.007634 |       0.007887\n",
    "               till - validity |       0.008172 |       0.003367 |       0.015625 |       0.006711 |       0.008469\n",
    "            evidence - started |       0.008159 |       0.003802 |       0.012048 |       0.007576 |       0.007896\n",
    "               forces - record |       0.008152 |       0.003876 |       0.011364 |       0.007722 |       0.007778\n",
    "               primary - stone |       0.008146 |       0.004065 |       0.009091 |       0.008097 |       0.007350\n",
    "             beneath - federal |       0.008134 |       0.004082 |       0.008403 |       0.008130 |       0.007187\n",
    "                factors - rose |       0.008113 |       0.004032 |       0.009346 |       0.008032 |       0.007381\n",
    "           evening - functions |       0.008069 |       0.004049 |       0.008333 |       0.008065 |       0.007129\n",
    "                   bone - told |       0.008061 |       0.003704 |       0.012346 |       0.007380 |       0.007873\n",
    "             building - occurs |       0.008002 |       0.003891 |       0.010309 |       0.007752 |       0.007489\n",
    "                 company - fig |       0.007913 |       0.003257 |       0.015152 |       0.006494 |       0.008204\n",
    "               chronic - north |       0.007803 |       0.003268 |       0.014493 |       0.006515 |       0.008020\n",
    "             evaluation - king |       0.007650 |       0.003030 |       0.015625 |       0.006042 |       0.008087\n",
    "             resulting - stood |       0.007650 |       0.003663 |       0.010417 |       0.007299 |       0.007257\n",
    "                 agent - round |       0.007515 |       0.003289 |       0.012821 |       0.006557 |       0.007546\n",
    "         afterwards - analysis |       0.007387 |       0.003521 |       0.010204 |       0.007018 |       0.007032\n",
    "            posterior - spirit |       0.007156 |       0.002660 |       0.016129 |       0.005305 |       0.007812"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW5.8 - Evaluation of synonyms that your discovered\n",
    "\n",
    "In this part of the assignment you will evaluate the success of you synonym detector. Take the top 1,000 closest/most similar/correlative pairs of words as determined by your measure in HW5.7, and use the synonyms function from the wordnet synonnyms list from the nltk package (see provided code below).\n",
    "\n",
    "For each (word1,word2) pair, check to see if word1 is in the list, \n",
    "synonyms(word2), and vice-versa. If one of the two is a synonym of the other, \n",
    "then consider this pair a 'hit', and then report the precision, recall, and F1 measure  of \n",
    "your detector across your 1,000 best guesses. Report the macro averages of these measures."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate performance measures:\n",
    "$$Precision (P) = \\frac{TP}{TP + FP} $$  \n",
    "$$Recall (R) = \\frac{TP}{TP + FN} $$  \n",
    "$$F1 = \\frac{2 * ( precision * recall )}{precision + recall}$$\n",
    "\n",
    "\n",
    "We calculate Precision by counting the number of hits and dividing by the number of occurances in our top1000 (opportunities)   \n",
    "We calculate Recall by counting the number of hits, and dividing by the number of synonyms in wordnet (syns)\n",
    "\n",
    "\n",
    "Other diagnostic measures not implemented here:  https://en.wikipedia.org/wiki/F1_score#Diagnostic_Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Performance measures '''\n",
    "from __future__ import division\n",
    "import numpy as np\n",
    "import json\n",
    "import nltk\n",
    "from nltk.corpus import wordnet as wn\n",
    "import sys\n",
    "#print all the synset element of an element\n",
    "def synonyms(string):\n",
    "    syndict = {}\n",
    "    for i,j in enumerate(wn.synsets(string)):\n",
    "        syns = j.lemma_names()\n",
    "        for syn in syns:\n",
    "            syndict.setdefault(syn,1)\n",
    "    return syndict.keys()\n",
    "hits = []\n",
    "\n",
    "TP = 0\n",
    "FP = 0\n",
    "\n",
    "TOTAL = 0\n",
    "flag = False # so we don't double count, but at the same time don't miss hits\n",
    "\n",
    "top1000sims = []\n",
    "with open(\"sims2/top1000sims\",\"r\") as f:\n",
    "    for line in f.readlines():\n",
    "\n",
    "        line = line.strip()\n",
    "        avg,lisst = line.split(\"\\t\")\n",
    "        lisst = json.loads(lisst)\n",
    "        lisst.append(avg)\n",
    "        top1000sims.append(lisst)\n",
    "    \n",
    "\n",
    "measures = {}\n",
    "not_in_wordnet = []\n",
    "\n",
    "for line in top1000sims:\n",
    "    TOTAL += 1\n",
    "\n",
    "    pair = line[0]\n",
    "    words = pair.split(\" - \")\n",
    "    \n",
    "    for word in words:\n",
    "        if word not in measures:\n",
    "            measures[word] = {\"syns\":0,\"opps\": 0,\"hits\":0}\n",
    "        measures[word][\"opps\"] += 1 \n",
    "    \n",
    "    syns0 = synonyms(words[0])\n",
    "    measures[words[1]][\"syns\"] = len(syns0)\n",
    "    if len(syns0) == 0:\n",
    "        not_in_wordnet.append(words[0])\n",
    "        \n",
    "    if words[1] in syns0:\n",
    "        TP += 1\n",
    "        hits.append(line)\n",
    "        flag = True\n",
    "        measures[words[1]][\"hits\"] += 1\n",
    "        \n",
    "        \n",
    "        \n",
    "    syns1 = synonyms(words[1]) \n",
    "    measures[words[0]][\"syns\"] = len(syns1)\n",
    "    if len(syns1) == 0:\n",
    "        not_in_wordnet.append(words[1])\n",
    "\n",
    "    if words[0] in syns1:\n",
    "        if flag == False:\n",
    "            TP += 1\n",
    "            hits.append(line)\n",
    "            measures[words[0]][\"hits\"] += 1\n",
    "            \n",
    "    flag = False    \n",
    "\n",
    "precision = []\n",
    "recall = []\n",
    "f1 = []\n",
    "\n",
    "for key in measures:\n",
    "    p,r,f = 0,0,0\n",
    "    if measures[key][\"hits\"] > 0 and measures[key][\"syns\"] > 0:\n",
    "        p = measures[key][\"hits\"]/measures[key][\"opps\"]\n",
    "        r = measures[key][\"hits\"]/measures[key][\"syns\"]\n",
    "        f = 2 * (p*r)/(p+r)\n",
    "    \n",
    "    # For calculating measures, only take into account words that have synonyms in wordnet\n",
    "    if measures[key][\"syns\"] > 0:\n",
    "        precision.append(p)\n",
    "        recall.append(r)\n",
    "        f1.append(f)\n",
    "\n",
    "    \n",
    "# Take the mean of each measure    \n",
    "print \"—\"*110    \n",
    "print \"Number of Hits:\",TP, \"out of top\",TOTAL\n",
    "print \"Number of words without synonyms:\",len(not_in_wordnet)\n",
    "print \"—\"*110 \n",
    "print \"Precision\\t\", np.mean(precision)\n",
    "print \"Recall\\t\\t\", np.mean(recall)\n",
    "print \"F1\\t\\t\", np.mean(f1)\n",
    "print \"—\"*110  \n",
    "\n",
    "print \"Words without synonyms:\"\n",
    "print \"-\"*100\n",
    "\n",
    "for word in not_in_wordnet:\n",
    "    print synonyms(word),word\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "——————————————————————————————————————————————————————————————————————————————————————————————————————————————\n",
    "Number of Hits: 31 out of top 1000\n",
    "Number of words without synonyms: 67\n",
    "——————————————————————————————————————————————————————————————————————————————————————————————————————————————\n",
    "Precision\t0.0280214404967\n",
    "Recall\t\t0.0178598869579\n",
    "F1\t\t0.013965517619\n",
    "——————————————————————————————————————————————————————————————————————————————————————————————————————————————\n",
    "Words without synonyms:\n",
    "----------------------------------------------------------------------------------------------------\n",
    "[] scotia\n",
    "[] hong\n",
    "[] kong\n",
    "[] angeles\n",
    "[] los\n",
    "[] nor\n",
    "[] themselves\n",
    "[] \n",
    "......."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW5.9 - OPTIONAL: using different vocabulary subsets\n",
    "\n",
    "Repeat HW5 using vocabulary words ranked from 8001,-10,000;  7001,-10,000; 6001,-10,000; 5001,-10,000; 3001,-10,000; and 1001,-10,000;\n",
    "Dont forget to report you Cluster configuration.\n",
    "\n",
    "Generate the following graphs:\n",
    "-- vocabulary size (X-Axis) versus CPU time for indexing\n",
    "-- vocabulary size (X-Axis) versus number of pairs processed\n",
    "-- vocabulary size (X-Axis) versus F1 measure, Precision, Recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW5.10  - OPTIONAL \n",
    "\n",
    "There are many good ways to build our synonym detectors, so for this optional homework, \n",
    "measure co-occurrence by (left/right/all) consecutive words only, \n",
    "or make stripes according to word co-occurrences with the accompanying \n",
    "2-, 3-, or 4-grams (note here that your output will no longer \n",
    "be interpretable as a network) inside of the 5-grams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW5.11 - OPTIONAL \n",
    "\n",
    "Once again, benchmark your top 10,000 associations (as in 5.7), this time for your\n",
    "results from 5.8. Has your detector improved?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:py27]",
   "language": "python",
   "name": "conda-env-py27-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  },
  "toc": {
   "nav_menu": {
    "height": "511px",
    "width": "251px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "1080px",
    "left": "0px",
    "right": "1300px",
    "top": "107px",
    "width": "318px"
   },
   "toc_section_display": "block",
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
