{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": "true"
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Table-of-Contents\" data-toc-modified-id=\"Table-of-Contents-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Table of Contents</a></span></li><li><span><a href=\"#MIDS---w261-Machine-Learning-At-Scale\" data-toc-modified-id=\"MIDS---w261-Machine-Learning-At-Scale-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>MIDS - w261 Machine Learning At Scale</a></span><ul class=\"toc-item\"><li><span><a href=\"#Assignment---HW5\" data-toc-modified-id=\"Assignment---HW5-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>Assignment - HW5</a></span><ul class=\"toc-item\"><li><span><a href=\"#INSTRUCTIONS-for-SUBMISSIONS\" data-toc-modified-id=\"INSTRUCTIONS-for-SUBMISSIONS-2.1.1\"><span class=\"toc-item-num\">2.1.1&nbsp;&nbsp;</span>INSTRUCTIONS for SUBMISSIONS</a></span></li></ul></li></ul></li><li><span><a href=\"#Optional-reading\" data-toc-modified-id=\"Optional-reading-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Optional reading</a></span></li><li><span><a href=\"#HW-Problems\" data-toc-modified-id=\"HW-Problems-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>HW Problems</a></span></li><li><span><a href=\"#HW5.0--data-warehouse;-star-schema\" data-toc-modified-id=\"HW5.0--data-warehouse;-star-schema-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>HW5.0  data warehouse; star schema</a></span></li><li><span><a href=\"#HW5.1-Databases:-3NF;-denormalized\" data-toc-modified-id=\"HW5.1-Databases:-3NF;-denormalized-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>HW5.1 Databases: 3NF; denormalized</a></span></li><li><span><a href=\"#HW5.2--Memory-backed-map-side\" data-toc-modified-id=\"HW5.2--Memory-backed-map-side-7\"><span class=\"toc-item-num\">7&nbsp;&nbsp;</span>HW5.2  Memory-backed map-side</a></span><ul class=\"toc-item\"><li><ul class=\"toc-item\"><li><span><a href=\"#INNER-JOIN\" data-toc-modified-id=\"INNER-JOIN-7.0.1\"><span class=\"toc-item-num\">7.0.1&nbsp;&nbsp;</span>INNER JOIN</a></span></li><li><span><a href=\"#LEFT-JOIN\" data-toc-modified-id=\"LEFT-JOIN-7.0.2\"><span class=\"toc-item-num\">7.0.2&nbsp;&nbsp;</span>LEFT JOIN</a></span></li><li><span><a href=\"#RIGHT-JOIN\" data-toc-modified-id=\"RIGHT-JOIN-7.0.3\"><span class=\"toc-item-num\">7.0.3&nbsp;&nbsp;</span>RIGHT JOIN</a></span></li></ul></li><li><span><a href=\"#Answers\" data-toc-modified-id=\"Answers-7.1\"><span class=\"toc-item-num\">7.1&nbsp;&nbsp;</span>Answers</a></span></li></ul></li><li><span><a href=\"#HW5.2.1-(OPTIONAL)-Almost-stateless-reducer-side-join\" data-toc-modified-id=\"HW5.2.1-(OPTIONAL)-Almost-stateless-reducer-side-join-8\"><span class=\"toc-item-num\">8&nbsp;&nbsp;</span>HW5.2.1 (OPTIONAL) Almost stateless reducer-side join</a></span></li><li><span><a href=\"#5.3-Pairwise-similarity----PHASE-1\" data-toc-modified-id=\"5.3-Pairwise-similarity----PHASE-1-9\"><span class=\"toc-item-num\">9&nbsp;&nbsp;</span>5.3 Pairwise similarity  - PHASE 1</a></span><ul class=\"toc-item\"><li><span><a href=\"#HW5.3.1---Run-Systems-tests-locally-on-small-datasets-(PHASE1)\" data-toc-modified-id=\"HW5.3.1---Run-Systems-tests-locally-on-small-datasets-(PHASE1)-9.1\"><span class=\"toc-item-num\">9.1&nbsp;&nbsp;</span>HW5.3.1   Run Systems tests locally on small datasets (PHASE1)</a></span><ul class=\"toc-item\"><li><ul class=\"toc-item\"><li><span><a href=\"#1:-unit/systems-first-10-lines\" data-toc-modified-id=\"1:-unit/systems-first-10-lines-9.1.0.1\"><span class=\"toc-item-num\">9.1.0.1&nbsp;&nbsp;</span>1: unit/systems first-10-lines</a></span></li><li><span><a href=\"#2:-unit/systems-atlas-boon\" data-toc-modified-id=\"2:-unit/systems-atlas-boon-9.1.0.2\"><span class=\"toc-item-num\">9.1.0.2&nbsp;&nbsp;</span>2: unit/systems atlas-boon</a></span></li><li><span><a href=\"#3:-unit/systems-stripe-docs-test\" data-toc-modified-id=\"3:-unit/systems-stripe-docs-test-9.1.0.3\"><span class=\"toc-item-num\">9.1.0.3&nbsp;&nbsp;</span>3: unit/systems stripe-docs-test</a></span></li></ul></li><li><span><a href=\"#(1)-build-stripes-for-all-the-test-data-sets---run-the-commands-and-insure-that-your-output-matches-the-output-below\" data-toc-modified-id=\"(1)-build-stripes-for-all-the-test-data-sets---run-the-commands-and-insure-that-your-output-matches-the-output-below-9.1.1\"><span class=\"toc-item-num\">9.1.1&nbsp;&nbsp;</span>(1) build stripes for all the test data sets - run the commands and insure that your output matches the output below</a></span></li><li><span><a href=\"#(2)-Build-Inverted-Index---run-the-commands-and-insure-that-your-output-matches-the-output-below\" data-toc-modified-id=\"(2)-Build-Inverted-Index---run-the-commands-and-insure-that-your-output-matches-the-output-below-9.1.2\"><span class=\"toc-item-num\">9.1.2&nbsp;&nbsp;</span>(2) Build Inverted Index - run the commands and insure that your output matches the output below</a></span></li><li><span><a href=\"#Inverted-Index\" data-toc-modified-id=\"Inverted-Index-9.1.3\"><span class=\"toc-item-num\">9.1.3&nbsp;&nbsp;</span>Inverted Index</a></span></li><li><span><a href=\"#(3)-Calculate-similarities---run-the-commands-and-insure-that-your-output-matches-the-output-below\" data-toc-modified-id=\"(3)-Calculate-similarities---run-the-commands-and-insure-that-your-output-matches-the-output-below-9.1.4\"><span class=\"toc-item-num\">9.1.4&nbsp;&nbsp;</span>(3) Calculate similarities - run the commands and insure that your output matches the output below</a></span><ul class=\"toc-item\"><li><span><a href=\"#NOTE:-you-must-run-in-hadoop-mode-to-generate-sorted-similarities\" data-toc-modified-id=\"NOTE:-you-must-run-in-hadoop-mode-to-generate-sorted-similarities-9.1.4.1\"><span class=\"toc-item-num\">9.1.4.1&nbsp;&nbsp;</span>NOTE: you must run in hadoop mode to generate sorted similarities</a></span></li></ul></li><li><span><a href=\"#Pairwise-Similairity\" data-toc-modified-id=\"Pairwise-Similairity-9.1.5\"><span class=\"toc-item-num\">9.1.5&nbsp;&nbsp;</span>Pairwise Similairity</a></span></li></ul></li></ul></li><li><span><a href=\"#===-END-OF-PHASE-1-===\" data-toc-modified-id=\"===-END-OF-PHASE-1-===-10\"><span class=\"toc-item-num\">10&nbsp;&nbsp;</span>=== END OF PHASE 1 ===</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MIDS - w261 Machine Learning At Scale\n",
    "__Course Lead:__ Dr James G. Shanahan (__email__ Jimi via  James.Shanahan _AT_ gmail.com)\n",
    "\n",
    "## Assignment - HW5\n",
    "\n",
    "\n",
    "---\n",
    "__Name:__  Leslie Teo \n",
    "__Class:__ MIDS w261 Winter 2018, Section 1     \n",
    "__Email:__  lteo@iSchool.Berkeley.edu; lteo01@berkeley.edu     \n",
    "__StudentId__  303218617    __End of StudentId__     \n",
    "__Week:__   5\n",
    "\n",
    "__NOTE:__ please replace `1234567` with your student id above      \n",
    "__Due Time:__ HW is due the Tuesday of the following week by 8AM (West coast time). I.e., Tuesday, Feb 14, 2017 in the case of this homework. \n",
    "\n",
    "* __HW5 Phase 1__ \n",
    "This can be done on a local machine (with a unit test on the cloud such as Altiscale's PaaS or on AWS) and is due Tuesday, Week 6 by 8AM (West coast time). It will primarily focus on building a unit/systems and for pairwise similarity calculations pipeline (for stripe documents)\n",
    "\n",
    "* __HW5 Phase 2__ \n",
    "This will require the Altiscale cluster and will be due Tuesday, Feb 21 by 8AM (West coast time). \n",
    "The focus of  HW5 Phase 2  will be to scale up the unit/systems tests to the Google 5 gram corpus. \n",
    "\n",
    "\n",
    "\n",
    "### INSTRUCTIONS for SUBMISSIONS \n",
    "Follow the instructions for submissions carefully.\n",
    "\n",
    "Each student has a `HW-<user>` repository for all assignments.   \n",
    "\n",
    "Push the following to your HW github repo into the master branch:\n",
    "* Your local HW5 directory. Your repo file structure should look like this:\n",
    "\n",
    "```\n",
    "HW-<user>\n",
    "    --HW3\n",
    "       |__MIDS-W261-HW-03-<Student_id>.ipynb\n",
    "       |__MIDS-W261-HW-03-<Student_id>.pdf\n",
    "       |__some other hw3 file\n",
    "    --HW4\n",
    "       |__MIDS-W261-HW-04-<Student_id>.ipynb\n",
    "       |__MIDS-W261-HW-04-<Student_id>.pdf\n",
    "       |__some other hw4 file\n",
    "    etc..\n",
    "```    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optional reading\n",
    "http://stanford.edu/~rezab/papers/disco.pdf   \n",
    "https://terpconnect.umd.edu/~oard/pdf/acl08elsayed2.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW Problems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW5.0  data warehouse; star schema\n",
    "\n",
    "- What is a data warehouse? What is a Star schema? When is it used?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">  A data warehouse -- usually used in the context of an organization -- refers to central repositories/databases of data integrated from one or more disparate sources. <p>\n",
    "\n",
    "> The star schema architecture is a model for organizing data into facts and dimensions. It is called a star schema because it resembles a star: the center of the star consists of fact table (measurable pieces of info) and the points of the star are the dimension tables (reference info such as name). Usually the fact tables in a star schema are in third normal form(3NF) whereas dimensional tables are de-normalized. <p>\n",
    "\n",
    "> This schema is widely used in SQL data warehouses e.g. common in many business intelligence tools. Each dimension table is associated to a fact table via a key, and by combining key we can query and identify (uniquely) every piece of data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW5.1 Databases: 3NF; denormalized \n",
    "\n",
    "- In the database world What is 3NF? Does machine learning use data in 3NF? If so why? \n",
    "- In what form does ML consume data?\n",
    "- Why would one use log files that are denormalized?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> __1)__ A data table is in 3NF if it is also 2NF and contains only columns that are non-transitively dependent on the primary key. (1st, 2nd, 3rd NF are ways to normalize data tables so as to make searches, queries more efficient while removing redundancies and inconsistencies. ML can use data in 3NL form, and the advantage is that such data are \"clean\" and features are more \"independent\" of each other. However, if we have lots of features or are exploring features, the need to join or combine disparate/disjoint normalized data can take up time and resources. Presumably this is worse when we are trying to do this in a distributed manner. <p>\n",
    "\n",
    "> __2)__ Many ML routines will consume a lot of data (need to have good read performance) and may require joins or combinations. ML models require features to be explicit and data is rigidly defined. <p>\n",
    "\n",
    "> __3)__ To improve read performance; to enable redundant copies that can also be read by different nodes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW5.2  Memory-backed map-side\n",
    "\n",
    "Using MRJob, implement a hashside join (memory-backed map-side) for left, right and inner joins. Use the following tables for this HW and join based on the country code (third column of the transactions table and the second column of the Countries table:\n",
    "\n",
    "<pre>\n",
    "transactions.dat\n",
    "Alice Bob|$10|US\n",
    "Sam Sneed|$1|CA\n",
    "Jon Sneed|$20|CA\n",
    "Arnold Wesise|$400|UK\n",
    "Henry Bob|$2|US\n",
    "Yo Yo Ma|$2|CA\n",
    "Jon York|$44|CA\n",
    "Alex Ball|$5|UK\n",
    "Jim Davis|$66|JA\n",
    "\n",
    "Countries.dat\n",
    "United States|US\n",
    "Canada|CA\n",
    "United Kingdom|UK\n",
    "Italy|IT\n",
    "\n",
    "</pre>\n",
    "\n",
    "Justify which table you chose as the Left table in this hashside join.\n",
    "\n",
    "Please report the number of rows resulting from:\n",
    "\n",
    "- (1) Left joining Table Left with Table Right\n",
    "- (2) Right joining Table Left with Table Right\n",
    "- (3) Inner joining Table Left with Table Right\n",
    "\n",
    "__NOTE:__ For the Right and Inner joins, no reducer should be used. For the left outer join you can use a reducer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python Version: 2.7.14 \n",
      "HDFS filesystem running at: \n",
      "\t hdfs://quickstart.cloudera:8020\n",
      "mkdir: `HW5': File exists\n",
      "Found 2 items\n",
      "drwxr-xr-x   - root supergroup          0 2018-02-12 07:08 HW5\n",
      "drwxr-xr-x   - root supergroup          0 2018-02-12 07:45 tmp\n"
     ]
    }
   ],
   "source": [
    "# Setup\n",
    "# general imports\n",
    "import os\n",
    "import re\n",
    "import sys\n",
    "import time\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# tell matplotlib not to open a new window\n",
    "%matplotlib inline\n",
    "\n",
    "# automatically reload modules \n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# print some configuration details for future replicability.\n",
    "print 'Python Version: %s' % (sys.version.split('|')[0])\n",
    "hdfs_conf = !hdfs getconf -confKey fs.defaultFS ### UNCOMMENT ON DOCKER\n",
    "#hdfs_conf = !hdfs getconf -confKey fs.default.name ### UNCOMMENT ON ALTISCALE\n",
    "print 'HDFS filesystem running at: \\n\\t %s' % (hdfs_conf[0])\n",
    "\n",
    "# create an HDFS directory for this assignment\n",
    "!hdfs dfs -mkdir HW5\n",
    "\n",
    "JAR_FILE = \"/usr/lib/hadoop-mapreduce/hadoop-streaming-2.6.0-cdh5.7.0.jar\"\n",
    "HDFS_DIR = \"/user/root/HW4\"\n",
    "HOME_DIR = \"/media/notebooks/SP18-1-maynard242\" # FILL IN HERE eg. /media/notebooks/w261-main/Assignments\n",
    "# save path for use in Hadoop jobs (-cmdenv PATH={PATH})\n",
    "from os import environ\n",
    "PATH  = environ['PATH']\n",
    "\n",
    "!hdfs dfs -ls\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting transactions.dat\n"
     ]
    }
   ],
   "source": [
    "%%writefile transactions.dat\n",
    "Alice Bob|$10|US\n",
    "Sam Sneed|$1|CA\n",
    "Jon Sneed|$20|CA\n",
    "Arnold Wesise|$400|UK\n",
    "Henry Bob|$2|US\n",
    "Yo Yo Ma|$2|CA\n",
    "Jon York|$44|CA\n",
    "Alex Ball|$5|UK\n",
    "Jim Davis|$66|JA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting Countries.dat\n"
     ]
    }
   ],
   "source": [
    "%%writefile Countries.dat\n",
    "United States|US\n",
    "Canada|CA\n",
    "United Kingdom|UK\n",
    "Italy|IT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### INNER JOIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting MappersideJoin.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile MappersideJoin.py\n",
    "\n",
    "from mrjob.job import MRJob \n",
    "from mrjob.step import MRStep \n",
    "\n",
    "class MRMappersideJoin(MRJob):\n",
    "\n",
    "    # Does an inner join - load smaller table into memoery (countries.dat)\n",
    "    \n",
    "    SORT_VALUES = True\n",
    "    countries = {}\n",
    "    \n",
    "    def mapper_init(self):\n",
    "        with open('/media/notebooks/SP18-1-maynard242/HW5/Countries.dat', 'r') as f:\n",
    "            for line in f:\n",
    "                tokens = line.strip().split('|')\n",
    "                self.countries[tokens[1]] = tokens[0]\n",
    "                                 \n",
    "    # Compare transactions dat against memory and append country names\n",
    "    \n",
    "    def mapper_innerjoin(self, _, record):\n",
    "        self.increment_counter('Execution Counts', 'inner mapper calls', 1)\n",
    "        tokens = record.strip().split('|')\n",
    "        if tokens[2] in self.countries:\n",
    "            yield tokens[2], (tokens[0]+'|'+tokens[1]+'|'+self.countries[tokens[2]])\n",
    "            \n",
    "    def steps(self):\n",
    "        return [\n",
    "            MRStep(\n",
    "                mapper_init=self.mapper_init,\n",
    "                mapper=self.mapper_innerjoin\n",
    "                )\n",
    "              ]\n",
    "    \n",
    "if __name__ == '__main__': \n",
    "    MRMappersideJoin.run()\n",
    "                       \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LEFT JOIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting MRMappersideLeftJoin.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile MRMappersideLeftJoin.py\n",
    "\n",
    "from mrjob.job import MRJob \n",
    "from mrjob.step import MRStep \n",
    "\n",
    "class MRMappersideLeftJoin(MRJob):\n",
    "    \n",
    "    # Does an left join - append smaller table in memory to transactions data\n",
    "    \n",
    "    SORT_VALUES = True\n",
    "    countries = {}\n",
    "    \n",
    "    def mapper_init(self):\n",
    "        with open('/media/notebooks/SP18-1-maynard242/HW5/Countries.dat', 'r') as f:\n",
    "            for line in f:\n",
    "                tokens = line.strip().split('|')\n",
    "                self.countries[tokens[1]] = tokens[0]\n",
    "                \n",
    "    # Iterate over records being read, append country names from memory, emit NULL if not in memory (right table)\n",
    "               \n",
    "    def mapper_leftjoin(self, _, record):\n",
    "        self.increment_counter('Execution Counts', 'left mapper calls', 1)\n",
    "        tokens = record.strip().split('|')\n",
    "        if tokens[2] in self.countries:\n",
    "            yield tokens[2], (tokens[0]+'|'+tokens[1]+'|'+self.countries[tokens[2]])\n",
    "        else:\n",
    "            yield tokens[2], (tokens[0]+'|'+tokens[1]+'|'+'NULL')\n",
    "    \n",
    "    def steps(self):\n",
    "        return [\n",
    "            MRStep(\n",
    "                mapper_init=self.mapper_init,\n",
    "                mapper=self.mapper_leftjoin\n",
    "                )\n",
    "              ]\n",
    "    \n",
    "if __name__ == '__main__': \n",
    "    MRMappersideLeftJoin.run()  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RIGHT JOIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting MRMappersideRightJoin.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile MRMappersideRightJoin.py\n",
    "\n",
    "from mrjob.job import MRJob\n",
    "\n",
    "class MRMappersideRightJoin(MRJob):\n",
    "    \n",
    "    countries = {}\n",
    "    ex_list = []\n",
    "    \n",
    "    def mapper_init(self):\n",
    "        with open('/media/notebooks/SP18-1-maynard242/HW5/Countries.dat', 'r') as f:\n",
    "            for line in f:\n",
    "                tokens = line.strip().split('|')\n",
    "                self.countries[tokens[1]] = tokens[0]\n",
    "                self.ex_list = self.countries.keys()\n",
    "                \n",
    "    # Emit records if keys exist in both tables; collect list of see country keys in ex_list\n",
    "    \n",
    "    def mapper(self, _, record):\n",
    "        self.increment_counter('Execution Counts', 'right mapper calls', 1)\n",
    "        tokens = record.strip().split('|')\n",
    "        \n",
    "        if tokens[2] in self.ex_list:\n",
    "            self.ex_list.remove(tokens[2])\n",
    "        \n",
    "        if tokens[2] in self.countries.keys():\n",
    "            yield tokens[2], (tokens[0]+'|'+tokens[1]+'|'+self.countries[tokens[2]])\n",
    "        \n",
    "    # Compare country list in memory with that see in records, emit null for that which is in memory but not seen\n",
    "        \n",
    "    def mapper_final(self):\n",
    "        self.increment_counter('Execution Counts', 'final mapper calls', 1)\n",
    "        for key in self.ex_list:\n",
    "            yield key, ('NULL|NULL|'+self.countries[key])\n",
    "            \n",
    "    def reducer(self, key, values):\n",
    "        cur_key = []\n",
    "        for value in values:\n",
    "            if key == cur_key and value.startswith('NULL'):\n",
    "                pass\n",
    "            else:\n",
    "                yield key, value\n",
    "            cur_key = key\n",
    "        \n",
    "        \n",
    "if __name__ == '__main__': \n",
    "    MRMappersideRightJoin.run()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!python MRMappersideRightJoin.py -r hadoop --cmdenv PATH=/opt/anaconda/bin:$PATH transactions.dat\n",
    "#!python MRMappersideRightJoin.py transactions.dat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mapperside Joins:\n",
      "\n",
      "CA Yo Yo Ma|$2|Canada\n",
      "CA Jon York|$44|Canada\n",
      "UK Alex Ball|$5|United Kingdom\n",
      "US Alice Bob|$10|United States\n",
      "CA Sam Sneed|$1|Canada\n",
      "CA Jon Sneed|$20|Canada\n",
      "UK Arnold Wesise|$400|United Kingdom\n",
      "US Henry Bob|$2|United States\n",
      "\"MRMappersideJoin\" : 8 rows\n",
      "\n",
      "Mapperside Joins:\n",
      "\n",
      "CA Yo Yo Ma|$2|Canada\n",
      "CA Jon York|$44|Canada\n",
      "UK Alex Ball|$5|United Kingdom\n",
      "JA Jim Davis|$66|NULL\n",
      "US Alice Bob|$10|United States\n",
      "CA Sam Sneed|$1|Canada\n",
      "CA Jon Sneed|$20|Canada\n",
      "UK Arnold Wesise|$400|United Kingdom\n",
      "US Henry Bob|$2|United States\n",
      "\"MRMappersideLeftJoin\" : 9 rows\n",
      "\n",
      "Mapperside Joins:\n",
      "\n",
      "CA Jon York|$44|Canada\n",
      "CA Yo Yo Ma|$2|Canada\n",
      "CA Jon Sneed|$20|Canada\n",
      "CA Sam Sneed|$1|Canada\n",
      "IT NULL|NULL|Italy\n",
      "UK Alex Ball|$5|United Kingdom\n",
      "UK Arnold Wesise|$400|United Kingdom\n",
      "US Henry Bob|$2|United States\n",
      "US Alice Bob|$10|United States\n",
      "\"MRMappersideRightJoin\" : 9 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from MappersideJoin import MRMappersideJoin \n",
    "from MRMappersideLeftJoin import MRMappersideLeftJoin\n",
    "from MRMappersideRightJoin import MRMappersideRightJoin\n",
    "\n",
    "for join_type in [MRMappersideJoin, MRMappersideLeftJoin, MRMappersideRightJoin]:\n",
    "    mr_job = join_type(['-r','hadoop', '--cmdenv', 'PATH=/opt/anaconda/bin:$PATH','transactions.dat'])    \n",
    "    with mr_job.make_runner() as runner:\n",
    "        runner.run()\n",
    "        counter = 0\n",
    "        print 'Mapperside Joins:\\n'\n",
    "        for line in runner.stream_output():\n",
    "            counter += 1\n",
    "            key, value = mr_job.parse_output_line(line)\n",
    "            print key, value\n",
    "            \n",
    "        className=str(mr_job).split(\".\")[1].split(\" \")[0]  \n",
    "        print '\"%s\" : %d rows\\n' % (className, counter)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Rows are printed above. <p>\n",
    "\n",
    "> Notes: 1) Per discussion on slack, I read the instructions as right instead of left. We want smaller table (in this case Country.dat) in memory. I call this the right table per discussion. 2) Reducer is needed in RightJoin to deal with duplicates (at least that is the only way I could figure out how to fix inconsistent missing values being passed.)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW5.2.1 (OPTIONAL) Almost stateless reducer-side join\n",
    "\n",
    "The following MRJob code, implements a reduce-side join for an inner join. The reducer is almost stateless, i.e., uses as little memory as possible. Use the tables from HW5.2 for this HW and join based on the country code (third column of the transactions table and the second column of the Countries table perform. Perform  an left, right, inner joins using the code provided below and report the number of rows resulting from:\n",
    "\n",
    "- (1) Left joining Table Left with Table Right\n",
    "- (2) Right joining Table Left with Table Right\n",
    "- (3) Inner joining Table Left with Table Right\n",
    "\n",
    "Again make smart decisions about which table should be the left table (i.e., crosscheck the code). \n",
    "\n",
    "__Some notes on the code__ \n",
    "Here, the mapper receives its set of input splits either from the transaction table or from the countries table and makes the appropriate transformations: splitting the line into fields, and emitting a key/value. The key is the join key - in this case, the country code field of both sets of records. The mapper knows which file and type of record it is receiving based on the length of the fields. The records it emits contain the join field as the key, which acts as the partitioning key; We use the SORT_VALUES option, which ensures the values are sorted as well. Then, we employ a trick to ensure that for each join key, country records are seen always before transaction records. We achieve this by adding an arbitrary key to the front of the value: 'A' for countries, 'B' for customers. This makes countries sort before customers for each and every join/partition key. After that trick, the join is simply a matter of storing countries ('A' records) and crossing this array with each customer record."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ReducerSideJoins.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ReducerSideJoins.py\n",
    "\n",
    "import sys, os, re\n",
    "from mrjob.job import MRJob\n",
    "\n",
    "class MRJoin(MRJob):\n",
    "  # Data will be sorted by key (the country id), nd then by value:\n",
    "  # Performs secondary sort on the value starting with either 'A' or 'B'\n",
    "  SORT_VALUES = True\n",
    "\n",
    "  def mapper(self, _, line):\n",
    "    splits = line.rstrip(\"\\n\").split(\"|\")\n",
    "\n",
    "    if len(splits) == 2: # countries\n",
    "      table = 'A' # make countries sort before transactions data\n",
    "      cid = splits[1]\n",
    "      yield cid, (table, splits)\n",
    "    else: # transactions\n",
    "      table = 'B'\n",
    "      cid = splits[2]\n",
    "      yield cid, (table, splits)\n",
    "\n",
    "  def reducer(self, key, values):\n",
    "    country = [None]\n",
    "    for value in values:\n",
    "      # country should come first, as countries are sorted on artificial key 'A'. \n",
    "      # Also, we assume that country id is a unique identifier\n",
    "      if value[0] == 'A':\n",
    "        country=value[1:]\n",
    "      if value[0] == 'B' and country[0] is not None:\n",
    "        transaction=value[1:]\n",
    "        yield key, country + transaction\n",
    "\n",
    "class MRLeftJoin(MRJoin):\n",
    "  \n",
    "  #####################################################################\n",
    "  # For a Left-Join we want to make sure that we are not emitting any \n",
    "  # rows where there is no row in the left table, hence this check: \n",
    "  # \"and country[0] is not None\"\n",
    "  #####################################################################\n",
    "  \n",
    "  \n",
    "  def reducer(self, key, values):\n",
    "    \n",
    "    ##################################################################\n",
    "    # transactionSeen = False\n",
    "    #\n",
    "    # keeps track of whether the transaction has been seen, in other \n",
    "    # words, whether there is an entry in the right table 'B'. This \n",
    "    # makes the reducer stateful, but only using a single value, \n",
    "    # so it is not a memory concern.\n",
    "    ##################################################################\n",
    "    \n",
    "    transactionSeen = False\n",
    "    \n",
    "    ##################################################################\n",
    "    # country = [None]\n",
    "    #\n",
    "    # initialize the country to None. Wrap 'None' in a list for  \n",
    "    # convenience so we can concatenate the country and transaction \n",
    "    # lists thus avoiding ugly string manipulation\n",
    "    ##################################################################\n",
    "    \n",
    "    country = [None]\n",
    "    \n",
    "    for value in values:\n",
    "      if value[0] == 'A': \n",
    "        country=value[1:]\n",
    "      if value[0] == 'B' and country[0] is not None: \n",
    "        transactionSeen = True\n",
    "        transaction=value[1:]\n",
    "        yield key, country + transaction\n",
    "    if transactionSeen == False and country[0] is not None:\n",
    "        yield key, country + [None]\n",
    "    \n",
    "class MRRightJoin(MRJoin):\n",
    "  \n",
    "  #################################################################\n",
    "  # For a Right-Join we want to make that we are not emitting any \n",
    "  # rows where there is no row in the right table, hence this check:\n",
    "  # we only yield a row if we come across a transaction.\n",
    "  #################################################################\n",
    "  \n",
    "  def reducer(self, key, values):\n",
    "    country = [None]\n",
    "    for value in values:\n",
    "      if value[0] == 'A':\n",
    "        country=value[1:]\n",
    "      if value[0] == 'B':\n",
    "        transaction=value[1:]\n",
    "        yield key, country + transaction\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "  MRJoin.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "US [[u'United States', u'US'], [u'Alice Bob', u'$10', u'US']]\n",
      "US [[u'United States', u'US'], [u'Henry Bob', u'$2', u'US']]\n",
      "CA [[u'Canada', u'CA'], [u'Jon Sneed', u'$20', u'CA']]\n",
      "CA [[u'Canada', u'CA'], [u'Jon York', u'$44', u'CA']]\n",
      "CA [[u'Canada', u'CA'], [u'Sam Sneed', u'$1', u'CA']]\n",
      "CA [[u'Canada', u'CA'], [u'Yo Yo Ma', u'$2', u'CA']]\n",
      "UK [[u'United Kingdom', u'UK'], [u'Alex Ball', u'$5', u'UK']]\n",
      "UK [[u'United Kingdom', u'UK'], [u'Arnold Wesise', u'$400', u'UK']]\n",
      "\"MRJoin\" : 8 rows\n",
      "\n",
      "US [[u'United States', u'US'], [u'Alice Bob', u'$10', u'US']]\n",
      "US [[u'United States', u'US'], [u'Henry Bob', u'$2', u'US']]\n",
      "CA [[u'Canada', u'CA'], [u'Jon Sneed', u'$20', u'CA']]\n",
      "CA [[u'Canada', u'CA'], [u'Jon York', u'$44', u'CA']]\n",
      "CA [[u'Canada', u'CA'], [u'Sam Sneed', u'$1', u'CA']]\n",
      "CA [[u'Canada', u'CA'], [u'Yo Yo Ma', u'$2', u'CA']]\n",
      "IT [[u'Italy', u'IT'], None]\n",
      "UK [[u'United Kingdom', u'UK'], [u'Alex Ball', u'$5', u'UK']]\n",
      "UK [[u'United Kingdom', u'UK'], [u'Arnold Wesise', u'$400', u'UK']]\n",
      "\"MRLeftJoin\" : 9 rows\n",
      "\n",
      "US [[u'United States', u'US'], [u'Alice Bob', u'$10', u'US']]\n",
      "US [[u'United States', u'US'], [u'Henry Bob', u'$2', u'US']]\n",
      "JA [None, [u'Jim Davis', u'$66', u'JA']]\n",
      "CA [[u'Canada', u'CA'], [u'Jon Sneed', u'$20', u'CA']]\n",
      "CA [[u'Canada', u'CA'], [u'Jon York', u'$44', u'CA']]\n",
      "CA [[u'Canada', u'CA'], [u'Sam Sneed', u'$1', u'CA']]\n",
      "CA [[u'Canada', u'CA'], [u'Yo Yo Ma', u'$2', u'CA']]\n",
      "UK [[u'United Kingdom', u'UK'], [u'Alex Ball', u'$5', u'UK']]\n",
      "UK [[u'United Kingdom', u'UK'], [u'Arnold Wesise', u'$400', u'UK']]\n",
      "\"MRRightJoin\" : 9 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from ReducerSideJoins import MRJoin, MRLeftJoin, MRRightJoin\n",
    "\n",
    "for join_type in [MRJoin, MRLeftJoin, MRRightJoin]:\n",
    "    mr_job = join_type([\"Countries.dat\", \"transactions.dat\"])    \n",
    "    with mr_job.make_runner() as runner:\n",
    "        runner.run()\n",
    "        counter = 0\n",
    "        for line in runner.stream_output():\n",
    "            counter += 1\n",
    "            key, value = mr_job.parse_output_line(line)\n",
    "            print key, value\n",
    "            \n",
    "        className=str(mr_job).split(\".\")[1].split(\" \")[0]    \n",
    "        print '\"%s\" : %d rows\\n' % (className, counter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5.3 Pairwise similarity  - PHASE 1\n",
    "\n",
    "In this part of the assignment we will focus on developing methods for detecting synonyms, using the Google 5-grams dataset. To accomplish this you must script three main tasks using MRJob:\n",
    "\n",
    "\n",
    "- (1) Using the systems tests data sets, write mrjob code to build the stripes\n",
    "- (2) Write mrjob code to build an inverted index from the stripes\n",
    "- (3) Using two (symmetric) comparison methods of your choice (e.g., correlations, distances, similarities), pairwise compare all stripes (vectors), and output to a file.   \n",
    "\n",
    "\n",
    "For this task you will have to determine a method of comparison.\n",
    "Here are a few that you might consider:\n",
    "\n",
    " - Jaccard\n",
    " - Cosine similarity\n",
    " - Spearman correlation\n",
    " - Euclidean distance\n",
    " - Taxicab (Manhattan) distance\n",
    " - Shortest path graph distance (a graph, because our data is symmetric!)\n",
    " - Pearson correlation\n",
    " - Kendall correlation\n",
    " ...\n",
    "\n",
    "However, be cautioned that some comparison methods are more difficult to parallelize than others, and do not perform more associations than is necessary, since your choice of association will be symmetric.\n",
    "\n",
    "Please use the inverted index (discussed in live session #5) based pattern to compute the pairwise (term-by-term) similarity matrix. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting buildStripes.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile buildStripes.py\n",
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "from __future__ import division\n",
    "import sys\n",
    "import re\n",
    "import mrjob\n",
    "import itertools\n",
    "import json\n",
    "from mrjob.protocol import RawProtocol\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "\n",
    "\n",
    "class MRbuildStripes(MRJob):\n",
    "\n",
    "  #START SUDENT CODE531_STRIPES\n",
    "\n",
    "    # Init Settings\n",
    "    SORT_VALUES = True\n",
    "    H = {}\n",
    "    \n",
    "    def steps(self):\n",
    "        \n",
    "        JOBCONF_STEP = {\n",
    "                'mapreduce.job.output.key.comparator.class': 'org.apache.hadoop.mapred.lib.KeyFieldBasedComparator',\n",
    "                'mapreduce.partition.keycomparator.options': '-k1'\n",
    "                }  \n",
    "        return [\n",
    "        MRStep(jobconf=JOBCONF_STEP, \n",
    "               mapper=self.mapper,\n",
    "               reducer=self.reducer),\n",
    "        MRStep(mapper=self.mapper_build\n",
    "              )\n",
    "        ]\n",
    "    \n",
    "    \n",
    "    def mapper(self, _, line):\n",
    "        sys.stderr.write(\"reporter:counter:Mapper Counters,Calls,1\\n\")\n",
    "        \n",
    "        words = re.findall(r'[a-z\\']+', line.lower())\n",
    "        counts = re.findall(r'[0-9]+', line)[0]\n",
    "\n",
    "        for subset in itertools.combinations(sorted(set(words)), 2):\n",
    "        \n",
    "            if subset[0] not in self.H.keys():\n",
    "                self.H[subset[0]] = {}\n",
    "                self.H[subset[0]][subset[1]] = counts \n",
    "            elif subset[1] not in self.H[subset[0]]:\n",
    "                self.H[subset[0]][subset[1]] = counts\n",
    "            else:\n",
    "                self.H[subset[0]][subset[1]] = counts\n",
    "\n",
    "            if subset[1] not in self.H.keys():\n",
    "                self.H[subset[1]] = {}\n",
    "                self.H[subset[1]][subset[0]] = counts \n",
    "            elif subset[0] not in self.H[subset[1]]:\n",
    "                self.H[subset[1]][subset[0]] = counts\n",
    "            else:\n",
    "                self.H[subset[1]][subset[0]] = counts\n",
    "        \n",
    "        for key in self.H:\n",
    "            yield key, self.H[key]\n",
    "         \n",
    "            \n",
    "    def reducer(self, key, values):\n",
    "        sys.stderr.write(\"reporter:counter:Reducer Counters,Calls,1\\n\")\n",
    "        yield key, values\n",
    "        \n",
    "    \n",
    "    def mapper_build(self, key, values):    \n",
    "        sys.stderr.write(\"reporter:counter:Final Mapper Counters,Calls,1\\n\")\n",
    "        z = {}\n",
    "        for i in values:\n",
    "            z.update(i)\n",
    "        z = {k:int(v) for k, v in z.items()}\n",
    "        yield key, z\n",
    "  \n",
    "  #END SUDENT CODE531_STRIPES\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    MRbuildStripes.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting invertedIndex.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile invertedIndex.py\n",
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "from __future__ import division\n",
    "import collections\n",
    "import sys\n",
    "import re\n",
    "import json\n",
    "import math\n",
    "import numpy as np\n",
    "import itertools\n",
    "import mrjob\n",
    "from mrjob.protocol import RawProtocol\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "\n",
    "class MRinvertedIndex(MRJob):\n",
    "    \n",
    "    #START SUDENT CODE531_INV_INDEX\n",
    "    \n",
    "    SORT_VALUES = True\n",
    "    \n",
    "    def steps(self):\n",
    "        \n",
    "        JOBCONF_STEP = {\n",
    "                'mapreduce.job.output.key.comparator.class': 'org.apache.hadoop.mapred.lib.KeyFieldBasedComparator',\n",
    "                'mapreduce.partition.keycomparator.options': '-k1'\n",
    "                }  \n",
    "        return [\n",
    "        MRStep(jobconf=JOBCONF_STEP, \n",
    "               mapper=self.mapper,\n",
    "               reducer=self.reducer)\n",
    "        ]\n",
    "    \n",
    "    def mapper(self, _, line):\n",
    "        sys.stderr.write(\"reporter:counter:Mapper Counters,Calls,1\\n\")\n",
    "        \n",
    "        tokens = line.strip().split('\\t')\n",
    "        value_dict = json.loads(tokens[1])\n",
    "        term_len = len(value_dict)\n",
    "        \n",
    "        for key in value_dict.keys():\n",
    "            yield key, [tokens[0], term_len]\n",
    "\n",
    "            \n",
    "    def reducer(self, key, values):\n",
    "        sys.stderr.write(\"reporter:counter:Reducer Counters,Calls,1\\n\")\n",
    "        out = []\n",
    "        for value_dict in values:\n",
    "            value_dict[0] = value_dict[0].replace('\"','')\n",
    "            out.append(value_dict)\n",
    "        \n",
    "        yield key, out\n",
    "\n",
    "\n",
    "  #END SUDENT CODE531_INV_INDEX\n",
    "        \n",
    "if __name__ == '__main__':\n",
    "    MRinvertedIndex.run() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting similarity.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile similarity.py\n",
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "from __future__ import division\n",
    "import sys\n",
    "import collections\n",
    "import re\n",
    "import json\n",
    "import math\n",
    "import numpy as np\n",
    "import itertools\n",
    "import mrjob\n",
    "from mrjob.protocol import RawProtocol\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "\n",
    "class MRsimilarity(MRJob):\n",
    "  \n",
    "    #START SUDENT CODE531_SIMILARITY\n",
    "    \n",
    "    SORT_VALUES = True\n",
    "    \n",
    "    def steps(self):\n",
    "        \n",
    "        JOBCONF_STEP = {\n",
    "                'mapreduce.job.output.key.comparator.class': 'org.apache.hadoop.mapred.lib.KeyFieldBasedComparator',\n",
    "                'mapreduce.partition.keycomparator.options': '-k1,1nr',\n",
    "                }  \n",
    "        return [\n",
    "        MRStep(#jobconf=JOBCONF_STEP, \n",
    "               mapper=self.mapper_pair_sim,\n",
    "               reducer=self.reducer_pair_sim\n",
    "            ),\n",
    "        MRStep(jobconf=JOBCONF_STEP, \n",
    "               mapper=None,\n",
    "               reducer=self.reducer_pair_sim2\n",
    "            ), \n",
    "        MRStep(jobconf=JOBCONF_STEP, \n",
    "               mapper=self.mapper_pair_sim2,\n",
    "               reducer=self.reducer_pair_sim_final\n",
    "              )\n",
    "        ]\n",
    "\n",
    "    def mapper_pair_sim(self, _, line):     \n",
    "        sys.stderr.write(\"reporter:counter:Mapper Counters,Calls,1\\n\")\n",
    "        line = line.strip()\n",
    "        index, posting = line.split('\\t')\n",
    "        posting = json.loads(posting)\n",
    "        posting = dict(posting)\n",
    "        \n",
    "        for docs in itertools.combinations(sorted(posting.keys()), 2):\n",
    "            yield (docs, posting[docs[0]], posting[docs[1]]), 1\n",
    "        \n",
    "\n",
    "    def reducer_pair_sim(self, key, values):\n",
    "        sys.stderr.write(\"reporter:counter:Reducer Counters,Calls,1\\n\")\n",
    "        total = sum(values)\n",
    "        cosine = total/(np.sqrt(key[1])*np.sqrt(key[2]))\n",
    "        jacard = total/(key[1]+key[2]-total)\n",
    "        overlap = total/min(key[1],key[2])\n",
    "        dice = 2*total/(key[1]+key[2])\n",
    "        \n",
    "        yield (cosine,jacard,overlap,dice), (key[0][0]+' - '+key[0][1]) \n",
    "\n",
    "\n",
    "    def reducer_pair_sim2(self, key, values):\n",
    "        sys.stderr.write(\"reporter:counter:Final Reducer Counters,Calls,1\\n\")\n",
    "        for value in values:\n",
    "            avg = np.mean(key)\n",
    "            yield avg, (value, key[0], key[1], key[2], key[3])\n",
    "    \n",
    "    def mapper_pair_sim2(self,key,values):\n",
    "        sys.stderr.write(\"reporter:counter:Final Mapper Counters,Calls,1\\n\")\n",
    "        yield key, values\n",
    "        \n",
    "    def reducer_pair_sim_final(self,key,values):\n",
    "        sys.stderr.write(\"reporter:counter:Final Reducer Counters,Calls,1\\n\")\n",
    "        yield key, values\n",
    "        \n",
    "        \n",
    "    #END SUDENT CODE531_SIMILARITY\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    MRsimilarity.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW5.3.1   Run Systems tests locally on small datasets (PHASE1)\n",
    "\n",
    "Complete 5.3 and systems test using the below test datasets. Phase 2 will focus on the entire Ngram dataset.\n",
    "\n",
    "To help you through these tasks please verify that your code gives the results below (for stripes, inverted index, and pairwise similarities).\n",
    "\n",
    "Test datasets:\n",
    "\n",
    "* googlebooks-eng-all-5gram-20090715-0-filtered.txt [see below]\n",
    "* atlas-boon-test [see below]\n",
    "* stripe-docs-test [see below]\n",
    "\n",
    "\n",
    "A large subset of the Google n-grams dataset   \n",
    "https://aws.amazon.com/datasets/google-books-ngrams/\n",
    "\n",
    "which we have placed in a bucket/folder on Dropbox and on s3:   \n",
    "https://www.dropbox.com/sh/tmqpc4o0xswhkvz/AACUifrl6wrMrlK6a3X3lZ9Ea?dl=0 \n",
    "\n",
    "You can download from Dropbox using wget:   \n",
    "`wget -O googlebooks-eng-all-5gram-20090715-0-filtered.txt https://www.dropbox.com/sh/tmqpc4o0xswhkvz/AACr50woxiBWoaiiLmnwduX8a/googlebooks-eng-all-5gram-20090715-0-filtered.txt?dl=0`\n",
    "\n",
    "Or access the files direclty from S3 if using EMR   \n",
    "`s3://filtered-5grams/`\n",
    "\n",
    "\n",
    "\n",
    "In particular, this bucket contains (~200) files (10Meg each) in the format:\n",
    "\n",
    "\t(ngram) \\t (count) \\t (pages_count) \\t (books_count)\n",
    "\n",
    "The next cell shows the first 10 lines of the googlebooks-eng-all-5gram-20090715-0-filtered.txt file.\n",
    "\n",
    "\n",
    "__DISCLAIMER__: Each record is already a 5-gram. In real life, we would calculate the stripes cooccurrence data from the raw text by windowing over the raw text and not from the 5-gram preprocessed data (as we are doing here).  Calculatating pairs on this 5-gram is a little corrupt as we will be double counting cooccurences. Having said that this exercise can still pull out some simialr terms. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1: unit/systems first-10-lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 443,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting googlebooks-eng-all-5gram-20090715-0-filtered-first-10-lines.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile googlebooks-eng-all-5gram-20090715-0-filtered-first-10-lines.txt\n",
    "A BILL FOR ESTABLISHING RELIGIOUS\t59\t59\t54\n",
    "A Biography of General George\t92\t90\t74\n",
    "A Case Study in Government\t102\t102\t78\n",
    "A Case Study of Female\t447\t447\t327\n",
    "A Case Study of Limited\t55\t55\t43\n",
    "A Child's Christmas in Wales\t1099\t1061\t866\n",
    "A Circumstantial Narrative of the\t62\t62\t50\n",
    "A City by the Sea\t62\t60\t49\n",
    "A Collection of Fairy Tales\t123\t117\t80\n",
    "A Collection of Forms of\t116\t103\t82"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2: unit/systems atlas-boon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 444,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting atlas-boon-systems-test.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile atlas-boon-systems-test.txt\n",
    "atlas boon\t50\t50\t50\n",
    "boon cava dipped\t10\t10\t10\n",
    "atlas dipped\t15\t15\t15"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3: unit/systems stripe-docs-test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Three terms, A,B,C and their corresponding stripe-docs of co-occurring terms\n",
    "\n",
    "- DocA {X:20, Y:30, Z:5}\n",
    "- DocB {X:100, Y:20}\n",
    "- DocC {M:5, N:20, Z:5}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1) build stripes for all the test data sets - run the commands and insure that your output matches the output below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm: Unknown command\n",
      "Did you mean -rm?  This command begins with a dash.\n",
      "No configs found; falling back on auto-configuration\n",
      "No configs specified for hadoop runner\n",
      "Looking for hadoop binary in $PATH...\n",
      "Found hadoop binary: /usr/bin/hadoop\n",
      "Using Hadoop version 2.6.0\n",
      "Looking for Hadoop streaming jar in /home/hadoop/contrib...\n",
      "Looking for Hadoop streaming jar in /usr/lib/hadoop-mapreduce...\n",
      "Found Hadoop streaming jar: /usr/lib/hadoop-mapreduce/hadoop-streaming.jar\n",
      "Creating temp directory /tmp/buildStripes.root.20180215.145123.403741\n",
      "Copying local files to hdfs:///user/root/tmp/mrjob/buildStripes.root.20180215.145123.403741/files/...\n",
      "Running step 1 of 2...\n",
      "  packageJobJar: [] [/usr/jars/hadoop-streaming-2.6.0-cdh5.7.0.jar] /tmp/streamjob4989179253487171320.jar tmpDir=null\n",
      "  Connecting to ResourceManager at /0.0.0.0:8032\n",
      "  Connecting to ResourceManager at /0.0.0.0:8032\n",
      "  Total input paths to process : 1\n",
      "  number of splits:2\n",
      "  Submitting tokens for job: job_1518662239671_0048\n",
      "  Submitted application application_1518662239671_0048\n",
      "  The url to track the job: http://docker.w261:8088/proxy/application_1518662239671_0048/\n",
      "  Running job: job_1518662239671_0048\n",
      "  Job job_1518662239671_0048 running in uber mode : false\n",
      "   map 0% reduce 0%\n",
      "   map 50% reduce 0%\n",
      "   map 100% reduce 0%\n",
      "   map 100% reduce 100%\n",
      "  Job job_1518662239671_0048 completed successfully\n",
      "  Output directory: hdfs:///user/root/tmp/mrjob/buildStripes.root.20180215.145123.403741/step-output/0000\n",
      "Counters: 51\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=563\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=7916\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=8934\n",
      "\t\tFILE: Number of bytes written=374849\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=995\n",
      "\t\tHDFS: Number of bytes written=7916\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of read operations=9\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tData-local map tasks=2\n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tTotal megabyte-seconds taken by all map tasks=4702208\n",
      "\t\tTotal megabyte-seconds taken by all reduce tasks=2481152\n",
      "\t\tTotal time spent by all map tasks (ms)=4592\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=4592\n",
      "\t\tTotal time spent by all reduce tasks (ms)=2423\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=2423\n",
      "\t\tTotal vcore-seconds taken by all map tasks=4592\n",
      "\t\tTotal vcore-seconds taken by all reduce tasks=2423\n",
      "\tMap-Reduce Framework\n",
      "\t\tCPU time spent (ms)=1580\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tGC time elapsed (ms)=39\n",
      "\t\tInput split bytes=432\n",
      "\t\tMap input records=10\n",
      "\t\tMap output bytes=8694\n",
      "\t\tMap output materialized bytes=8940\n",
      "\t\tMap output records=113\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tPhysical memory (bytes) snapshot=940740608\n",
      "\t\tReduce input groups=49\n",
      "\t\tReduce input records=113\n",
      "\t\tReduce output records=28\n",
      "\t\tReduce shuffle bytes=8940\n",
      "\t\tShuffled Maps =2\n",
      "\t\tSpilled Records=226\n",
      "\t\tTotal committed heap usage (bytes)=1513095168\n",
      "\t\tVirtual memory (bytes) snapshot=4243832832\n",
      "\tMapper Counters\n",
      "\t\tCalls=10\n",
      "\tReducer Counters\n",
      "\t\tCalls=28\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "Running step 2 of 2...\n",
      "  packageJobJar: [] [/usr/jars/hadoop-streaming-2.6.0-cdh5.7.0.jar] /tmp/streamjob7807017132322164642.jar tmpDir=null\n",
      "  Connecting to ResourceManager at /0.0.0.0:8032\n",
      "  Connecting to ResourceManager at /0.0.0.0:8032\n",
      "  Total input paths to process : 1\n",
      "  number of splits:2\n",
      "  Submitting tokens for job: job_1518662239671_0049\n",
      "  Submitted application application_1518662239671_0049\n",
      "  The url to track the job: http://docker.w261:8088/proxy/application_1518662239671_0049/\n",
      "  Running job: job_1518662239671_0049\n",
      "  Job job_1518662239671_0049 running in uber mode : false\n",
      "   map 0% reduce 0%\n",
      "   map 50% reduce 0%\n",
      "   map 100% reduce 0%\n",
      "  Job job_1518662239671_0049 completed successfully\n",
      "  Output directory: hdfs:///user/root/tmp/mrjob/buildStripes.root.20180215.145123.403741/output\n",
      "Counters: 31\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=11874\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=2132\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=0\n",
      "\t\tFILE: Number of bytes written=236370\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=12218\n",
      "\t\tHDFS: Number of bytes written=2132\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of read operations=10\n",
      "\t\tHDFS: Number of write operations=4\n",
      "\tFinal Mapper Counters\n",
      "\t\tCalls=28\n",
      "\tJob Counters \n",
      "\t\tData-local map tasks=2\n",
      "\t\tLaunched map tasks=2\n",
      "\t\tTotal megabyte-seconds taken by all map tasks=4816896\n",
      "\t\tTotal time spent by all map tasks (ms)=4704\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=4704\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=0\n",
      "\t\tTotal vcore-seconds taken by all map tasks=4704\n",
      "\tMap-Reduce Framework\n",
      "\t\tCPU time spent (ms)=980\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tGC time elapsed (ms)=26\n",
      "\t\tInput split bytes=344\n",
      "\t\tMap input records=28\n",
      "\t\tMap output records=28\n",
      "\t\tMerged Map outputs=0\n",
      "\t\tPhysical memory (bytes) snapshot=471650304\n",
      "\t\tSpilled Records=0\n",
      "\t\tTotal committed heap usage (bytes)=1008730112\n",
      "\t\tVirtual memory (bytes) snapshot=2834440192\n",
      "Streaming final output from hdfs:///user/root/tmp/mrjob/buildStripes.root.20180215.145123.403741/output...\n",
      "Removing HDFS temp directory hdfs:///user/root/tmp/mrjob/buildStripes.root.20180215.145123.403741...\n",
      "Removing temp directory /tmp/buildStripes.root.20180215.145123.403741...\n"
     ]
    }
   ],
   "source": [
    "###########################################################################\n",
    "# Make Stripes from ngrams for systems test 1\n",
    "###########################################################################\n",
    "\n",
    "!hdfs dfs rm --recursive systems_test_stripes_1\n",
    "!python buildStripes.py -r hadoop --cmdenv PATH=/opt/anaconda/bin:$PATH googlebooks-eng-all-5gram-20090715-0-filtered-first-10-lines.txt > systems_test_stripes_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"a\"\t{\"limited\":55,\"sea\":62,\"general\":92,\"female\":447,\"in\":1099,\"religious\":59,\"george\":92,\"biography\":92,\"city\":62,\"for\":59,\"tales\":123,\"government\":102,\"the\":62,\"forms\":116,\"wales\":1099,\"christmas\":1099,\"child's\":1099,\"collection\":123,\"by\":62,\"case\":55,\"circumstantial\":62,\"of\":92,\"study\":55,\"bill\":59,\"establishing\":59,\"narrative\":62,\"fairy\":123}\t\n",
      "\"bill\"\t{\"a\":59,\"religious\":59,\"for\":59,\"establishing\":59}\t\n",
      "\"biography\"\t{\"a\":92,\"of\":92,\"george\":92,\"general\":92}\t\n",
      "\"by\"\t{\"a\":62,\"city\":62,\"the\":62,\"sea\":62}\t\n",
      "\"case\"\t{\"a\":55,\"limited\":55,\"government\":102,\"of\":55,\"study\":55,\"female\":447,\"in\":102}\t\n",
      "\"child's\"\t{\"a\":1099,\"wales\":1099,\"christmas\":1099,\"in\":1099}\t\n",
      "\"christmas\"\t{\"a\":1099,\"wales\":1099,\"in\":1099,\"child's\":1099}\t\n",
      "\"circumstantial\"\t{\"a\":62,\"of\":62,\"the\":62,\"narrative\":62}\t\n",
      "\"city\"\t{\"a\":62,\"the\":62,\"by\":62,\"sea\":62}\t\n",
      "\"collection\"\t{\"a\":123,\"of\":123,\"tales\":123,\"fairy\":123,\"forms\":116}\t\n",
      "\"establishing\"\t{\"a\":59,\"bill\":59,\"religious\":59,\"for\":59}\t\n",
      "\"fairy\"\t{\"a\":123,\"of\":123,\"tales\":123,\"collection\":123}\t\n",
      "\"female\"\t{\"a\":447,\"case\":447,\"study\":447,\"of\":447}\t\n",
      "\"for\"\t{\"a\":59,\"bill\":59,\"religious\":59,\"establishing\":59}\t\n",
      "\"forms\"\t{\"a\":116,\"of\":116,\"collection\":116}\t\n",
      "\"general\"\t{\"a\":92,\"of\":92,\"george\":92,\"biography\":92}\t\n",
      "\"george\"\t{\"a\":92,\"of\":92,\"biography\":92,\"general\":92}\t\n",
      "\"government\"\t{\"a\":102,\"case\":102,\"study\":102,\"in\":102}\t\n",
      "\"in\"\t{\"a\":1099,\"case\":102,\"child's\":1099,\"study\":102,\"government\":102,\"wales\":1099,\"christmas\":1099}\t\n",
      "\"limited\"\t{\"a\":55,\"case\":55,\"study\":55,\"of\":55}\t\n",
      "\"narrative\"\t{\"a\":62,\"of\":62,\"the\":62,\"circumstantial\":62}\t\n",
      "\"of\"\t{\"case\":55,\"limited\":55,\"circumstantial\":62,\"the\":62,\"tales\":123,\"collection\":123,\"general\":92,\"forms\":116,\"a\":92,\"female\":447,\"narrative\":62,\"study\":55,\"fairy\":123,\"george\":92,\"biography\":92}\t\n",
      "\"religious\"\t{\"a\":59,\"bill\":59,\"for\":59,\"establishing\":59}\t\n",
      "\"sea\"\t{\"a\":62,\"city\":62,\"the\":62,\"by\":62}\t\n",
      "\"study\"\t{\"a\":55,\"case\":55,\"limited\":55,\"government\":102,\"of\":55,\"female\":447,\"in\":102}\t\n",
      "\"tales\"\t{\"a\":123,\"of\":123,\"fairy\":123,\"collection\":123}\t\n",
      "\"the\"\t{\"a\":62,\"city\":62,\"circumstantial\":62,\"of\":62,\"sea\":62,\"narrative\":62,\"by\":62}\t\n",
      "\"wales\"\t{\"a\":1099,\"in\":1099,\"christmas\":1099,\"child's\":1099}\t\n"
     ]
    }
   ],
   "source": [
    "# Print out and post-process data (to be the same as example)\n",
    "!cat systems_test_stripes_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Original"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<pre>\n",
    "\"a\"\t{\"limited\": 55, \"female\": 447, \"general\": 92, \"sea\": 62, \"in\": 1201, \"religious\": 59, \"george\": 92, \"biography\": 92, \"city\": 62, \"for\": 59, \"tales\": 123, \"child's\": 1099, \"by\": 62, \"forms\": 116, \"wales\": 1099, \"christmas\": 1099, \"government\": 102, \"collection\": 239, \"fairy\": 123, \"case\": 604, \"circumstantial\": 62, \"of\": 1011, \"study\": 604, \"bill\": 59, \"establishing\": 59, \"narrative\": 62, \"the\": 124}\n",
    "\"bill\"\t{\"a\": 59, \"religious\": 59, \"for\": 59, \"establishing\": 59}\n",
    "\"biography\"\t{\"a\": 92, \"of\": 92, \"george\": 92, \"general\": 92}\n",
    "\"by\"\t{\"a\": 62, \"city\": 62, \"the\": 62, \"sea\": 62}\n",
    "\"case\"\t{\"a\": 604, \"limited\": 55, \"government\": 102, \"of\": 502, \"study\": 604, \"female\": 447, \"in\": 102}\n",
    "\"child's\"\t{\"a\": 1099, \"wales\": 1099, \"christmas\": 1099, \"in\": 1099}\n",
    "\"christmas\"\t{\"a\": 1099, \"wales\": 1099, \"in\": 1099, \"child's\": 1099}\n",
    "\"circumstantial\"\t{\"a\": 62, \"of\": 62, \"the\": 62, \"narrative\": 62}\n",
    "\"city\"\t{\"a\": 62, \"the\": 62, \"by\": 62, \"sea\": 62}\n",
    "\"collection\"\t{\"a\": 239, \"forms\": 116, \"fairy\": 123, \"tales\": 123, \"of\": 355}\n",
    "\"establishing\"\t{\"a\": 59, \"bill\": 59, \"religious\": 59, \"for\": 59}\n",
    "\"fairy\"\t{\"a\": 123, \"of\": 123, \"tales\": 123, \"collection\": 123}\n",
    "\"female\"\t{\"a\": 447, \"case\": 447, \"study\": 447, \"of\": 447}\n",
    "\"for\"\t{\"a\": 59, \"bill\": 59, \"religious\": 59, \"establishing\": 59}\n",
    "\"forms\"\t{\"a\": 116, \"of\": 232, \"collection\": 116}\n",
    "\"general\"\t{\"a\": 92, \"of\": 92, \"george\": 92, \"biography\": 92}\n",
    "\"george\"\t{\"a\": 92, \"of\": 92, \"biography\": 92, \"general\": 92}\n",
    "\"government\"\t{\"a\": 102, \"case\": 102, \"study\": 102, \"in\": 102}\n",
    "\"in\"\t{\"a\": 1201, \"case\": 102, \"government\": 102, \"study\": 102, \"child's\": 1099, \"wales\": 1099, \"christmas\": 1099}\n",
    "\"limited\"\t{\"a\": 55, \"case\": 55, \"study\": 55, \"of\": 55}\n",
    "\"narrative\"\t{\"a\": 62, \"of\": 62, \"the\": 62, \"circumstantial\": 62}\n",
    "\"of\"\t{\"a\": 1011, \"case\": 502, \"circumstantial\": 62, \"george\": 92, \"limited\": 55, \"tales\": 123, \"collection\": 355, \"general\": 92, \"forms\": 232, \"female\": 447, \"narrative\": 62, \"study\": 502, \"fairy\": 123, \"the\": 62, \"biography\": 92}\n",
    "\"religious\"\t{\"a\": 59, \"bill\": 59, \"for\": 59, \"establishing\": 59}\n",
    "\"sea\"\t{\"a\": 62, \"city\": 62, \"the\": 62, \"by\": 62}\n",
    "\"study\"\t{\"a\": 604, \"case\": 604, \"limited\": 55, \"government\": 102, \"of\": 502, \"female\": 447, \"in\": 102}\n",
    "\"tales\"\t{\"a\": 123, \"of\": 123, \"fairy\": 123, \"collection\": 123}\n",
    "\"the\"\t{\"a\": 124, \"city\": 62, \"circumstantial\": 62, \"of\": 62, \"sea\": 62, \"narrative\": 62, \"by\": 62}\n",
    "\"wales\"\t{\"a\": 1099, \"in\": 1099, \"christmas\": 1099, \"child's\": 1099}\n",
    "\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm: Unknown command\n",
      "Did you mean -rm?  This command begins with a dash.\n",
      "No configs found; falling back on auto-configuration\n",
      "No configs specified for hadoop runner\n",
      "Looking for hadoop binary in $PATH...\n",
      "Found hadoop binary: /usr/bin/hadoop\n",
      "Using Hadoop version 2.6.0\n",
      "Looking for Hadoop streaming jar in /home/hadoop/contrib...\n",
      "Looking for Hadoop streaming jar in /usr/lib/hadoop-mapreduce...\n",
      "Found Hadoop streaming jar: /usr/lib/hadoop-mapreduce/hadoop-streaming.jar\n",
      "Creating temp directory /tmp/buildStripes.root.20180215.145209.618924\n",
      "Copying local files to hdfs:///user/root/tmp/mrjob/buildStripes.root.20180215.145209.618924/files/...\n",
      "Running step 1 of 2...\n",
      "  packageJobJar: [] [/usr/jars/hadoop-streaming-2.6.0-cdh5.7.0.jar] /tmp/streamjob6813838327385954048.jar tmpDir=null\n",
      "  Connecting to ResourceManager at /0.0.0.0:8032\n",
      "  Connecting to ResourceManager at /0.0.0.0:8032\n",
      "  Total input paths to process : 1\n",
      "  number of splits:2\n",
      "  Submitting tokens for job: job_1518662239671_0050\n",
      "  Submitted application application_1518662239671_0050\n",
      "  The url to track the job: http://docker.w261:8088/proxy/application_1518662239671_0050/\n",
      "  Running job: job_1518662239671_0050\n",
      "  Job job_1518662239671_0050 running in uber mode : false\n",
      "   map 0% reduce 0%\n",
      "   map 50% reduce 0%\n",
      "   map 100% reduce 0%\n",
      "   map 100% reduce 100%\n",
      "  Job job_1518662239671_0050 completed successfully\n",
      "  Output directory: hdfs:///user/root/tmp/mrjob/buildStripes.root.20180215.145209.618924/step-output/0000\n",
      "Counters: 51\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=101\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=208\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=262\n",
      "\t\tFILE: Number of bytes written=357394\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=457\n",
      "\t\tHDFS: Number of bytes written=208\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of read operations=9\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tData-local map tasks=2\n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tTotal megabyte-seconds taken by all map tasks=4689920\n",
      "\t\tTotal megabyte-seconds taken by all reduce tasks=2476032\n",
      "\t\tTotal time spent by all map tasks (ms)=4580\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=4580\n",
      "\t\tTotal time spent by all reduce tasks (ms)=2418\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=2418\n",
      "\t\tTotal vcore-seconds taken by all map tasks=4580\n",
      "\t\tTotal vcore-seconds taken by all reduce tasks=2418\n",
      "\tMap-Reduce Framework\n",
      "\t\tCPU time spent (ms)=1620\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tGC time elapsed (ms)=42\n",
      "\t\tInput split bytes=356\n",
      "\t\tMap input records=3\n",
      "\t\tMap output bytes=240\n",
      "\t\tMap output materialized bytes=268\n",
      "\t\tMap output records=8\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tPhysical memory (bytes) snapshot=914636800\n",
      "\t\tReduce input groups=7\n",
      "\t\tReduce input records=8\n",
      "\t\tReduce output records=4\n",
      "\t\tReduce shuffle bytes=268\n",
      "\t\tShuffled Maps =2\n",
      "\t\tSpilled Records=16\n",
      "\t\tTotal committed heap usage (bytes)=1513095168\n",
      "\t\tVirtual memory (bytes) snapshot=4230496256\n",
      "\tMapper Counters\n",
      "\t\tCalls=3\n",
      "\tReducer Counters\n",
      "\t\tCalls=4\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "Running step 2 of 2...\n",
      "  packageJobJar: [] [/usr/jars/hadoop-streaming-2.6.0-cdh5.7.0.jar] /tmp/streamjob2576247764565535280.jar tmpDir=null\n",
      "  Connecting to ResourceManager at /0.0.0.0:8032\n",
      "  Connecting to ResourceManager at /0.0.0.0:8032\n",
      "  Total input paths to process : 1\n",
      "  number of splits:2\n",
      "  Submitting tokens for job: job_1518662239671_0051\n",
      "  Submitted application application_1518662239671_0051\n",
      "  The url to track the job: http://docker.w261:8088/proxy/application_1518662239671_0051/\n",
      "  Running job: job_1518662239671_0051\n",
      "  Job job_1518662239671_0051 running in uber mode : false\n",
      "   map 0% reduce 0%\n",
      "   map 50% reduce 0%\n",
      "   map 100% reduce 0%\n",
      "  Job job_1518662239671_0051 completed successfully\n",
      "  Output directory: hdfs:///user/root/tmp/mrjob/buildStripes.root.20180215.145209.618924/output\n",
      "Counters: 31\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=312\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=151\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=0\n",
      "\t\tFILE: Number of bytes written=236370\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=656\n",
      "\t\tHDFS: Number of bytes written=151\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of read operations=10\n",
      "\t\tHDFS: Number of write operations=4\n",
      "\tFinal Mapper Counters\n",
      "\t\tCalls=4\n",
      "\tJob Counters \n",
      "\t\tData-local map tasks=2\n",
      "\t\tLaunched map tasks=2\n",
      "\t\tTotal megabyte-seconds taken by all map tasks=4911104\n",
      "\t\tTotal time spent by all map tasks (ms)=4796\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=4796\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=0\n",
      "\t\tTotal vcore-seconds taken by all map tasks=4796\n",
      "\tMap-Reduce Framework\n",
      "\t\tCPU time spent (ms)=1160\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tGC time elapsed (ms)=26\n",
      "\t\tInput split bytes=344\n",
      "\t\tMap input records=4\n",
      "\t\tMap output records=4\n",
      "\t\tMerged Map outputs=0\n",
      "\t\tPhysical memory (bytes) snapshot=491069440\n",
      "\t\tSpilled Records=0\n",
      "\t\tTotal committed heap usage (bytes)=1008730112\n",
      "\t\tVirtual memory (bytes) snapshot=2815524864\n",
      "Streaming final output from hdfs:///user/root/tmp/mrjob/buildStripes.root.20180215.145209.618924/output...\n",
      "Removing HDFS temp directory hdfs:///user/root/tmp/mrjob/buildStripes.root.20180215.145209.618924...\n",
      "Removing temp directory /tmp/buildStripes.root.20180215.145209.618924...\n"
     ]
    }
   ],
   "source": [
    "###########################################################################\n",
    "# Make Stripes from ngrams for systems test 2\n",
    "#####################################################################|######\n",
    "\n",
    "!hdfs dfs rm --recursive systems_test_stripes_2\n",
    "!python buildStripes.py -r hadoop --cmdenv PATH=/opt/anaconda/bin:$PATH atlas-boon-systems-test.txt > systems_test_stripes_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"atlas\"\t{\"dipped\":15,\"boon\":50}\t\n",
      "\"boon\"\t{\"atlas\":50,\"dipped\":10,\"cava\":10}\t\n",
      "\"cava\"\t{\"dipped\":10,\"boon\":10}\t\n",
      "\"dipped\"\t{\"atlas\":15,\"boon\":10,\"cava\":10}\t\n"
     ]
    }
   ],
   "source": [
    "# Print data \n",
    "!cat systems_test_stripes_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Original"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<pre>\n",
    "\"atlas\"   {\"dipped\": 15, \"boon\": 50}   \n",
    "\"boon\"    {\"atlas\": 50, \"dipped\": 10, \"cava\": 10}   \n",
    "\"cava\"    {\"dipped\": 10, \"boon\": 10} \n",
    "\"dipped\"  {\"atlas\": 15, \"boon\": 10, \"cava\": 10}\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"DocA\"\t{\"X\":20, \"Y\":30, \"Z\":5}\n",
      "\"DocB\"\t{\"X\":100, \"Y\":20}\n",
      "\"DocC\"\t{\"M\":5, \"N\":20, \"Z\":5, \"Y\":1}\n"
     ]
    }
   ],
   "source": [
    "########################################################################\n",
    "# Stripes for systems test 3 (given, no need to build stripes)\n",
    "########################################################################\n",
    "\n",
    "with open(\"systems_test_stripes_3\", \"w\") as f:\n",
    "    f.writelines([\n",
    "        '\"DocA\"\\t{\"X\":20, \"Y\":30, \"Z\":5}\\n',\n",
    "        '\"DocB\"\\t{\"X\":100, \"Y\":20}\\n',  \n",
    "        '\"DocC\"\\t{\"M\":5, \"N\":20, \"Z\":5, \"Y\":1}\\n'\n",
    "    ])\n",
    "!cat systems_test_stripes_3   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2) Build Inverted Index - run the commands and insure that your output matches the output below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No configs found; falling back on auto-configuration\n",
      "No configs specified for hadoop runner\n",
      "Looking for hadoop binary in $PATH...\n",
      "Found hadoop binary: /usr/bin/hadoop\n",
      "Using Hadoop version 2.6.0\n",
      "Looking for Hadoop streaming jar in /home/hadoop/contrib...\n",
      "Looking for Hadoop streaming jar in /usr/lib/hadoop-mapreduce...\n",
      "Found Hadoop streaming jar: /usr/lib/hadoop-mapreduce/hadoop-streaming.jar\n",
      "Creating temp directory /tmp/invertedIndex.root.20180215.145255.246025\n",
      "Copying local files to hdfs:///user/root/tmp/mrjob/invertedIndex.root.20180215.145255.246025/files/...\n",
      "Running step 1 of 1...\n",
      "  packageJobJar: [] [/usr/jars/hadoop-streaming-2.6.0-cdh5.7.0.jar] /tmp/streamjob4743632484152819092.jar tmpDir=null\n",
      "  Connecting to ResourceManager at /0.0.0.0:8032\n",
      "  Connecting to ResourceManager at /0.0.0.0:8032\n",
      "  Total input paths to process : 1\n",
      "  number of splits:2\n",
      "  Submitting tokens for job: job_1518662239671_0052\n",
      "  Submitted application application_1518662239671_0052\n",
      "  The url to track the job: http://docker.w261:8088/proxy/application_1518662239671_0052/\n",
      "  Running job: job_1518662239671_0052\n",
      "  Job job_1518662239671_0052 running in uber mode : false\n",
      "   map 0% reduce 0%\n",
      "   map 100% reduce 0%\n",
      "   map 100% reduce 100%\n",
      "  Job job_1518662239671_0052 completed successfully\n",
      "  Output directory: hdfs:///user/root/tmp/mrjob/invertedIndex.root.20180215.145255.246025/output\n",
      "Counters: 51\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=3198\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=2220\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=4262\n",
      "\t\tFILE: Number of bytes written=365394\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=3546\n",
      "\t\tHDFS: Number of bytes written=2220\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of read operations=9\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tData-local map tasks=2\n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tTotal megabyte-seconds taken by all map tasks=4746240\n",
      "\t\tTotal megabyte-seconds taken by all reduce tasks=2512896\n",
      "\t\tTotal time spent by all map tasks (ms)=4635\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=4635\n",
      "\t\tTotal time spent by all reduce tasks (ms)=2454\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=2454\n",
      "\t\tTotal vcore-seconds taken by all map tasks=4635\n",
      "\t\tTotal vcore-seconds taken by all reduce tasks=2454\n",
      "\tMap-Reduce Framework\n",
      "\t\tCPU time spent (ms)=1590\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tGC time elapsed (ms)=46\n",
      "\t\tInput split bytes=348\n",
      "\t\tMap input records=28\n",
      "\t\tMap output bytes=3940\n",
      "\t\tMap output materialized bytes=4268\n",
      "\t\tMap output records=158\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tPhysical memory (bytes) snapshot=922279936\n",
      "\t\tReduce input groups=158\n",
      "\t\tReduce input records=158\n",
      "\t\tReduce output records=28\n",
      "\t\tReduce shuffle bytes=4268\n",
      "\t\tShuffled Maps =2\n",
      "\t\tSpilled Records=316\n",
      "\t\tTotal committed heap usage (bytes)=1513095168\n",
      "\t\tVirtual memory (bytes) snapshot=4237139968\n",
      "\tMapper Counters\n",
      "\t\tCalls=28\n",
      "\tReducer Counters\n",
      "\t\tCalls=28\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "Streaming final output from hdfs:///user/root/tmp/mrjob/invertedIndex.root.20180215.145255.246025/output...\n",
      "Removing HDFS temp directory hdfs:///user/root/tmp/mrjob/invertedIndex.root.20180215.145255.246025...\n",
      "Removing temp directory /tmp/invertedIndex.root.20180215.145255.246025...\n"
     ]
    }
   ],
   "source": [
    "!python invertedIndex.py -r hadoop --cmdenv PATH=/opt/anaconda/bin:$PATH systems_test_stripes_1 > systems_test_index_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No configs found; falling back on auto-configuration\n",
      "No configs specified for hadoop runner\n",
      "Looking for hadoop binary in $PATH...\n",
      "Found hadoop binary: /usr/bin/hadoop\n",
      "Using Hadoop version 2.6.0\n",
      "Looking for Hadoop streaming jar in /home/hadoop/contrib...\n",
      "Looking for Hadoop streaming jar in /usr/lib/hadoop-mapreduce...\n",
      "Found Hadoop streaming jar: /usr/lib/hadoop-mapreduce/hadoop-streaming.jar\n",
      "Creating temp directory /tmp/invertedIndex.root.20180215.145326.821881\n",
      "Copying local files to hdfs:///user/root/tmp/mrjob/invertedIndex.root.20180215.145326.821881/files/...\n",
      "Running step 1 of 1...\n",
      "  packageJobJar: [] [/usr/jars/hadoop-streaming-2.6.0-cdh5.7.0.jar] /tmp/streamjob5298305788207410661.jar tmpDir=null\n",
      "  Connecting to ResourceManager at /0.0.0.0:8032\n",
      "  Connecting to ResourceManager at /0.0.0.0:8032\n",
      "  Total input paths to process : 1\n",
      "  number of splits:2\n",
      "  Submitting tokens for job: job_1518662239671_0053\n",
      "  Submitted application application_1518662239671_0053\n",
      "  The url to track the job: http://docker.w261:8088/proxy/application_1518662239671_0053/\n",
      "  Running job: job_1518662239671_0053\n",
      "  Job job_1518662239671_0053 running in uber mode : false\n",
      "   map 0% reduce 0%\n",
      "   map 100% reduce 0%\n",
      "   map 100% reduce 100%\n",
      "  Job job_1518662239671_0053 completed successfully\n",
      "  Output directory: hdfs:///user/root/tmp/mrjob/invertedIndex.root.20180215.145326.821881/output\n",
      "Counters: 51\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=227\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=157\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=272\n",
      "\t\tFILE: Number of bytes written=357414\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=575\n",
      "\t\tHDFS: Number of bytes written=157\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of read operations=9\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tData-local map tasks=2\n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tTotal megabyte-seconds taken by all map tasks=4737024\n",
      "\t\tTotal megabyte-seconds taken by all reduce tasks=2452480\n",
      "\t\tTotal time spent by all map tasks (ms)=4626\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=4626\n",
      "\t\tTotal time spent by all reduce tasks (ms)=2395\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=2395\n",
      "\t\tTotal vcore-seconds taken by all map tasks=4626\n",
      "\t\tTotal vcore-seconds taken by all reduce tasks=2395\n",
      "\tMap-Reduce Framework\n",
      "\t\tCPU time spent (ms)=1520\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tGC time elapsed (ms)=44\n",
      "\t\tInput split bytes=348\n",
      "\t\tMap input records=4\n",
      "\t\tMap output bytes=246\n",
      "\t\tMap output materialized bytes=278\n",
      "\t\tMap output records=10\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tPhysical memory (bytes) snapshot=919539712\n",
      "\t\tReduce input groups=10\n",
      "\t\tReduce input records=10\n",
      "\t\tReduce output records=4\n",
      "\t\tReduce shuffle bytes=278\n",
      "\t\tShuffled Maps =2\n",
      "\t\tSpilled Records=20\n",
      "\t\tTotal committed heap usage (bytes)=1513095168\n",
      "\t\tVirtual memory (bytes) snapshot=4247887872\n",
      "\tMapper Counters\n",
      "\t\tCalls=4\n",
      "\tReducer Counters\n",
      "\t\tCalls=4\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "Streaming final output from hdfs:///user/root/tmp/mrjob/invertedIndex.root.20180215.145326.821881/output...\n",
      "Removing HDFS temp directory hdfs:///user/root/tmp/mrjob/invertedIndex.root.20180215.145326.821881...\n",
      "Removing temp directory /tmp/invertedIndex.root.20180215.145326.821881...\n"
     ]
    }
   ],
   "source": [
    "!python invertedIndex.py -r hadoop --cmdenv PATH=/opt/anaconda/bin:$PATH systems_test_stripes_2 > systems_test_index_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No configs found; falling back on auto-configuration\n",
      "No configs specified for hadoop runner\n",
      "Looking for hadoop binary in $PATH...\n",
      "Found hadoop binary: /usr/bin/hadoop\n",
      "Using Hadoop version 2.6.0\n",
      "Looking for Hadoop streaming jar in /home/hadoop/contrib...\n",
      "Looking for Hadoop streaming jar in /usr/lib/hadoop-mapreduce...\n",
      "Found Hadoop streaming jar: /usr/lib/hadoop-mapreduce/hadoop-streaming.jar\n",
      "Creating temp directory /tmp/invertedIndex.root.20180215.145358.624551\n",
      "Copying local files to hdfs:///user/root/tmp/mrjob/invertedIndex.root.20180215.145358.624551/files/...\n",
      "Running step 1 of 1...\n",
      "  packageJobJar: [] [/usr/jars/hadoop-streaming-2.6.0-cdh5.7.0.jar] /tmp/streamjob8810081556622414214.jar tmpDir=null\n",
      "  Connecting to ResourceManager at /0.0.0.0:8032\n",
      "  Connecting to ResourceManager at /0.0.0.0:8032\n",
      "  Total input paths to process : 1\n",
      "  number of splits:2\n",
      "  Submitting tokens for job: job_1518662239671_0054\n",
      "  Submitted application application_1518662239671_0054\n",
      "  The url to track the job: http://docker.w261:8088/proxy/application_1518662239671_0054/\n",
      "  Running job: job_1518662239671_0054\n",
      "  Job job_1518662239671_0054 running in uber mode : false\n",
      "   map 0% reduce 0%\n",
      "   map 50% reduce 0%\n",
      "   map 100% reduce 0%\n",
      "   map 100% reduce 100%\n",
      "  Job job_1518662239671_0054 completed successfully\n",
      "  Output directory: hdfs:///user/root/tmp/mrjob/invertedIndex.root.20180215.145358.624551/output\n",
      "Counters: 51\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=140\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=129\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=204\n",
      "\t\tFILE: Number of bytes written=357278\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=488\n",
      "\t\tHDFS: Number of bytes written=129\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of read operations=9\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tData-local map tasks=2\n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tTotal megabyte-seconds taken by all map tasks=4710400\n",
      "\t\tTotal megabyte-seconds taken by all reduce tasks=2435072\n",
      "\t\tTotal time spent by all map tasks (ms)=4600\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=4600\n",
      "\t\tTotal time spent by all reduce tasks (ms)=2378\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=2378\n",
      "\t\tTotal vcore-seconds taken by all map tasks=4600\n",
      "\t\tTotal vcore-seconds taken by all reduce tasks=2378\n",
      "\tMap-Reduce Framework\n",
      "\t\tCPU time spent (ms)=1580\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tGC time elapsed (ms)=39\n",
      "\t\tInput split bytes=348\n",
      "\t\tMap input records=3\n",
      "\t\tMap output bytes=180\n",
      "\t\tMap output materialized bytes=210\n",
      "\t\tMap output records=9\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tPhysical memory (bytes) snapshot=912543744\n",
      "\t\tReduce input groups=9\n",
      "\t\tReduce input records=9\n",
      "\t\tReduce output records=5\n",
      "\t\tReduce shuffle bytes=210\n",
      "\t\tShuffled Maps =2\n",
      "\t\tSpilled Records=18\n",
      "\t\tTotal committed heap usage (bytes)=1513095168\n",
      "\t\tVirtual memory (bytes) snapshot=4244295680\n",
      "\tMapper Counters\n",
      "\t\tCalls=3\n",
      "\tReducer Counters\n",
      "\t\tCalls=5\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "Streaming final output from hdfs:///user/root/tmp/mrjob/invertedIndex.root.20180215.145358.624551/output...\n",
      "Removing HDFS temp directory hdfs:///user/root/tmp/mrjob/invertedIndex.root.20180215.145358.624551...\n",
      "Removing temp directory /tmp/invertedIndex.root.20180215.145358.624551...\n"
     ]
    }
   ],
   "source": [
    "!python invertedIndex.py -r hadoop --cmdenv PATH=/opt/anaconda/bin:$PATH systems_test_stripes_3 > systems_test_index_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Systems test  1  - Inverted Index\n",
      "\n",
      "             \"a\" |          bill 4 |     biography 4 |            by 4\n",
      "          \"bill\" |            a 27 |  establishing 4 |           for 4\n",
      "     \"biography\" |            a 27 |       general 4 |        george 4\n",
      "            \"by\" |            a 27 |          city 4 |           sea 4\n",
      "          \"case\" |            a 27 |        female 4 |    government 4\n",
      "       \"child's\" |            a 27 |     christmas 4 |            in 7\n",
      "     \"christmas\" |            a 27 |       child's 4 |            in 7\n",
      "\"circumstantial\" |            a 27 |     narrative 4 |           of 15\n",
      "          \"city\" |            a 27 |            by 4 |           sea 4\n",
      "    \"collection\" |            a 27 |         fairy 4 |         forms 3\n",
      "  \"establishing\" |            a 27 |          bill 4 |           for 4\n",
      "         \"fairy\" |            a 27 |    collection 5 |           of 15\n",
      "        \"female\" |            a 27 |          case 7 |           of 15\n",
      "           \"for\" |            a 27 |          bill 4 |  establishing 4\n",
      "         \"forms\" |            a 27 |    collection 5 |           of 15\n",
      "       \"general\" |            a 27 |     biography 4 |        george 4\n",
      "        \"george\" |            a 27 |     biography 4 |       general 4\n",
      "    \"government\" |            a 27 |          case 7 |            in 7\n",
      "            \"in\" |            a 27 |          case 7 |       child's 4\n",
      "       \"limited\" |            a 27 |          case 7 |           of 15\n",
      "     \"narrative\" |            a 27 |circumstantial 4 |           of 15\n",
      "            \"of\" |            a 27 |     biography 4 |          case 7\n",
      "     \"religious\" |            a 27 |          bill 4 |  establishing 4\n",
      "           \"sea\" |            a 27 |            by 4 |          city 4\n",
      "         \"study\" |            a 27 |          case 7 |        female 4\n",
      "         \"tales\" |            a 27 |    collection 5 |         fairy 4\n",
      "           \"the\" |            a 27 |            by 4 |circumstantial 4\n",
      "         \"wales\" |            a 27 |       child's 4 |     christmas 4\n",
      "\n",
      "Systems test  2  - Inverted Index\n",
      "\n",
      "         \"atlas\" |          boon 3 |        dipped 3 |                \n",
      "          \"boon\" |         atlas 2 |          cava 2 |        dipped 3\n",
      "          \"cava\" |          boon 3 |        dipped 3 |                \n",
      "        \"dipped\" |         atlas 2 |          boon 3 |          cava 2\n",
      "\n",
      "Systems test  3  - Inverted Index\n",
      "\n",
      "             \"M\" |          DocC 4 |                 |                \n",
      "             \"N\" |          DocC 4 |                 |                \n",
      "             \"X\" |          DocA 3 |          DocB 2 |                \n",
      "             \"Y\" |          DocA 3 |          DocB 2 |          DocC 4\n",
      "             \"Z\" |          DocA 3 |          DocC 4 |                \n"
     ]
    }
   ],
   "source": [
    "##########################################################\n",
    "# Pretty print systems tests for generating Inverted Index\n",
    "##########################################################\n",
    "\n",
    "import json\n",
    "\n",
    "for i in range(1,4):\n",
    "    print \"\"*100\n",
    "    print \"Systems test \",i,\" - Inverted Index\"\n",
    "    print \"\"*100  \n",
    "    with open(\"systems_test_index_\"+str(i),\"r\") as f:\n",
    "        lines = f.readlines()\n",
    "        for line in lines:\n",
    "            line = line.strip()\n",
    "            word, stripe = line.split(\"\\t\")\n",
    "            stripe = json.loads(stripe)\n",
    "            stripe.extend([[\"\",\"\"] for _ in xrange(3 - len(stripe))])\n",
    "\n",
    "            print \"{0:>16} |{1:>16} |{2:>16} |{3:>16}\".format(\n",
    "          (word), stripe[0][0]+\" \"+str(stripe[0][1]), stripe[1][0]+\" \"+str(stripe[1][1]), stripe[2][0]+\" \"+str(stripe[2][1]))\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inverted Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "Systems test  1  - Inverted Index\n",
    "\n",
    "             \"a\" |          bill 4 |     biography 4 |            by 4\n",
    "          \"bill\" |            a 27 |  establishing 4 |           for 4\n",
    "     \"biography\" |            a 27 |       general 4 |        george 4\n",
    "            \"by\" |            a 27 |          city 4 |           sea 4\n",
    "          \"case\" |            a 27 |        female 4 |    government 4\n",
    "       \"child's\" |            a 27 |     christmas 4 |            in 7\n",
    "     \"christmas\" |            a 27 |       child's 4 |            in 7\n",
    "\"circumstantial\" |            a 27 |     narrative 4 |           of 15\n",
    "          \"city\" |            a 27 |            by 4 |           sea 4\n",
    "    \"collection\" |            a 27 |         fairy 4 |         forms 3\n",
    "  \"establishing\" |            a 27 |          bill 4 |           for 4\n",
    "         \"fairy\" |            a 27 |    collection 5 |           of 15\n",
    "        \"female\" |            a 27 |          case 7 |           of 15\n",
    "           \"for\" |            a 27 |          bill 4 |  establishing 4\n",
    "         \"forms\" |            a 27 |    collection 5 |           of 15\n",
    "       \"general\" |            a 27 |     biography 4 |        george 4\n",
    "        \"george\" |            a 27 |     biography 4 |       general 4\n",
    "    \"government\" |            a 27 |          case 7 |            in 7\n",
    "            \"in\" |            a 27 |          case 7 |       child's 4\n",
    "       \"limited\" |            a 27 |          case 7 |           of 15\n",
    "     \"narrative\" |            a 27 |circumstantial 4 |           of 15\n",
    "            \"of\" |            a 27 |     biography 4 |          case 7\n",
    "     \"religious\" |            a 27 |          bill 4 |  establishing 4\n",
    "           \"sea\" |            a 27 |            by 4 |          city 4\n",
    "         \"study\" |            a 27 |          case 7 |        female 4\n",
    "         \"tales\" |            a 27 |    collection 5 |         fairy 4\n",
    "           \"the\" |            a 27 |            by 4 |circumstantial 4\n",
    "         \"wales\" |            a 27 |       child's 4 |     christmas 4\n",
    "\n",
    "Systems test  2  - Inverted Index\n",
    "\n",
    "         \"atlas\" |          boon 3 |        dipped 3 |                \n",
    "          \"boon\" |         atlas 2 |          cava 2 |        dipped 3\n",
    "          \"cava\" |          boon 3 |        dipped 3 |                \n",
    "        \"dipped\" |         atlas 2 |          boon 3 |          cava 2\n",
    "\n",
    "Systems test  3  - Inverted Index\n",
    "\n",
    "             \"M\" |          DocC 4 |                 |                \n",
    "             \"N\" |          DocC 4 |                 |                \n",
    "             \"X\" |          DocA 3 |          DocB 2 |                \n",
    "             \"Y\" |          DocA 3 |          DocB 2 |          DocC 4\n",
    "             \"Z\" |          DocA 3 |          DocC 4 |                \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (3) Calculate similarities - run the commands and insure that your output matches the output below"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NOTE: you must run in hadoop mode to generate sorted similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No configs found; falling back on auto-configuration\n",
      "No configs specified for hadoop runner\n",
      "Looking for hadoop binary in $PATH...\n",
      "Found hadoop binary: /usr/bin/hadoop\n",
      "Using Hadoop version 2.6.0\n",
      "Looking for Hadoop streaming jar in /home/hadoop/contrib...\n",
      "Looking for Hadoop streaming jar in /usr/lib/hadoop-mapreduce...\n",
      "Found Hadoop streaming jar: /usr/lib/hadoop-mapreduce/hadoop-streaming.jar\n",
      "Creating temp directory /tmp/similarity.root.20180215.145429.375579\n",
      "Copying local files to hdfs:///user/root/tmp/mrjob/similarity.root.20180215.145429.375579/files/...\n",
      "Running step 1 of 3...\n",
      "  packageJobJar: [] [/usr/jars/hadoop-streaming-2.6.0-cdh5.7.0.jar] /tmp/streamjob4296767661455786604.jar tmpDir=null\n",
      "  Connecting to ResourceManager at /0.0.0.0:8032\n",
      "  Connecting to ResourceManager at /0.0.0.0:8032\n",
      "  Total input paths to process : 1\n",
      "  number of splits:2\n",
      "  Submitting tokens for job: job_1518662239671_0055\n",
      "  Submitted application application_1518662239671_0055\n",
      "  The url to track the job: http://docker.w261:8088/proxy/application_1518662239671_0055/\n",
      "  Running job: job_1518662239671_0055\n",
      "  Job job_1518662239671_0055 running in uber mode : false\n",
      "   map 0% reduce 0%\n",
      "   map 50% reduce 0%\n",
      "   map 100% reduce 0%\n",
      "   map 100% reduce 100%\n",
      "  Job job_1518662239671_0055 completed successfully\n",
      "  Output directory: hdfs:///user/root/tmp/mrjob/similarity.root.20180215.145429.375579/step-output/0000\n",
      "Counters: 51\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=3330\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=20276\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=20591\n",
      "\t\tFILE: Number of bytes written=396870\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=3668\n",
      "\t\tHDFS: Number of bytes written=20276\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of read operations=9\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tData-local map tasks=2\n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tTotal megabyte-seconds taken by all map tasks=4763648\n",
      "\t\tTotal megabyte-seconds taken by all reduce tasks=2527232\n",
      "\t\tTotal time spent by all map tasks (ms)=4652\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=4652\n",
      "\t\tTotal time spent by all reduce tasks (ms)=2468\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=2468\n",
      "\t\tTotal vcore-seconds taken by all map tasks=4652\n",
      "\t\tTotal vcore-seconds taken by all reduce tasks=2468\n",
      "\tMap-Reduce Framework\n",
      "\t\tCPU time spent (ms)=1820\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tGC time elapsed (ms)=40\n",
      "\t\tInput split bytes=338\n",
      "\t\tMap input records=28\n",
      "\t\tMap output bytes=19239\n",
      "\t\tMap output materialized bytes=20597\n",
      "\t\tMap output records=673\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tPhysical memory (bytes) snapshot=921337856\n",
      "\t\tReduce input groups=378\n",
      "\t\tReduce input records=673\n",
      "\t\tReduce output records=378\n",
      "\t\tReduce shuffle bytes=20597\n",
      "\t\tShuffled Maps =2\n",
      "\t\tSpilled Records=1346\n",
      "\t\tTotal committed heap usage (bytes)=1513095168\n",
      "\t\tVirtual memory (bytes) snapshot=4233379840\n",
      "\tMapper Counters\n",
      "\t\tCalls=28\n",
      "\tReducer Counters\n",
      "\t\tCalls=378\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "Running step 2 of 3...\n",
      "  packageJobJar: [] [/usr/jars/hadoop-streaming-2.6.0-cdh5.7.0.jar] /tmp/streamjob3355324926835317812.jar tmpDir=null\n",
      "  Connecting to ResourceManager at /0.0.0.0:8032\n",
      "  Connecting to ResourceManager at /0.0.0.0:8032\n",
      "  Total input paths to process : 1\n",
      "  number of splits:2\n",
      "  Submitting tokens for job: job_1518662239671_0056\n",
      "  Submitted application application_1518662239671_0056\n",
      "  The url to track the job: http://docker.w261:8088/proxy/application_1518662239671_0056/\n",
      "  Running job: job_1518662239671_0056\n",
      "  Job job_1518662239671_0056 running in uber mode : false\n",
      "   map 0% reduce 0%\n",
      "   map 50% reduce 0%\n",
      "   map 100% reduce 0%\n",
      "   map 100% reduce 100%\n",
      "  Job job_1518662239671_0056 completed successfully\n",
      "  Output directory: hdfs:///user/root/tmp/mrjob/similarity.root.20180215.145429.375579/step-output/0001\n",
      "Counters: 50\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=24372\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=25050\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=21416\n",
      "\t\tFILE: Number of bytes written=399378\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=24712\n",
      "\t\tHDFS: Number of bytes written=25050\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of read operations=9\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tFinal Reducer Counters\n",
      "\t\tCalls=293\n",
      "\tJob Counters \n",
      "\t\tData-local map tasks=2\n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tTotal megabyte-seconds taken by all map tasks=4435968\n",
      "\t\tTotal megabyte-seconds taken by all reduce tasks=2438144\n",
      "\t\tTotal time spent by all map tasks (ms)=4332\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=4332\n",
      "\t\tTotal time spent by all reduce tasks (ms)=2381\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=2381\n",
      "\t\tTotal vcore-seconds taken by all map tasks=4332\n",
      "\t\tTotal vcore-seconds taken by all reduce tasks=2381\n",
      "\tMap-Reduce Framework\n",
      "\t\tCPU time spent (ms)=1620\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tGC time elapsed (ms)=42\n",
      "\t\tInput split bytes=340\n",
      "\t\tMap input records=378\n",
      "\t\tMap output bytes=20654\n",
      "\t\tMap output materialized bytes=21422\n",
      "\t\tMap output records=378\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tPhysical memory (bytes) snapshot=926035968\n",
      "\t\tReduce input groups=378\n",
      "\t\tReduce input records=378\n",
      "\t\tReduce output records=378\n",
      "\t\tReduce shuffle bytes=21422\n",
      "\t\tShuffled Maps =2\n",
      "\t\tSpilled Records=756\n",
      "\t\tTotal committed heap usage (bytes)=1513095168\n",
      "\t\tVirtual memory (bytes) snapshot=4251803648\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "Running step 3 of 3...\n",
      "  packageJobJar: [] [/usr/jars/hadoop-streaming-2.6.0-cdh5.7.0.jar] /tmp/streamjob1072685692252397046.jar tmpDir=null\n",
      "  Connecting to ResourceManager at /0.0.0.0:8032\n",
      "  Connecting to ResourceManager at /0.0.0.0:8032\n",
      "  Total input paths to process : 1\n",
      "  number of splits:2\n",
      "  Submitting tokens for job: job_1518662239671_0057\n",
      "  Submitted application application_1518662239671_0057\n",
      "  The url to track the job: http://docker.w261:8088/proxy/application_1518662239671_0057/\n",
      "  Running job: job_1518662239671_0057\n",
      "  Job job_1518662239671_0057 running in uber mode : false\n",
      "   map 0% reduce 0%\n",
      "   map 50% reduce 0%\n",
      "   map 100% reduce 0%\n",
      "   map 100% reduce 100%\n",
      "  Job job_1518662239671_0057 completed successfully\n",
      "  Output directory: hdfs:///user/root/tmp/mrjob/similarity.root.20180215.145429.375579/output\n",
      "Counters: 51\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=29146\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=20767\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=26190\n",
      "\t\tFILE: Number of bytes written=409091\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=29486\n",
      "\t\tHDFS: Number of bytes written=20767\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of read operations=9\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tFinal Mapper Counters\n",
      "\t\tCalls=378\n",
      "\tFinal Reducer Counters\n",
      "\t\tCalls=34\n",
      "\tJob Counters \n",
      "\t\tData-local map tasks=2\n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tTotal megabyte-seconds taken by all map tasks=4795392\n",
      "\t\tTotal megabyte-seconds taken by all reduce tasks=2481152\n",
      "\t\tTotal time spent by all map tasks (ms)=4683\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=4683\n",
      "\t\tTotal time spent by all reduce tasks (ms)=2423\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=2423\n",
      "\t\tTotal vcore-seconds taken by all map tasks=4683\n",
      "\t\tTotal vcore-seconds taken by all reduce tasks=2423\n",
      "\tMap-Reduce Framework\n",
      "\t\tCPU time spent (ms)=1660\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tGC time elapsed (ms)=40\n",
      "\t\tInput split bytes=340\n",
      "\t\tMap input records=378\n",
      "\t\tMap output bytes=25428\n",
      "\t\tMap output materialized bytes=26196\n",
      "\t\tMap output records=378\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tPhysical memory (bytes) snapshot=948940800\n",
      "\t\tReduce input groups=378\n",
      "\t\tReduce input records=378\n",
      "\t\tReduce output records=34\n",
      "\t\tReduce shuffle bytes=26196\n",
      "\t\tShuffled Maps =2\n",
      "\t\tSpilled Records=756\n",
      "\t\tTotal committed heap usage (bytes)=1513095168\n",
      "\t\tVirtual memory (bytes) snapshot=4240797696\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "Streaming final output from hdfs:///user/root/tmp/mrjob/similarity.root.20180215.145429.375579/output...\n",
      "Removing HDFS temp directory hdfs:///user/root/tmp/mrjob/similarity.root.20180215.145429.375579...\n",
      "Removing temp directory /tmp/similarity.root.20180215.145429.375579...\n"
     ]
    }
   ],
   "source": [
    "!python similarity.py -r hadoop --cmdenv PATH=/opt/anaconda/bin:$PATH systems_test_index_1 > systems_test_similarities_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No configs found; falling back on auto-configuration\n",
      "No configs specified for hadoop runner\n",
      "Looking for hadoop binary in $PATH...\n",
      "Found hadoop binary: /usr/bin/hadoop\n",
      "Using Hadoop version 2.6.0\n",
      "Looking for Hadoop streaming jar in /home/hadoop/contrib...\n",
      "Looking for Hadoop streaming jar in /usr/lib/hadoop-mapreduce...\n",
      "Found Hadoop streaming jar: /usr/lib/hadoop-mapreduce/hadoop-streaming.jar\n",
      "Creating temp directory /tmp/similarity.root.20180215.145533.206201\n",
      "Copying local files to hdfs:///user/root/tmp/mrjob/similarity.root.20180215.145533.206201/files/...\n",
      "Running step 1 of 3...\n",
      "  packageJobJar: [] [/usr/jars/hadoop-streaming-2.6.0-cdh5.7.0.jar] /tmp/streamjob4799030707590395673.jar tmpDir=null\n",
      "  Connecting to ResourceManager at /0.0.0.0:8032\n",
      "  Connecting to ResourceManager at /0.0.0.0:8032\n",
      "  Total input paths to process : 1\n",
      "  number of splits:2\n",
      "  Submitting tokens for job: job_1518662239671_0058\n",
      "  Submitted application application_1518662239671_0058\n",
      "  The url to track the job: http://docker.w261:8088/proxy/application_1518662239671_0058/\n",
      "  Running job: job_1518662239671_0058\n",
      "  Job job_1518662239671_0058 running in uber mode : false\n",
      "   map 0% reduce 0%\n",
      "   map 50% reduce 0%\n",
      "   map 100% reduce 0%\n",
      "   map 100% reduce 100%\n",
      "  Job job_1518662239671_0058 completed successfully\n",
      "  Output directory: hdfs:///user/root/tmp/mrjob/similarity.root.20180215.145533.206201/step-output/0000\n",
      "Counters: 51\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=236\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=268\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=234\n",
      "\t\tFILE: Number of bytes written=356156\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=574\n",
      "\t\tHDFS: Number of bytes written=268\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of read operations=9\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tData-local map tasks=2\n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tTotal megabyte-seconds taken by all map tasks=4774912\n",
      "\t\tTotal megabyte-seconds taken by all reduce tasks=2441216\n",
      "\t\tTotal time spent by all map tasks (ms)=4663\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=4663\n",
      "\t\tTotal time spent by all reduce tasks (ms)=2384\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=2384\n",
      "\t\tTotal vcore-seconds taken by all map tasks=4663\n",
      "\t\tTotal vcore-seconds taken by all reduce tasks=2384\n",
      "\tMap-Reduce Framework\n",
      "\t\tCPU time spent (ms)=1570\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tGC time elapsed (ms)=38\n",
      "\t\tInput split bytes=338\n",
      "\t\tMap input records=4\n",
      "\t\tMap output bytes=212\n",
      "\t\tMap output materialized bytes=240\n",
      "\t\tMap output records=8\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tPhysical memory (bytes) snapshot=922906624\n",
      "\t\tReduce input groups=6\n",
      "\t\tReduce input records=8\n",
      "\t\tReduce output records=6\n",
      "\t\tReduce shuffle bytes=240\n",
      "\t\tShuffled Maps =2\n",
      "\t\tSpilled Records=16\n",
      "\t\tTotal committed heap usage (bytes)=1513095168\n",
      "\t\tVirtual memory (bytes) snapshot=4242337792\n",
      "\tMapper Counters\n",
      "\t\tCalls=4\n",
      "\tReducer Counters\n",
      "\t\tCalls=6\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "Running step 2 of 3...\n",
      "  packageJobJar: [] [/usr/jars/hadoop-streaming-2.6.0-cdh5.7.0.jar] /tmp/streamjob2248019797988731652.jar tmpDir=null\n",
      "  Connecting to ResourceManager at /0.0.0.0:8032\n",
      "  Connecting to ResourceManager at /0.0.0.0:8032\n",
      "  Total input paths to process : 1\n",
      "  number of splits:2\n",
      "  Submitting tokens for job: job_1518662239671_0059\n",
      "  Submitted application application_1518662239671_0059\n",
      "  The url to track the job: http://docker.w261:8088/proxy/application_1518662239671_0059/\n",
      "  Running job: job_1518662239671_0059\n",
      "  Job job_1518662239671_0059 running in uber mode : false\n",
      "   map 0% reduce 0%\n",
      "   map 50% reduce 0%\n",
      "   map 100% reduce 0%\n",
      "   map 100% reduce 100%\n",
      "  Job job_1518662239671_0059 completed successfully\n",
      "  Output directory: hdfs:///user/root/tmp/mrjob/similarity.root.20180215.145533.206201/step-output/0001\n",
      "Counters: 50\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=402\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=330\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=292\n",
      "\t\tFILE: Number of bytes written=357130\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=742\n",
      "\t\tHDFS: Number of bytes written=330\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of read operations=9\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tFinal Reducer Counters\n",
      "\t\tCalls=5\n",
      "\tJob Counters \n",
      "\t\tData-local map tasks=2\n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tTotal megabyte-seconds taken by all map tasks=4390912\n",
      "\t\tTotal megabyte-seconds taken by all reduce tasks=2486272\n",
      "\t\tTotal time spent by all map tasks (ms)=4288\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=4288\n",
      "\t\tTotal time spent by all reduce tasks (ms)=2428\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=2428\n",
      "\t\tTotal vcore-seconds taken by all map tasks=4288\n",
      "\t\tTotal vcore-seconds taken by all reduce tasks=2428\n",
      "\tMap-Reduce Framework\n",
      "\t\tCPU time spent (ms)=1550\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tGC time elapsed (ms)=40\n",
      "\t\tInput split bytes=340\n",
      "\t\tMap input records=6\n",
      "\t\tMap output bytes=274\n",
      "\t\tMap output materialized bytes=298\n",
      "\t\tMap output records=6\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tPhysical memory (bytes) snapshot=921366528\n",
      "\t\tReduce input groups=6\n",
      "\t\tReduce input records=6\n",
      "\t\tReduce output records=6\n",
      "\t\tReduce shuffle bytes=298\n",
      "\t\tShuffled Maps =2\n",
      "\t\tSpilled Records=12\n",
      "\t\tTotal committed heap usage (bytes)=1513095168\n",
      "\t\tVirtual memory (bytes) snapshot=4225597440\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "Running step 3 of 3...\n",
      "  packageJobJar: [] [/usr/jars/hadoop-streaming-2.6.0-cdh5.7.0.jar] /tmp/streamjob937897686106267147.jar tmpDir=null\n",
      "  Connecting to ResourceManager at /0.0.0.0:8032\n",
      "  Connecting to ResourceManager at /0.0.0.0:8032\n",
      "  Total input paths to process : 1\n",
      "  number of splits:2\n",
      "  Submitting tokens for job: job_1518662239671_0060\n",
      "  Submitted application application_1518662239671_0060\n",
      "  The url to track the job: http://docker.w261:8088/proxy/application_1518662239671_0060/\n",
      "  Running job: job_1518662239671_0060\n",
      "  Job job_1518662239671_0060 running in uber mode : false\n",
      "   map 0% reduce 0%\n",
      "   map 50% reduce 0%\n",
      "   map 100% reduce 0%\n",
      "   map 100% reduce 100%\n",
      "  Job job_1518662239671_0060 completed successfully\n",
      "  Output directory: hdfs:///user/root/tmp/mrjob/similarity.root.20180215.145533.206201/output\n",
      "Counters: 51\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=495\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=297\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=354\n",
      "\t\tFILE: Number of bytes written=357416\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=835\n",
      "\t\tHDFS: Number of bytes written=297\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of read operations=9\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tFinal Mapper Counters\n",
      "\t\tCalls=6\n",
      "\tFinal Reducer Counters\n",
      "\t\tCalls=3\n",
      "\tJob Counters \n",
      "\t\tData-local map tasks=2\n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tTotal megabyte-seconds taken by all map tasks=4699136\n",
      "\t\tTotal megabyte-seconds taken by all reduce tasks=2530304\n",
      "\t\tTotal time spent by all map tasks (ms)=4589\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=4589\n",
      "\t\tTotal time spent by all reduce tasks (ms)=2471\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=2471\n",
      "\t\tTotal vcore-seconds taken by all map tasks=4589\n",
      "\t\tTotal vcore-seconds taken by all reduce tasks=2471\n",
      "\tMap-Reduce Framework\n",
      "\t\tCPU time spent (ms)=1570\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tGC time elapsed (ms)=38\n",
      "\t\tInput split bytes=340\n",
      "\t\tMap input records=6\n",
      "\t\tMap output bytes=336\n",
      "\t\tMap output materialized bytes=360\n",
      "\t\tMap output records=6\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tPhysical memory (bytes) snapshot=924979200\n",
      "\t\tReduce input groups=6\n",
      "\t\tReduce input records=6\n",
      "\t\tReduce output records=3\n",
      "\t\tReduce shuffle bytes=360\n",
      "\t\tShuffled Maps =2\n",
      "\t\tSpilled Records=12\n",
      "\t\tTotal committed heap usage (bytes)=1513095168\n",
      "\t\tVirtual memory (bytes) snapshot=4243914752\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "Streaming final output from hdfs:///user/root/tmp/mrjob/similarity.root.20180215.145533.206201/output...\n",
      "Removing HDFS temp directory hdfs:///user/root/tmp/mrjob/similarity.root.20180215.145533.206201...\n",
      "Removing temp directory /tmp/similarity.root.20180215.145533.206201...\n"
     ]
    }
   ],
   "source": [
    "!python similarity.py -r hadoop --cmdenv PATH=/opt/anaconda/bin:$PATH systems_test_index_2 > systems_test_similarities_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No configs found; falling back on auto-configuration\n",
      "No configs specified for hadoop runner\n",
      "Looking for hadoop binary in $PATH...\n",
      "Found hadoop binary: /usr/bin/hadoop\n",
      "Using Hadoop version 2.6.0\n",
      "Looking for Hadoop streaming jar in /home/hadoop/contrib...\n",
      "Looking for Hadoop streaming jar in /usr/lib/hadoop-mapreduce...\n",
      "Found Hadoop streaming jar: /usr/lib/hadoop-mapreduce/hadoop-streaming.jar\n",
      "Creating temp directory /tmp/similarity.root.20180215.145637.409136\n",
      "Copying local files to hdfs:///user/root/tmp/mrjob/similarity.root.20180215.145637.409136/files/...\n",
      "Running step 1 of 3...\n",
      "  packageJobJar: [] [/usr/jars/hadoop-streaming-2.6.0-cdh5.7.0.jar] /tmp/streamjob2611345433675234864.jar tmpDir=null\n",
      "  Connecting to ResourceManager at /0.0.0.0:8032\n",
      "  Connecting to ResourceManager at /0.0.0.0:8032\n",
      "  Total input paths to process : 1\n",
      "  number of splits:2\n",
      "  Submitting tokens for job: job_1518662239671_0061\n",
      "  Submitted application application_1518662239671_0061\n",
      "  The url to track the job: http://docker.w261:8088/proxy/application_1518662239671_0061/\n",
      "  Running job: job_1518662239671_0061\n",
      "  Job job_1518662239671_0061 running in uber mode : false\n",
      "   map 0% reduce 0%\n",
      "   map 50% reduce 0%\n",
      "   map 100% reduce 0%\n",
      "   map 100% reduce 100%\n",
      "  Job job_1518662239671_0061 completed successfully\n",
      "  Output directory: hdfs:///user/root/tmp/mrjob/similarity.root.20180215.145637.409136/step-output/0000\n",
      "Counters: 51\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=194\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=159\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=141\n",
      "\t\tFILE: Number of bytes written=355970\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=532\n",
      "\t\tHDFS: Number of bytes written=159\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of read operations=9\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tData-local map tasks=2\n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tTotal megabyte-seconds taken by all map tasks=4787200\n",
      "\t\tTotal megabyte-seconds taken by all reduce tasks=2434048\n",
      "\t\tTotal time spent by all map tasks (ms)=4675\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=4675\n",
      "\t\tTotal time spent by all reduce tasks (ms)=2377\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=2377\n",
      "\t\tTotal vcore-seconds taken by all map tasks=4675\n",
      "\t\tTotal vcore-seconds taken by all reduce tasks=2377\n",
      "\tMap-Reduce Framework\n",
      "\t\tCPU time spent (ms)=1610\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tGC time elapsed (ms)=37\n",
      "\t\tInput split bytes=338\n",
      "\t\tMap input records=5\n",
      "\t\tMap output bytes=125\n",
      "\t\tMap output materialized bytes=147\n",
      "\t\tMap output records=5\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tPhysical memory (bytes) snapshot=907902976\n",
      "\t\tReduce input groups=3\n",
      "\t\tReduce input records=5\n",
      "\t\tReduce output records=3\n",
      "\t\tReduce shuffle bytes=147\n",
      "\t\tShuffled Maps =2\n",
      "\t\tSpilled Records=10\n",
      "\t\tTotal committed heap usage (bytes)=1513095168\n",
      "\t\tVirtual memory (bytes) snapshot=4244062208\n",
      "\tMapper Counters\n",
      "\t\tCalls=5\n",
      "\tReducer Counters\n",
      "\t\tCalls=3\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "Running step 2 of 3...\n",
      "  packageJobJar: [] [/usr/jars/hadoop-streaming-2.6.0-cdh5.7.0.jar] /tmp/streamjob3063738883080013384.jar tmpDir=null\n",
      "  Connecting to ResourceManager at /0.0.0.0:8032\n",
      "  Connecting to ResourceManager at /0.0.0.0:8032\n",
      "  Total input paths to process : 1\n",
      "  number of splits:2\n",
      "  Submitting tokens for job: job_1518662239671_0062\n",
      "  Submitted application application_1518662239671_0062\n",
      "  The url to track the job: http://docker.w261:8088/proxy/application_1518662239671_0062/\n",
      "  Running job: job_1518662239671_0062\n",
      "  Job job_1518662239671_0062 running in uber mode : false\n",
      "   map 0% reduce 0%\n",
      "   map 100% reduce 0%\n",
      "   map 100% reduce 100%\n",
      "  Job job_1518662239671_0062 completed successfully\n",
      "  Output directory: hdfs:///user/root/tmp/mrjob/similarity.root.20180215.145637.409136/step-output/0001\n",
      "Counters: 50\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=239\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=197\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=174\n",
      "\t\tFILE: Number of bytes written=356894\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=579\n",
      "\t\tHDFS: Number of bytes written=197\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of read operations=9\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tFinal Reducer Counters\n",
      "\t\tCalls=3\n",
      "\tJob Counters \n",
      "\t\tData-local map tasks=2\n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tTotal megabyte-seconds taken by all map tasks=4338688\n",
      "\t\tTotal megabyte-seconds taken by all reduce tasks=2471936\n",
      "\t\tTotal time spent by all map tasks (ms)=4237\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=4237\n",
      "\t\tTotal time spent by all reduce tasks (ms)=2414\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=2414\n",
      "\t\tTotal vcore-seconds taken by all map tasks=4237\n",
      "\t\tTotal vcore-seconds taken by all reduce tasks=2414\n",
      "\tMap-Reduce Framework\n",
      "\t\tCPU time spent (ms)=1510\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tGC time elapsed (ms)=40\n",
      "\t\tInput split bytes=340\n",
      "\t\tMap input records=3\n",
      "\t\tMap output bytes=162\n",
      "\t\tMap output materialized bytes=180\n",
      "\t\tMap output records=3\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tPhysical memory (bytes) snapshot=922845184\n",
      "\t\tReduce input groups=3\n",
      "\t\tReduce input records=3\n",
      "\t\tReduce output records=3\n",
      "\t\tReduce shuffle bytes=180\n",
      "\t\tShuffled Maps =2\n",
      "\t\tSpilled Records=6\n",
      "\t\tTotal committed heap usage (bytes)=1513095168\n",
      "\t\tVirtual memory (bytes) snapshot=4247928832\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "Running step 3 of 3...\n",
      "  packageJobJar: [] [/usr/jars/hadoop-streaming-2.6.0-cdh5.7.0.jar] /tmp/streamjob4740717786373005711.jar tmpDir=null\n",
      "  Connecting to ResourceManager at /0.0.0.0:8032\n",
      "  Connecting to ResourceManager at /0.0.0.0:8032\n",
      "  Total input paths to process : 1\n",
      "  number of splits:2\n",
      "  Submitting tokens for job: job_1518662239671_0063\n",
      "  Submitted application application_1518662239671_0063\n",
      "  The url to track the job: http://docker.w261:8088/proxy/application_1518662239671_0063/\n",
      "  Running job: job_1518662239671_0063\n",
      "  Job job_1518662239671_0063 running in uber mode : false\n",
      "   map 0% reduce 0%\n",
      "   map 100% reduce 0%\n",
      "   map 100% reduce 100%\n",
      "  Job job_1518662239671_0063 completed successfully\n",
      "  Output directory: hdfs:///user/root/tmp/mrjob/similarity.root.20180215.145637.409136/output\n",
      "Counters: 51\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=296\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=203\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=212\n",
      "\t\tFILE: Number of bytes written=357135\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=636\n",
      "\t\tHDFS: Number of bytes written=203\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of read operations=9\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tFinal Mapper Counters\n",
      "\t\tCalls=3\n",
      "\tFinal Reducer Counters\n",
      "\t\tCalls=3\n",
      "\tJob Counters \n",
      "\t\tData-local map tasks=2\n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tTotal megabyte-seconds taken by all map tasks=4723712\n",
      "\t\tTotal megabyte-seconds taken by all reduce tasks=2459648\n",
      "\t\tTotal time spent by all map tasks (ms)=4613\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=4613\n",
      "\t\tTotal time spent by all reduce tasks (ms)=2402\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=2402\n",
      "\t\tTotal vcore-seconds taken by all map tasks=4613\n",
      "\t\tTotal vcore-seconds taken by all reduce tasks=2402\n",
      "\tMap-Reduce Framework\n",
      "\t\tCPU time spent (ms)=1560\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tGC time elapsed (ms)=40\n",
      "\t\tInput split bytes=340\n",
      "\t\tMap input records=3\n",
      "\t\tMap output bytes=200\n",
      "\t\tMap output materialized bytes=218\n",
      "\t\tMap output records=3\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tPhysical memory (bytes) snapshot=923856896\n",
      "\t\tReduce input groups=3\n",
      "\t\tReduce input records=3\n",
      "\t\tReduce output records=3\n",
      "\t\tReduce shuffle bytes=218\n",
      "\t\tShuffled Maps =2\n",
      "\t\tSpilled Records=6\n",
      "\t\tTotal committed heap usage (bytes)=1513095168\n",
      "\t\tVirtual memory (bytes) snapshot=4247662592\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "Streaming final output from hdfs:///user/root/tmp/mrjob/similarity.root.20180215.145637.409136/output...\n",
      "Removing HDFS temp directory hdfs:///user/root/tmp/mrjob/similarity.root.20180215.145637.409136...\n",
      "Removing temp directory /tmp/similarity.root.20180215.145637.409136...\n"
     ]
    }
   ],
   "source": [
    "!python similarity.py -r hadoop --cmdenv PATH=/opt/anaconda/bin:$PATH systems_test_index_3 > systems_test_similarities_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Systems test  1  - Similarity measures\n",
      "\n",
      "        average |           pair |         cosine |        jaccard |        overlap |           dice\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "       1.000000 |female - limited |       1.000000 |       1.000000 |       1.000000 |       1.000000\n",
      "       0.868292 |  forms - tales |       0.866025 |       0.750000 |       1.000000 |       0.857143\n",
      "       0.830357 |   case - study |       0.857143 |       0.750000 |       0.857143 |       0.857143\n",
      "       0.712500 |       by - sea |       0.750000 |       0.600000 |       0.750000 |       0.750000\n",
      "       0.698916 |         a - of |       0.695666 |       0.500000 |       0.933333 |       0.666667\n",
      "       0.646872 |collection - tales |       0.670820 |       0.500000 |       0.750000 |       0.666667\n",
      "       0.559350 |   in - limited |       0.566947 |       0.375000 |       0.750000 |       0.545455\n",
      "       0.553861 |biography - forms |       0.577350 |       0.400000 |       0.666667 |       0.571429\n",
      "       0.504099 |collection - forms |       0.516398 |       0.333333 |       0.666667 |       0.500000\n",
      "       0.477970 |collection - of |       0.461880 |       0.250000 |       0.800000 |       0.400000\n",
      "       0.465201 |      a - study |       0.436436 |       0.214286 |       0.857143 |       0.352941\n",
      "       0.458333 |biography - narrative |       0.500000 |       0.333333 |       0.500000 |       0.500000\n",
      "       0.438276 |    forms - the |       0.436436 |       0.250000 |       0.666667 |       0.400000\n",
      "       0.419343 |biography - collection |       0.447214 |       0.285714 |       0.500000 |       0.444444\n",
      "       0.410147 |government - of |       0.387298 |       0.187500 |       0.750000 |       0.315789\n",
      "       0.389610 |      case - in |       0.428571 |       0.272727 |       0.428571 |       0.428571\n",
      "       0.386912 |     of - study |       0.390360 |       0.222222 |       0.571429 |       0.363636\n",
      "       0.384281 | a - collection |       0.344265 |       0.142857 |       0.800000 |       0.250000\n",
      "       0.365956 |case - circumstantial |       0.377964 |       0.222222 |       0.500000 |       0.363636\n",
      "       0.334842 |       a - bill |       0.288675 |       0.107143 |       0.750000 |       0.193548\n",
      "       0.328008 |     forms - of |       0.298142 |       0.125000 |       0.666667 |       0.222222\n",
      "       0.317849 |collection - study |       0.338062 |       0.200000 |       0.400000 |       0.333333\n",
      "       0.287991 |        in - of |       0.292770 |       0.157895 |       0.428571 |       0.272727\n",
      "       0.273413 |      a - forms |       0.222222 |       0.071429 |       0.666667 |       0.133333\n",
      "       0.271593 |        by - of |       0.258199 |       0.117647 |       0.500000 |       0.210526\n",
      "       0.268597 |forms - government |       0.288675 |       0.166667 |       0.333333 |       0.285714\n",
      "       0.255952 |     case - the |       0.285714 |       0.166667 |       0.285714 |       0.285714\n",
      "       0.223214 |establishing - tales |       0.250000 |       0.142857 |       0.250000 |       0.250000\n",
      "       0.215666 |     forms - in |       0.218218 |       0.111111 |       0.333333 |       0.200000\n",
      "       0.205207 |collection - wales |       0.223607 |       0.125000 |       0.250000 |       0.222222\n",
      "       0.180200 |case - religious |       0.188982 |       0.100000 |       0.250000 |       0.181818\n",
      "       0.156652 |collection - in |       0.169031 |       0.090909 |       0.200000 |       0.166667\n",
      "       0.134980 | of - religious |       0.129099 |       0.055556 |       0.250000 |       0.105263\n",
      "       0.126374 |       in - the |       0.142857 |       0.076923 |       0.142857 |       0.142857\n",
      "\n",
      "Systems test  2  - Similarity measures\n",
      "\n",
      "        average |           pair |         cosine |        jaccard |        overlap |           dice\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "       1.000000 |   atlas - cava |       1.000000 |       1.000000 |       1.000000 |       1.000000\n",
      "       0.625000 |  boon - dipped |       0.666667 |       0.500000 |       0.666667 |       0.666667\n",
      "       0.389562 |    boon - cava |       0.408248 |       0.250000 |       0.500000 |       0.400000\n",
      "\n",
      "Systems test  3  - Similarity measures\n",
      "\n",
      "        average |           pair |         cosine |        jaccard |        overlap |           dice\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "       0.820791 |    DocA - DocB |       0.816497 |       0.666667 |       1.000000 |       0.800000\n",
      "       0.553861 |    DocA - DocC |       0.577350 |       0.400000 |       0.666667 |       0.571429\n",
      "       0.346722 |    DocB - DocC |       0.353553 |       0.200000 |       0.500000 |       0.333333\n"
     ]
    }
   ],
   "source": [
    "############################################\n",
    "# Pretty print systems tests\n",
    "############################################\n",
    "\n",
    "import json\n",
    "for i in range(1,4):\n",
    "  print ''*110\n",
    "  print \"Systems test \",i,\" - Similarity measures\"\n",
    "  print ''*110\n",
    "  print \"{0:>15} |{1:>15} |{2:>15} |{3:>15} |{4:>15} |{5:>15}\".format(\n",
    "          \"average\", \"pair\", \"cosine\", \"jaccard\", \"overlap\", \"dice\")\n",
    "  print '-'*110\n",
    "\n",
    "  with open(\"systems_test_similarities_\"+str(i),\"r\") as f:\n",
    "      lines = f.readlines()\n",
    "      for line in lines:\n",
    "          line = line.strip()\n",
    "          avg,stripe = line.split('\\t')\n",
    "          stripe = json.loads(stripe)\n",
    "        \n",
    "          print \"{0:>15f} |{1:>15} |{2:>15f} |{3:>15f} |{4:>15f} |{5:>15f}\".format(\n",
    "              float(avg), stripe[0][0], float(stripe[0][1]), float(stripe[0][2]), float(stripe[0][3]), float(stripe[0][4]))\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pairwise Similairity "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "Systems test  1  - Similarity measures\n",
    "\n",
    "   average |                pair |         cosine |        jaccard |        overlap |           dice\n",
    "--------------------------------------------------------------------------------------------------------------\n",
    "  1.000000 |    female - limited |       1.000000 |       1.000000 |       1.000000 |       1.000000\n",
    "  0.868292 |       fairy - forms |       0.866025 |       0.750000 |       1.000000 |       0.857143\n",
    "  0.868292 |       forms - tales |       0.866025 |       0.750000 |       1.000000 |       0.857143\n",
    "  0.830357 |        case - study |       0.857143 |       0.750000 |       0.857143 |       0.857143\n",
    "  0.712500 | bill - establishing |       0.750000 |       0.600000 |       0.750000 |       0.750000\n",
    "  0.712500 |   christmas - wales |       0.750000 |       0.600000 |       0.750000 |       0.750000\n",
    "  0.712500 |circumstantial - narrative |       0.750000 |       0.600000 |       0.750000 |       0.750000\n",
    "  0.712500 |            by - sea |       0.750000 |       0.600000 |       0.750000 |       0.750000\n",
    "  0.712500 |           by - city |       0.750000 |       0.600000 |       0.750000 |       0.750000\n",
    "  0.712500 |     child's - wales |       0.750000 |       0.600000 |       0.750000 |       0.750000\n",
    "  0.712500 |  biography - george |       0.750000 |       0.600000 |       0.750000 |       0.750000\n",
    "  0.712500 | child's - christmas |       0.750000 |       0.600000 |       0.750000 |       0.750000\n",
    "  ...\n",
    "  \n",
    "\n",
    "Systems test  2  - Similarity measures\n",
    "\n",
    "   average |                pair |         cosine |        jaccard |        overlap |           dice\n",
    "--------------------------------------------------------------------------------------------------------------\n",
    "  1.000000 |        atlas - cava |       1.000000 |       1.000000 |       1.000000 |       1.000000\n",
    "  0.625000 |       boon - dipped |       0.666667 |       0.500000 |       0.666667 |       0.666667\n",
    "  0.389562 |       cava - dipped |       0.408248 |       0.250000 |       0.500000 |       0.400000\n",
    "  0.389562 |         boon - cava |       0.408248 |       0.250000 |       0.500000 |       0.400000\n",
    "  0.389562 |      atlas - dipped |       0.408248 |       0.250000 |       0.500000 |       0.400000\n",
    "  0.389562 |        atlas - boon |       0.408248 |       0.250000 |       0.500000 |       0.400000\n",
    "\n",
    "Systems test  3  - Similarity measures\n",
    "\n",
    "   average |                pair |         cosine |        jaccard |        overlap |           dice\n",
    "--------------------------------------------------------------------------------------------------------------\n",
    "  0.820791 |         DocA - DocB |       0.816497 |       0.666667 |       1.000000 |       0.800000\n",
    "  0.553861 |         DocA - DocC |       0.577350 |       0.400000 |       0.666667 |       0.571429\n",
    "  0.346722 |         DocB - DocC |       0.353553 |       0.200000 |       0.500000 |       0.333333\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# === END OF PHASE 1 ==="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  },
  "toc": {
   "nav_menu": {
    "height": "512px",
    "width": "252px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "1080px",
    "left": "0px",
    "right": "1331px",
    "top": "107px",
    "width": "287px"
   },
   "toc_section_display": "block",
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
