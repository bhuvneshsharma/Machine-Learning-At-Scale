{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": "true"
   },
   "source": [
    "# Table of Contents\n",
    " <p><div class=\"lev1 toc-item\"><a href=\"#MIDS---w261-Machine-Learning-At-Scale\" data-toc-modified-id=\"MIDS---w261-Machine-Learning-At-Scale-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>MIDS - w261 Machine Learning At Scale</a></div><div class=\"lev2 toc-item\"><a href=\"#Assignment---HW3\" data-toc-modified-id=\"Assignment---HW3-11\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Assignment - HW3</a></div><div class=\"lev2 toc-item\"><a href=\"#INSTRUCTIONS-for-SUBMISSION\" data-toc-modified-id=\"INSTRUCTIONS-for-SUBMISSION-12\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>INSTRUCTIONS for SUBMISSION</a></div><div class=\"lev2 toc-item\"><a href=\"#CONFIGURATION\" data-toc-modified-id=\"CONFIGURATION-13\"><span class=\"toc-item-num\">1.3&nbsp;&nbsp;</span>CONFIGURATION</a></div><div class=\"lev2 toc-item\"><a href=\"#DATASETS\" data-toc-modified-id=\"DATASETS-14\"><span class=\"toc-item-num\">1.4&nbsp;&nbsp;</span>DATASETS</a></div><div class=\"lev1 toc-item\"><a href=\"#HW-Problems\" data-toc-modified-id=\"HW-Problems-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>HW Problems</a></div><div class=\"lev2 toc-item\"><a href=\"#HW3.0.\" data-toc-modified-id=\"HW3.0.-21\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>HW3.0.</a></div><div class=\"lev2 toc-item\"><a href=\"#HW3.1-Consumer-Complaints-EDA---exploring-counters\" data-toc-modified-id=\"HW3.1-Consumer-Complaints-EDA---exploring-counters-22\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>HW3.1 Consumer Complaints EDA - exploring counters</a></div><div class=\"lev3 toc-item\"><a href=\"#HW-3.2-Analyze-the-performance-of-your-Mappers,-Combiners-and-Reducers-using-Counters\" data-toc-modified-id=\"HW-3.2-Analyze-the-performance-of-your-Mappers,-Combiners-and-Reducers-using-Counters-221\"><span class=\"toc-item-num\">2.2.1&nbsp;&nbsp;</span>HW 3.2 Analyze the performance of your Mappers, Combiners and Reducers using Counters</a></div><div class=\"lev3 toc-item\"><a href=\"#3.2.A-SOLUTION\" data-toc-modified-id=\"3.2.A-SOLUTION-222\"><span class=\"toc-item-num\">2.2.2&nbsp;&nbsp;</span>3.2.A SOLUTION</a></div><div class=\"lev4 toc-item\"><a href=\"#3.2.A-EXPLANATION\" data-toc-modified-id=\"3.2.A-EXPLANATION-2221\"><span class=\"toc-item-num\">2.2.2.1&nbsp;&nbsp;</span>3.2.A EXPLANATION</a></div><div class=\"lev3 toc-item\"><a href=\"#3.2.B-SOLUTION\" data-toc-modified-id=\"3.2.B-SOLUTION-223\"><span class=\"toc-item-num\">2.2.3&nbsp;&nbsp;</span>3.2.B SOLUTION</a></div><div class=\"lev3 toc-item\"><a href=\"#3.2.C-SOLUTION\" data-toc-modified-id=\"3.2.C-SOLUTION-224\"><span class=\"toc-item-num\">2.2.4&nbsp;&nbsp;</span>3.2.C SOLUTION</a></div><div class=\"lev3 toc-item\"><a href=\"#3.2.1\" data-toc-modified-id=\"3.2.1-225\"><span class=\"toc-item-num\">2.2.5&nbsp;&nbsp;</span>3.2.1</a></div><div class=\"lev4 toc-item\"><a href=\"#START-STUDENT-CODE-HW321-(INSERT-CELLS-BELOW-AS-NEEDED)\" data-toc-modified-id=\"START-STUDENT-CODE-HW321-(INSERT-CELLS-BELOW-AS-NEEDED)-2251\"><span class=\"toc-item-num\">2.2.5.1&nbsp;&nbsp;</span>START STUDENT CODE HW321 (INSERT CELLS BELOW AS NEEDED)</a></div><div class=\"lev4 toc-item\"><a href=\"#END-STUDENT-CODE-HW321\" data-toc-modified-id=\"END-STUDENT-CODE-HW321-2252\"><span class=\"toc-item-num\">2.2.5.2&nbsp;&nbsp;</span>END STUDENT CODE HW321</a></div><div class=\"lev2 toc-item\"><a href=\"#HW3.3.-Shopping-Cart-Analysis\" data-toc-modified-id=\"HW3.3.-Shopping-Cart-Analysis-23\"><span class=\"toc-item-num\">2.3&nbsp;&nbsp;</span>HW3.3. Shopping Cart Analysis</a></div><div class=\"lev4 toc-item\"><a href=\"#START-STUDENT-CODE-HW33-(INSERT-CELLS-BELOW-AS-NEEDED)\" data-toc-modified-id=\"START-STUDENT-CODE-HW33-(INSERT-CELLS-BELOW-AS-NEEDED)-2301\"><span class=\"toc-item-num\">2.3.0.1&nbsp;&nbsp;</span>START STUDENT CODE HW33 (INSERT CELLS BELOW AS NEEDED)</a></div><div class=\"lev4 toc-item\"><a href=\"#END-STUDENT-CODE-HW33\" data-toc-modified-id=\"END-STUDENT-CODE-HW33-2302\"><span class=\"toc-item-num\">2.3.0.2&nbsp;&nbsp;</span>END STUDENT CODE HW33</a></div><div class=\"lev2 toc-item\"><a href=\"#HW3.3.1-OPTIONAL\" data-toc-modified-id=\"HW3.3.1-OPTIONAL-24\"><span class=\"toc-item-num\">2.4&nbsp;&nbsp;</span>HW3.3.1 OPTIONAL</a></div><div class=\"lev4 toc-item\"><a href=\"#START-STUDENT-CODE-HW331-(INSERT-CELLS-BELOW-AS-NEEDED)\" data-toc-modified-id=\"START-STUDENT-CODE-HW331-(INSERT-CELLS-BELOW-AS-NEEDED)-2401\"><span class=\"toc-item-num\">2.4.0.1&nbsp;&nbsp;</span>START STUDENT CODE HW331 (INSERT CELLS BELOW AS NEEDED)</a></div><div class=\"lev4 toc-item\"><a href=\"#END-STUDENT-CODE-HW331\" data-toc-modified-id=\"END-STUDENT-CODE-HW331-2402\"><span class=\"toc-item-num\">2.4.0.2&nbsp;&nbsp;</span>END STUDENT CODE HW331</a></div><div class=\"lev2 toc-item\"><a href=\"#HW3.4.-(Computationally-prohibitive-but-then-again-Hadoop-can-handle-this)-Pairs\" data-toc-modified-id=\"HW3.4.-(Computationally-prohibitive-but-then-again-Hadoop-can-handle-this)-Pairs-25\"><span class=\"toc-item-num\">2.5&nbsp;&nbsp;</span>HW3.4. (Computationally prohibitive but then again Hadoop can handle this) Pairs</a></div><div class=\"lev4 toc-item\"><a href=\"#START-STUDENT-CODE-HW34-(INSERT-CELLS-BELOW-AS-NEEDED)\" data-toc-modified-id=\"START-STUDENT-CODE-HW34-(INSERT-CELLS-BELOW-AS-NEEDED)-2501\"><span class=\"toc-item-num\">2.5.0.1&nbsp;&nbsp;</span>START STUDENT CODE HW34 (INSERT CELLS BELOW AS NEEDED)</a></div><div class=\"lev4 toc-item\"><a href=\"#END-STUDENT-CODE-HW34\" data-toc-modified-id=\"END-STUDENT-CODE-HW34-2502\"><span class=\"toc-item-num\">2.5.0.2&nbsp;&nbsp;</span>END STUDENT CODE HW34</a></div><div class=\"lev2 toc-item\"><a href=\"#HW3.5:-Stripes\" data-toc-modified-id=\"HW3.5:-Stripes-26\"><span class=\"toc-item-num\">2.6&nbsp;&nbsp;</span>HW3.5: Stripes</a></div><div class=\"lev4 toc-item\"><a href=\"#START-STUDENT-CODE-HW35-(INSERT-CELLS-BELOW-AS-NEEDED)\" data-toc-modified-id=\"START-STUDENT-CODE-HW35-(INSERT-CELLS-BELOW-AS-NEEDED)-2601\"><span class=\"toc-item-num\">2.6.0.1&nbsp;&nbsp;</span>START STUDENT CODE HW35 (INSERT CELLS BELOW AS NEEDED)</a></div><div class=\"lev4 toc-item\"><a href=\"#END-STUDENT-CODE-HW35\" data-toc-modified-id=\"END-STUDENT-CODE-HW35-2602\"><span class=\"toc-item-num\">2.6.0.2&nbsp;&nbsp;</span>END STUDENT CODE HW35</a></div><div class=\"lev1 toc-item\"><a href=\"#OPTIONAL\" data-toc-modified-id=\"OPTIONAL-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>OPTIONAL</a></div><div class=\"lev2 toc-item\"><a href=\"#HW3.6-Computing-Relative-Frequencies-on-100K-WikiPedia-pages-(93Meg)\" data-toc-modified-id=\"HW3.6-Computing-Relative-Frequencies-on-100K-WikiPedia-pages-(93Meg)-31\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>HW3.6 Computing Relative Frequencies on 100K WikiPedia pages (93Meg)</a></div><div class=\"lev2 toc-item\"><a href=\"#HW3.7-Apriori-Algorithm\" data-toc-modified-id=\"HW3.7-Apriori-Algorithm-32\"><span class=\"toc-item-num\">3.2&nbsp;&nbsp;</span>HW3.7 Apriori Algorithm</a></div><div class=\"lev2 toc-item\"><a href=\"#HW3.8.-Shopping-Cart-Analysis\" data-toc-modified-id=\"HW3.8.-Shopping-Cart-Analysis-33\"><span class=\"toc-item-num\">3.3&nbsp;&nbsp;</span>HW3.8. Shopping Cart Analysis</a></div><div class=\"lev2 toc-item\"><a href=\"#HW3.8.1\" data-toc-modified-id=\"HW3.8.1-34\"><span class=\"toc-item-num\">3.4&nbsp;&nbsp;</span>HW3.8.1</a></div><div class=\"lev1 toc-item\"><a href=\"#END-OF-HOMEWORK\" data-toc-modified-id=\"END-OF-HOMEWORK-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>END OF HOMEWORK</a></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MIDS - w261 Machine Learning At Scale\n",
    "__Course Lead:__ Dr James G. Shanahan (__email__ Jimi via  James.Shanahan _AT_ gmail.com)\n",
    "\n",
    "## Assignment - HW3\n",
    "\n",
    "\n",
    "---\n",
    "__Name:__  *Your Name Goes Here*   \n",
    "__Class:__ MIDS w261 (Section *Your Section Goes Here*, e.g., Fall 2016 Group 1)     \n",
    "__Email:__  *Your UC Berkeley Email Goes Here*@iSchool.Berkeley.edu     \n",
    "__StudentId__  123457    __End of StudentId__     \n",
    "\n",
    "__NOTE:__ please replace `1234567` with your student id above      \n",
    "\n",
    "## INSTRUCTIONS for SUBMISSION\n",
    "\n",
    "This homework can be completed locally on your computer. __Please submit your notebook to your classroom github repository 24 hours prior to the next live session.__ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CONFIGURATION\n",
    "Before starting your homework, run the following cells to confirm your setup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# general imports\n",
    "import os\n",
    "import re\n",
    "import sys\n",
    "import time\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# tell matplotlib not to open a new window\n",
    "%matplotlib inline\n",
    "\n",
    "# automatically reload modules \n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python Version: 2.7.14 \n",
      "HDFS filesystem running at: \n",
      "\t hdfs://quickstart.cloudera:8020\n"
     ]
    }
   ],
   "source": [
    "# print some configuration details for future replicability.\n",
    "print 'Python Version: %s' % (sys.version.split('|')[0])\n",
    "hdfs_conf = !hdfs getconf -confKey fs.defaultFS ### UNCOMMENT ON DOCKER\n",
    "#hdfs_conf = !hdfs getconf -confKey fs.default.name ### UNCOMMENT ON ALTISCALE\n",
    "print 'HDFS filesystem running at: \\n\\t %s' % (hdfs_conf[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create an HDFS directory for this assignment\n",
    "!hdfs dfs -mkdir /user/root/HW3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__[OPTIONAL]:__ Save yourself some typing by defining global variables for paths we'll reuse frequently (like the hadoop-streaming jar file and the HDFS directory for your output)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "JAR_FILE = \"/usr/lib/hadoop-mapreduce/hadoop-streaming-2.6.0-cdh5.7.0.jar\"\n",
    "HDFS_DIR = \"/user/root/HW3\"\n",
    "HOME_DIR = \"/media/notebooks/SP18-1-maynard242\" # FILL IN HERE eg. /media/notebooks/w261-main/Assignments\n",
    "# save path for use in Hadoop jobs (-cmdenv PATH={PATH})\n",
    "from os import environ\n",
    "PATH  = environ['PATH']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__[OPTIONAL]:__ Fix chrome formatting. _The cell below implements a quick hack based on [this stackoverflow thread](http://stackoverflow.com/questions/34277967/chrome-rendering-mathjax-equations-with-a-trailing-vertical-line) to fix [this known issue](https://github.com/mathjax/MathJax/issues/1300) with Mathjax formatting in Chrome (a rounding issue adds a border to the right of mathjax markup)._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1 items\n",
      "drwxr-xr-x   - root supergroup          0 2018-02-24 14:30 HW3\n"
     ]
    }
   ],
   "source": [
    "#%%javascript\n",
    "#$('.math>span').css(\"border-left-color\",\"transparent\")\n",
    "!hdfs dfs -ls "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DATASETS\n",
    "For this homework we be using a few different datasets. These include:\n",
    "* __The Consumer Complaints Dataset [available here](https://www.dropbox.com/s/vbalm3yva2rr86m/Consumer_Complaints.csv?dl=0) -__ _consists of diverse consumer complaints which have been reported across the United States regarding various types of loans. We'll use this in HW 3.1 to learn about Hadoop Counters and then in HW 3.2 where we count words and compute relative frequencies._\n",
    "\n",
    "* __The Product Purchase Dataset [available here](https://www.dropbox.com/s/zlfyiwa70poqg74/ProductPurchaseData.txt?dl=0) -__ _consists of data about products sold online and customer browsing behavior. We'll use this data in 3.3 to perform a shopping cart analysis._\n",
    "\n",
    "Follow the directions below to load/create each of these datasets. You may want to familiarize yourself with their contents before proceeding to the homework questions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__`Consumer_Complaints.csv`__  \n",
    "The dataset consists of records of the form:  \n",
    ">Complaint ID,Product,Sub-product,Issue,Sub-issue,State,ZIP code,Submitted via,Date received,Date sent to company,Company,Company response,Timely response?,Consumer disputed?\n",
    "\n",
    "Download the csv file to your current directory using the dropbox link provided above. You can do this manually or with `curl` or `wget` as we did in HW2. Then use the following cell to put the data into HDFS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Complaint ID,Product,Sub-product,Issue,Sub-issue,State,ZIP code,Submitted via,Date received,Date sent to company,Company,Company response,Timely response?,Consumer disputed?\n",
      "1114245,Debt collection,Medical,Disclosure verification of debt,Not given enough info to verify debt,FL,32219,Web,11/13/2014,11/13/2014,\"Choice Recovery, Inc.\",Closed with explanation,Yes,\n"
     ]
    }
   ],
   "source": [
    "# take a look at the first two lines\n",
    "!head -n 2 Consumer_Complaints.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Put the data into HDFS - adjust path or filename as needed\n",
    "!hdfs dfs -put Consumer_Complaints.csv {HDFS_DIR}/Consumer_Complaints.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!hdfs dfs -ls {HDFS_DIR}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__`ProductPurchaseData.txt`__  \n",
    "Notes about the data format:\n",
    "> Each line in this dataset represents a browsing session of a customer. On each line, each string of 8 characters represents the id of an item browsed during that session. For example:  \n",
    "`FRO11987 ELE17451 ELE89019 SNA90258 GRO99222`  \n",
    "> would represent a single customer's browsing session in which they viewed 5 products.  \n",
    "\n",
    "Download the csv file to your current directory using the dropbox link provided above. You can do this manually or with `curl` or `wget` as we did in HW2. Take a look at the first few lines of the file, then use the following cell to put the data into HDFS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FRO11987 ELE17451 ELE89019 SNA90258 GRO99222 \n",
      "GRO99222 GRO12298 FRO12685 ELE91550 SNA11465 ELE26917 ELE52966 FRO90334 SNA30755 ELE17451 FRO84225 SNA80192 \n",
      "ELE17451 GRO73461 DAI22896 SNA99873 FRO86643 \n",
      "ELE17451 ELE37798 FRO86643 GRO56989 ELE23393 SNA11465 \n",
      "ELE17451 SNA69641 FRO86643 FRO78087 SNA11465 GRO39357 ELE28573 ELE11375 DAI54444 \n"
     ]
    }
   ],
   "source": [
    "# Take a peak at the data\n",
    "!head -n 5 ProductPurchaseData.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Put the data into HDFS - adjust path or filename if needed\n",
    "!hdfs dfs -put ProductPurchaseData.txt {HDFS_DIR}/ProductPurchaseData.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# HW Problems\n",
    "\n",
    "## HW3.0.\n",
    "1. How do you merge  two sorted  lists/arrays of records of the form [key, value]?\n",
    "1. Where is this  used in Hadoop MapReduce? [Hint within the shuffle]\n",
    "1. What is  a combiner function in the context of Hadoop? \n",
    "1. Give an example where it can be used and justify why it should be used in the context of this problem.\n",
    "1. What is the Hadoop shuffle?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# START STUDENT RESPONSE HW3.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# END STUDENT RESPONSE HW3.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## HW3.1 Consumer Complaints EDA - exploring counters\n",
    "Notes about Counters:\n",
    ">Counters are lightweight objects in Hadoop that allow you to keep track of system progress in both the map and reduce stages of processing. By default, Hadoop defines a number of standard counters in \"groups\"; these show up in the jobtracker webapp, giving you information such as \"Map input records\", \"Map output records\", etc. \n",
    "\n",
    ">While processing information/data using MapReduce job, it is a challenge to monitor the progress of parallel threads running across nodes of distributed clusters. Moreover, it is also complicated to distinguish between the data that has been processed and the data which is yet to be processed. The MapReduce Framework also offers a provision of user-defined Counters, which can be effectively utilized to monitor the progress of data across nodes of distributed clusters.\n",
    "\n",
    "In this homework problem, we'll use Hadoop Counters to identify the number of complaints pertaining to debt collection, mortgage and other categories (all other categories get lumped into this one) in the consumer complaints dataset (__`Consumer_Complaints.csv`__). Basically, your goal is to produce the distribution of the Product column in this dataset using counters (limited to 3 counters here).\n",
    "\n",
    "Hadoop offers Job Tracker, an UI tool to determine the status and statistics of all jobs. Using the job tracker UI, developers can view the Counters that have been created. Screenshot your job tracker UI as your job completes and include it here. Make sure that your user defined counters are visible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Confirm the data location in HDFS - adjust path as needed\n",
    "!hdfs dfs -ls {HDFS_DIR} | grep Consumer*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting complaintCountsMapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile complaintCountsMapper.py\n",
    "#!/usr/bin/env python\n",
    "\n",
    "# START STUDENT CODE HW31MAPPER\n",
    "\n",
    "\"\"\"\n",
    "Reads stdin line by line and prints for each prodcut, product \\t 1\n",
    "INPUT:\n",
    "    text file\n",
    "OUTPUT:\n",
    "    product \\t 1\n",
    "USAGE:\n",
    "    python complaintCountsMapper.py < textfile\n",
    "\"\"\"\n",
    "\n",
    "import sys\n",
    "sys.stderr.write(\"reporter:counter:Mapper Counters,Calls,1\\n\")\n",
    "\n",
    "# read from standard input\n",
    "for line in sys.stdin:\n",
    "    token = line.strip().split(',')\n",
    "    if token[1].lower() == 'debt collection':\n",
    "        print '%s\\t%s' % (token[1].lower(), 1)\n",
    "        sys.stderr.write(\"reporter:counter:Product,debt collecction,1\\n\")\n",
    "    elif token[1].lower() == 'mortgage':\n",
    "        print '%s\\t%s' % (token[1], 1)\n",
    "        sys.stderr.write(\"reporter:counter:Product,mortgage,1\\n\")\n",
    "    else:\n",
    "        print '%s\\t%s' % ('others', 1)\n",
    "        sys.stderr.write(\"reporter:counter:Product,others,1\\n\")\n",
    "        \n",
    "# END STUDENT CODE HW31MAPPER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!chmod a+x complaintCountsMapper.py ; ls -las\n",
    "#!rm test\n",
    "#!cat Consumer_Complaints.csv | ./complaintCountsMapper.py > test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "others\t1\n",
      "debt collection\t1\n",
      "debt collection\t1\n",
      "others\t1\n",
      "debt collection\t1\n",
      "others\t1\n",
      "debt collection\t1\n",
      "others\t1\n",
      "debt collection\t1\n",
      "others\t1\n",
      "cat: write error: Broken pipe\n"
     ]
    }
   ],
   "source": [
    "!cat test | head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%writefile complaintCountsReducer.py\n",
    "#!/usr/bin/env python\n",
    "# START STUDENT CODE HW31REDUCER\n",
    "\n",
    "\"\"\"    \n",
    "Reads output from mapper.py and counts frequency for each product. \n",
    "INPUT:\n",
    "    product \\t 1\n",
    "OUTPUT:\n",
    "    product \\t count\n",
    "USAGE:\n",
    "    python complaintCountReducer.py < output of mapper.py\n",
    "\"\"\"\n",
    "\n",
    "import sys\n",
    "sys.stderr.write(\"reporter:counter:Reducer Counters,Calls,1\\n\")\n",
    "\n",
    "# initialize trackers\n",
    "cur_product = None\n",
    "cur_count = 0\n",
    "\n",
    "# read input key-value pairs from standard input\n",
    "for line in sys.stdin:\n",
    "    # split product \\ t into key and value\n",
    "    key, value = line.split('\\t')\n",
    "    if key == cur_product: \n",
    "        cur_count += int(value)\n",
    "    # OR emit current total and start a new tally \n",
    "    else: \n",
    "        if cur_product:\n",
    "            print '%s\\t%s' % (cur_product, cur_count)\n",
    "        cur_product, cur_count  = key, int(value)\n",
    "\n",
    "# don't forget the last record! \n",
    "print '%s\\t%s' % (cur_product, cur_count)\n",
    "    \n",
    "    \n",
    "# END STUDENT CODE HW31REDUCER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!chmod a+x complaintCountsReducer.py ; ls -las\n",
    "!cat test | sort | ./complaintCountsReducer.py "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!hdfs dfs -rm -r {HDFS_DIR}/complaints-output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Hadoop command\n",
    "# START STUDENT CODE HW31HADOOP\n",
    "\n",
    "!hadoop jar {JAR_FILE} \\\n",
    "  -D mapreduce.job.output.key.comparator.class=org.apache.hadoop.mapred.lib.KeyFieldBasedComparator \\\n",
    "  -D mapreduce.partition.keycomparator.options='-k1' \\\n",
    "  -files complaintCountsMapper.py,complaintCountsReducer.py \\\n",
    "  -mapper complaintCountsMapper.py \\\n",
    "  -reducer complaintCountsReducer.py \\\n",
    "  -input {HDFS_DIR}/Consumer_Complaints.csv \\\n",
    "  -output {HDFS_DIR}/complaints-output \\\n",
    "\n",
    "# END STUDENT CODE HW31HADOOP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# take a look at your results\n",
    "#!hdfs dfs -ls {HDFS_DIR}/complaints-output\n",
    "!hdfs dfs -cat {HDFS_DIR}/complaints-output/part-0000* > complaintCounts.tsv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_table('complaintCounts.tsv',header=None)\n",
    "complaints = (data[0])\n",
    "y_pos = np.arange(len(complaints))\n",
    "counts = data[1]\n",
    "\n",
    "plt.bar(y_pos, counts, align='center')\n",
    "plt.xticks(y_pos, complaints, rotation='vertical')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Complaint')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mrjob3_1.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mrjob3_1.py\n",
    "\n",
    "import sys\n",
    "from mrjob.job import MRJob\n",
    "  \n",
    "class MR31(MRJob):\n",
    "\n",
    "    def mapper(self, _, line):\n",
    "        token = line.strip().split(',')\n",
    "        if token[1].lower() == 'debt collection':\n",
    "            yield token[1].lower(), 1\n",
    "            sys.stderr.write(\"reporter:counter:Product,debt collecction,1\\n\")\n",
    "        elif token[1].lower() == 'mortgage':\n",
    "            yield token[1], 1\n",
    "            sys.stderr.write(\"reporter:counter:Product,mortgage,1\\n\")\n",
    "        else:\n",
    "            yield 'others', 1\n",
    "            sys.stderr.write(\"reporter:counter:Product,others,1\\n\")\n",
    "     \n",
    "    def combiner(self, key, counts):\n",
    "        yield key, sum(counts)\n",
    "\n",
    "    #hello, (1,1,1,1,1,1): using a combiner? NO and YEs\n",
    "    def reducer(self, key, counts):\n",
    "        yield key, sum(counts)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    MR31.run()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No configs found; falling back on auto-configuration\n",
      "No configs specified for hadoop runner\n",
      "Looking for hadoop binary in $PATH...\n",
      "Found hadoop binary: /usr/bin/hadoop\n",
      "Using Hadoop version 2.6.0\n",
      "Looking for Hadoop streaming jar in /home/hadoop/contrib...\n",
      "Looking for Hadoop streaming jar in /usr/lib/hadoop-mapreduce...\n",
      "Found Hadoop streaming jar: /usr/lib/hadoop-mapreduce/hadoop-streaming.jar\n",
      "Creating temp directory /tmp/mrjob3_1.root.20180219.120814.896004\n",
      "Copying local files to hdfs:///user/root/tmp/mrjob/mrjob3_1.root.20180219.120814.896004/files/...\n",
      "Running step 1 of 1...\n",
      "  packageJobJar: [] [/usr/jars/hadoop-streaming-2.6.0-cdh5.7.0.jar] /tmp/streamjob2698454272317823685.jar tmpDir=null\n",
      "  Connecting to ResourceManager at /0.0.0.0:8032\n",
      "  Connecting to ResourceManager at /0.0.0.0:8032\n",
      "  Total input paths to process : 1\n",
      "  number of splits:2\n",
      "  Submitting tokens for job: job_1519041572493_0003\n",
      "  Submitted application application_1519041572493_0003\n",
      "  The url to track the job: http://docker.w261:8088/proxy/application_1519041572493_0003/\n",
      "  Running job: job_1519041572493_0003\n",
      "  Job job_1519041572493_0003 running in uber mode : false\n",
      "   map 0% reduce 0%\n",
      "   map 50% reduce 0%\n",
      "   map 100% reduce 0%\n",
      "   map 100% reduce 100%\n",
      "  Job job_1519041572493_0003 completed successfully\n",
      "  Output directory: hdfs:///user/root/tmp/mrjob/mrjob3_1.root.20180219.120814.896004/output\n",
      "Counters: 52\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=50910582\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=58\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=129\n",
      "\t\tFILE: Number of bytes written=355508\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=50910922\n",
      "\t\tHDFS: Number of bytes written=58\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of read operations=9\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tData-local map tasks=2\n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tTotal megabyte-seconds taken by all map tasks=7241728\n",
      "\t\tTotal megabyte-seconds taken by all reduce tasks=2425856\n",
      "\t\tTotal time spent by all map tasks (ms)=7072\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=7072\n",
      "\t\tTotal time spent by all reduce tasks (ms)=2369\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=2369\n",
      "\t\tTotal vcore-seconds taken by all map tasks=7072\n",
      "\t\tTotal vcore-seconds taken by all reduce tasks=2369\n",
      "\tMap-Reduce Framework\n",
      "\t\tCPU time spent (ms)=6910\n",
      "\t\tCombine input records=312913\n",
      "\t\tCombine output records=6\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tGC time elapsed (ms)=57\n",
      "\t\tInput split bytes=340\n",
      "\t\tMap input records=312913\n",
      "\t\tMap output bytes=4092895\n",
      "\t\tMap output materialized bytes=135\n",
      "\t\tMap output records=312913\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tPhysical memory (bytes) snapshot=959680512\n",
      "\t\tReduce input groups=3\n",
      "\t\tReduce input records=6\n",
      "\t\tReduce output records=3\n",
      "\t\tReduce shuffle bytes=135\n",
      "\t\tShuffled Maps =2\n",
      "\t\tSpilled Records=12\n",
      "\t\tTotal committed heap usage (bytes)=1513095168\n",
      "\t\tVirtual memory (bytes) snapshot=4253573120\n",
      "\tProduct\n",
      "\t\tdebt collecction=44372\n",
      "\t\tmortgage=125752\n",
      "\t\tothers=142789\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "Streaming final output from hdfs:///user/root/tmp/mrjob/mrjob3_1.root.20180219.120814.896004/output...\n",
      "\"Mortgage\"\t125752\n",
      "\"debt collection\"\t44372\n",
      "\"others\"\t142789\n",
      "Removing HDFS temp directory hdfs:///user/root/tmp/mrjob/mrjob3_1.root.20180219.120814.896004...\n",
      "Removing temp directory /tmp/mrjob3_1.root.20180219.120814.896004...\n"
     ]
    }
   ],
   "source": [
    "!python mrjob3_1.py \\\n",
    "    -r hadoop Consumer_Complaints.csv \\\n",
    "    --cmdenv PATH=/opt/anaconda/bin:$PATH \\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No handlers could be found for logger \"mrjob.conf\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(u'others', 142789)\n",
      "(u'Mortgage', 125752)\n",
      "(u'debt collection', 44372)\n"
     ]
    }
   ],
   "source": [
    "from mrjob3_1 import MR31\n",
    "mr_job = MR31(args=['-r',\n",
    "                       'local',\n",
    "                       'Consumer_Complaints.csv',\n",
    "                      ])\n",
    "with mr_job.make_runner() as runner: \n",
    "    runner.run()\n",
    "    # stream_output: get access of the output \n",
    "    for line in runner.stream_output():\n",
    "        print mr_job.parse_output_line(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HW 3.2 Analyze the performance of your Mappers, Combiners and Reducers using Counters\n",
    "\n",
    "For this brief study the Input file will be one record (the next line only):    \n",
    "`foo foo quux labs foo bar quux`\n",
    "\n",
    "\n",
    "__3.2.A__     \n",
    "Perform a word count analysis of this single record dataset using a Mapper and Reducer based WordCount (i.e., no combiners are used here) using user defined Counters to count up how many times the mapper and reducer are called. What is the value of your user defined Mapper Counter, and Reducer Counter after completing this word count job. The answer  should be 1 and 4 respectively. Please explain.\n",
    "\n",
    "__3.2.B__   \n",
    "Please use mulitple mappers and reducers for these jobs (at least 2 mappers and 2 reducers).\n",
    "Perform a word count analysis of the Issue column of the Consumer Complaints  Dataset using a Mapper and Reducer based WordCount (i.e., no combiners used anywhere)  using user defined Counters to count up how many time the mapper and reducer are called. What is the value of your user defined Mapper Counter, and Reducer Counter after completing your word count job. \n",
    "\n",
    "__3.2.C__     \n",
    "Perform a word count analysis of the Issue column of the Consumer Complaints  Dataset using a Mapper, Reducer, and standalone combiner (i.e., not an in-memory combiner) based WordCount using user defined Counters to count up how many time the mapper, combiner, reducer are called. What is the value of your user defined Mapper Counter, and Reducer Counter after completing your word count job. \n",
    "\n",
    "Then, using a single reducer: \n",
    "- What are the top 50 most frequent terms in your word count analysis?    \n",
    "- Present the top 50 terms and their frequency and their relative frequency. If there are ties please sort the tokens in alphanumeric/string order.    \n",
    "- Present bottom 10 tokens (least frequent items). \n",
    "\n",
    "__NOTE:__ You can use: `WORD_RE = re.compile(r\"[\\w']+\")` to tokenize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting test.data\n"
     ]
    }
   ],
   "source": [
    "%%writefile test.data\n",
    "foo foo quux labs foo bar quux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!hdfs dfs -put test.data {HDFS_DIR}/test.data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.A SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%writefile mapper3.2.A.py\n",
    "#!/usr/bin/env python\n",
    "# START STUDENT CODE HW32AMAPPER\n",
    "\n",
    "import sys\n",
    "sys.stderr.write(\"reporter:counter:Mapper Counters,Calls,1\\n\")\n",
    "\n",
    "# read from standard input\n",
    "for line in sys.stdin:\n",
    "    words = line.strip().split()\n",
    "    for word in words:\n",
    "        print '%s\\t%s' % (word, 1)\n",
    "        #sys.stderr.write(\"reporter:counter:Product,debt collecction,1\\n\")\n",
    "\n",
    "\n",
    "# END STUDENT CODE HW32AMAPPER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%writefile reducer3.2.A.py\n",
    "#!/usr/bin/env python\n",
    "# START STUDENT CODE HW32AREDUCER\n",
    "\n",
    "import sys\n",
    "sys.stderr.write(\"reporter:counter:Reducer Counters,Calls,1\\n\")\n",
    "\n",
    "# initialize trackers\n",
    "cur_word = None\n",
    "cur_count = 0\n",
    "\n",
    "# read input key-value pairs from standard input\n",
    "for line in sys.stdin:\n",
    "    # split product \\ t into key and value\n",
    "    key, value = line.split('\\t')\n",
    "    if key == cur_word: \n",
    "        cur_count += int(value)\n",
    "    # OR emit current total and start a new tally \n",
    "    else: \n",
    "        if cur_word:\n",
    "            print '%s\\t%s' % (cur_word, cur_count)\n",
    "        cur_word, cur_count  = key, int(value)\n",
    "\n",
    "# don't forget the last record! \n",
    "print '%s\\t%s' % (cur_word, cur_count)\n",
    "    \n",
    "\n",
    "# END STUDENT CODE HW32AREDUCER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!hdfs dfs -rm -r {HDFS_DIR}/counters-output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Hadoop command\n",
    "# START STUDENT CODE HW32AHADOOP\n",
    "\n",
    "!hadoop jar {JAR_FILE} \\\n",
    "  -D mapreduce.job.maps=1 \\\n",
    "  -files mapper3.2.A.py,reducer3.2.A.py \\\n",
    "  -mapper mapper3.2.A.py \\\n",
    "  -reducer reducer3.2.A.py \\\n",
    "  -input {HDFS_DIR}/test.data \\\n",
    "  -output {HDFS_DIR}/counters-output \\\n",
    "  -numReduceTasks 4\n",
    "\n",
    "# END STUDENT CODE HW32AHADOOP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# INSERT SCREENSHOT OF JOB TRACKER UI COUNTERS\n",
    "!hdfs dfs -cat /user/root/HW3/counters-output/part-0000*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mrjob3_2A.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mrjob3_2A.py\n",
    "\n",
    "import sys\n",
    "from mrjob.job import MRJob\n",
    "  \n",
    "class MR32A(MRJob):\n",
    "    \n",
    "    SORT_VALUES = True\n",
    "\n",
    "    def mapper(self, _, line):\n",
    "        words = line.strip().split()\n",
    "        for word in words:\n",
    "            sys.stderr.write(\"reporter:counter:Mapper,Calls,1\\n\")\n",
    "            yield word, 1\n",
    "        \n",
    "    def reducer(self, key, counts):\n",
    "        sys.stderr.write(\"reporter:counter:Reducer,Calls,1\\n\")\n",
    "        yield key, sum(counts)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    MR32A.run()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No configs found; falling back on auto-configuration\n",
      "No configs specified for hadoop runner\n",
      "Looking for hadoop binary in $PATH...\n",
      "Found hadoop binary: /usr/bin/hadoop\n",
      "Using Hadoop version 2.6.0\n",
      "Looking for Hadoop streaming jar in /home/hadoop/contrib...\n",
      "Looking for Hadoop streaming jar in /usr/lib/hadoop-mapreduce...\n",
      "Found Hadoop streaming jar: /usr/lib/hadoop-mapreduce/hadoop-streaming.jar\n",
      "Creating temp directory /tmp/mrjob3_2A.root.20180219.123150.046308\n",
      "Copying local files to hdfs:///user/root/tmp/mrjob/mrjob3_2A.root.20180219.123150.046308/files/...\n",
      "Running step 1 of 1...\n",
      "  packageJobJar: [] [/usr/jars/hadoop-streaming-2.6.0-cdh5.7.0.jar] /tmp/streamjob5554389781587126328.jar tmpDir=null\n",
      "  Connecting to ResourceManager at /0.0.0.0:8032\n",
      "  Connecting to ResourceManager at /0.0.0.0:8032\n",
      "  Total input paths to process : 1\n",
      "  number of splits:2\n",
      "  Submitting tokens for job: job_1519041572493_0011\n",
      "  Submitted application application_1519041572493_0011\n",
      "  The url to track the job: http://docker.w261:8088/proxy/application_1519041572493_0011/\n",
      "  Running job: job_1519041572493_0011\n",
      "  Job job_1519041572493_0011 running in uber mode : false\n",
      "   map 0% reduce 0%\n",
      "   map 50% reduce 0%\n",
      "   map 100% reduce 0%\n",
      "   map 100% reduce 100%\n",
      "  Job job_1519041572493_0011 completed successfully\n",
      "  Output directory: hdfs:///user/root/tmp/mrjob/mrjob3_2A.root.20180219.123150.046308/output\n",
      "Counters: 51\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=45\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=34\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=86\n",
      "\t\tFILE: Number of bytes written=356118\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=359\n",
      "\t\tHDFS: Number of bytes written=34\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of read operations=9\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tData-local map tasks=2\n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tTotal megabyte-seconds taken by all map tasks=4647936\n",
      "\t\tTotal megabyte-seconds taken by all reduce tasks=2404352\n",
      "\t\tTotal time spent by all map tasks (ms)=4539\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=4539\n",
      "\t\tTotal time spent by all reduce tasks (ms)=2348\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=2348\n",
      "\t\tTotal vcore-seconds taken by all map tasks=4539\n",
      "\t\tTotal vcore-seconds taken by all reduce tasks=2348\n",
      "\tMap-Reduce Framework\n",
      "\t\tCPU time spent (ms)=1540\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tGC time elapsed (ms)=41\n",
      "\t\tInput split bytes=314\n",
      "\t\tMap input records=1\n",
      "\t\tMap output bytes=66\n",
      "\t\tMap output materialized bytes=92\n",
      "\t\tMap output records=7\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tPhysical memory (bytes) snapshot=920035328\n",
      "\t\tReduce input groups=4\n",
      "\t\tReduce input records=7\n",
      "\t\tReduce output records=4\n",
      "\t\tReduce shuffle bytes=92\n",
      "\t\tShuffled Maps =2\n",
      "\t\tSpilled Records=14\n",
      "\t\tTotal committed heap usage (bytes)=1513095168\n",
      "\t\tVirtual memory (bytes) snapshot=4243431424\n",
      "\tMapper\n",
      "\t\tCalls=7\n",
      "\tReducer\n",
      "\t\tCalls=4\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "Streaming final output from hdfs:///user/root/tmp/mrjob/mrjob3_2A.root.20180219.123150.046308/output...\n",
      "\"bar\"\t1\n",
      "\"foo\"\t3\n",
      "\"labs\"\t1\n",
      "\"quux\"\t2\n",
      "Removing HDFS temp directory hdfs:///user/root/tmp/mrjob/mrjob3_2A.root.20180219.123150.046308...\n",
      "Removing temp directory /tmp/mrjob3_2A.root.20180219.123150.046308...\n"
     ]
    }
   ],
   "source": [
    "!python mrjob3_2A.py \\\n",
    "    -r hadoop test.data \\\n",
    "    --jobconf numReduceTasks=1 \\\n",
    "    --cmdenv PATH=/opt/anaconda/bin:$PATH \\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(u'bar', 1)\n",
      "(u'foo', 3)\n",
      "(u'labs', 1)\n",
      "(u'quux', 2)\n"
     ]
    }
   ],
   "source": [
    "from mrjob3_2A import MR32A\n",
    "mr_job = MR32A(args=['-r',\n",
    "                    'hadoop',\n",
    "                    'test.data',\n",
    "                    '--cmdenv',\n",
    "                    'PATH=/opt/anaconda/bin:$PATH', \n",
    "                      ])\n",
    "with mr_job.make_runner() as runner: \n",
    "    runner.run()\n",
    "    # stream_output: get access of the output \n",
    "    for line in runner.stream_output():\n",
    "        print mr_job.parse_output_line(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2.A EXPLANATION\n",
    "STUDENT TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.B SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%writefile mapper3.2.B.py\n",
    "#!/usr/bin/env python\n",
    "# START STUDENT CODE HW32BMAPPER\n",
    "\n",
    "import re\n",
    "import sys\n",
    "sys.stderr.write(\"reporter:counter:Mapper Counters,Calls,1\\n\")\n",
    "\n",
    "# read from standard input\n",
    "for line in sys.stdin:\n",
    "    issue = line.strip().split(',')[3]\n",
    "    words = re.findall(r'[a-z]+', issue.lower())\n",
    "    for word in words:\n",
    "        print '%s\\t%s' % (word, 1)\n",
    "        sys.stderr.write(\"reporter:counter:Issue,word,1\\n\")\n",
    "\n",
    "# END STUDENT CODE HW32BMAPPER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!head Consumer_Complaints.csv  | ./mapper3.2.B.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%writefile reducer3.2.B.py\n",
    "#!/usr/bin/env python\n",
    "# START STUDENT CODE HW32BREDUCER\n",
    "\n",
    "import sys\n",
    "sys.stderr.write(\"reporter:counter:Reducer Counters,Calls,1\\n\")\n",
    "\n",
    "# initialize trackers\n",
    "cur_issue = None\n",
    "cur_count = 0\n",
    "\n",
    "# read input key-value pairs from standard input\n",
    "for line in sys.stdin:\n",
    "    # split product \\ t into key and value\n",
    "    key, value = line.split('\\t')\n",
    "    if key == cur_issue: \n",
    "        cur_count += int(value)\n",
    "    # OR emit current total and start a new tally \n",
    "    else: \n",
    "        if cur_issue:\n",
    "            print '%s\\t%s' % (cur_issue, cur_count)\n",
    "        cur_issue, cur_count  = key, int(value)\n",
    "\n",
    "# don't forget the last record! \n",
    "print '%s\\t%s' % (cur_issue, cur_count)\n",
    "    \n",
    "\n",
    "# END STUDENT CODE HW32BREDUCER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!hdfs dfs -rm -r {HDFS_DIR}/complaints-output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Hadoop command\n",
    "# START STUDENT CODE HW32BHADOOP\n",
    "\n",
    "!hadoop jar {JAR_FILE} \\\n",
    "  -D mapreduce.job.maps=2 \\\n",
    "  -files mapper3.2.B.py,reducer3.2.B.py \\\n",
    "  -mapper mapper3.2.B.py \\\n",
    "  -reducer reducer3.2.B.py \\\n",
    "  -input {HDFS_DIR}/Consumer_Complaints.csv \\\n",
    "  -output {HDFS_DIR}/complaints-output \\\n",
    "  -numReduceTasks 4\n",
    "\n",
    "\n",
    "# END STUDENT CODE HW32BHADOOP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 3.2.B OUTPUT/ANSWER\n",
    "!hdfs dfs -ls {HDFS_DIR}/complaints-output/\n",
    "#!hdfs dfs -cat {HDFS_DIR}/complaints-output/part-0000* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# INSERT SCREENSHOT OF JOB TRACKER UI COUNTERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mrjob3_2B.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mrjob3_2B.py\n",
    "\n",
    "import sys\n",
    "import re\n",
    "from mrjob.job import MRJob\n",
    "  \n",
    "class MR32B(MRJob):\n",
    "    \n",
    "    SORT_VALUES = True\n",
    "\n",
    "    def mapper(self, _, line):\n",
    "        issue = line.strip().split(',')[3]\n",
    "        words = re.findall(r'[a-z]+', issue.lower())\n",
    "        for word in words:\n",
    "            sys.stderr.write(\"reporter:counter:Issue,word,1\\n\")\n",
    "            sys.stderr.write(\"reporter:counter:Mapper,Calls,1\\n\")\n",
    "            yield word, 1\n",
    "        \n",
    "    def reducer(self, key, counts):\n",
    "        sys.stderr.write(\"reporter:counter:Reducer,Calls,1\\n\")\n",
    "        yield key, sum(counts)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    MR32B.run()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No configs found; falling back on auto-configuration\n",
      "No configs specified for local runner\n",
      "Creating temp directory /tmp/mrjob3_2B.root.20180219.124143.270621\n",
      "Running step 1 of 1...\n",
      "\n",
      "Counters: 3\n",
      "\tIssue\n",
      "\t\tword=1000113\n",
      "\tMapper\n",
      "\t\tCalls=1000113\n",
      "\tReducer\n",
      "\t\tCalls=172\n",
      "\n",
      "Streaming final output from /tmp/mrjob3_2B.root.20180219.124143.270621/output...\n",
      "\"company\"\t4858\n",
      "\"cont\"\t11848\n",
      "\"contact\"\t3053\n",
      "\"convenience\"\t75\n",
      "\"costs\"\t4350\n",
      "\"credit\"\t55251\n",
      "\"embezzlement\"\t3276\n",
      "\"expect\"\t807\n",
      "\"false\"\t2508\n",
      "\"fee\"\t3198\n",
      "\"fees\"\t807\n",
      "\"for\"\t929\n",
      "\"forbearance\"\t350\n",
      "\"fraud\"\t3842\n",
      "\"funds\"\t5663\n",
      "\"get\"\t4357\n",
      "\"getting\"\t291\n",
      "\"health\"\t12545\n",
      "\"money\"\t413\n",
      "\"monitoring\"\t1453\n",
      "\"my\"\t10731\n",
      "\"not\"\t12353\n",
      "\"of\"\t10885\n",
      "\"on\"\t29069\n",
      "\"low\"\t5663\n",
      "\"making\"\t3226\n",
      "\"managing\"\t5006\n",
      "\"marketing\"\t1193\n",
      "\"missing\"\t64\n",
      "\"modification\"\t70487\n",
      "\"i\"\t925\n",
      "\"identity\"\t4729\n",
      "\"illegal\"\t2505\n",
      "\"improper\"\t4309\n",
      "\"incorrect\"\t29133\n",
      "\"increase\"\t1149\n",
      "\"info\"\t2896\n",
      "\"information\"\t29069\n",
      "\"settlement\"\t4350\n",
      "\"sharing\"\t2832\n",
      "\"shopping\"\t672\n",
      "\"statement\"\t1220\n",
      "\"statements\"\t2508\n",
      "\"stop\"\t131\n",
      "\"t\"\t2924\n",
      "\"tactics\"\t6920\n",
      "\"taking\"\t3747\n",
      "\"terms\"\t350\n",
      "\"the\"\t6248\n",
      "\"available\"\t274\n",
      "\"balance\"\t597\n",
      "\"bank\"\t202\n",
      "\"bankruptcy\"\t222\n",
      "\"being\"\t5663\n",
      "\"billing\"\t8158\n",
      "\"by\"\t5663\n",
      "\"can\"\t1999\n",
      "\"cancelling\"\t2795\n",
      "\"card\"\t4405\n",
      "\"cash\"\t240\n",
      "\"caused\"\t5663\n",
      "\"a\"\t3503\n",
      "\"account\"\t20681\n",
      "\"acct\"\t163\n",
      "\"action\"\t2505\n",
      "\"advance\"\t240\n",
      "\"advertising\"\t1193\n",
      "\"amount\"\t98\n",
      "\"amt\"\t71\n",
      "\"an\"\t2505\n",
      "\"and\"\t16448\n",
      "\"processing\"\t243\n",
      "\"promised\"\t274\n",
      "\"protection\"\t4139\n",
      "\"rate\"\t3431\n",
      "\"receive\"\t139\n",
      "\"received\"\t216\n",
      "\"receiving\"\t3226\n",
      "\"relations\"\t1367\n",
      "\"repay\"\t1647\n",
      "\"repaying\"\t3844\n",
      "\"report\"\t34903\n",
      "\"interest\"\t4238\n",
      "\"investigation\"\t4858\n",
      "\"issuance\"\t640\n",
      "\"issue\"\t1099\n",
      "\"issues\"\t538\n",
      "\"late\"\t1797\n",
      "\"lease\"\t6337\n",
      "\"lender\"\t2165\n",
      "\"line\"\t1732\n",
      "\"loan\"\t119630\n",
      "\"application\"\t8868\n",
      "\"applied\"\t139\n",
      "\"apply\"\t118\n",
      "\"apr\"\t3431\n",
      "\"arbitration\"\t168\n",
      "\"are\"\t3821\n",
      "\"atm\"\t2422\n",
      "\"attempts\"\t11848\n",
      "\"opening\"\t16205\n",
      "\"or\"\t22533\n",
      "\"other\"\t7886\n",
      "\"changes\"\t350\n",
      "\"charged\"\t976\n",
      "\"charges\"\t131\n",
      "\"checks\"\t75\n",
      "\"closing\"\t2795\n",
      "\"club\"\t12545\n",
      "\"collect\"\t11848\n",
      "\"collection\"\t1907\n",
      "\"communication\"\t6920\n",
      "\"credited\"\t92\n",
      "\"customer\"\t2734\n",
      "\"d\"\t11848\n",
      "\"day\"\t71\n",
      "\"dealing\"\t1944\n",
      "\"debit\"\t2422\n",
      "\"debt\"\t19309\n",
      "\"decision\"\t2774\n",
      "\"reporting\"\t6559\n",
      "\"representation\"\t2508\n",
      "\"rewards\"\t1002\n",
      "\"s\"\t4858\n",
      "\"sale\"\t139\n",
      "\"scam\"\t566\n",
      "\"score\"\t4357\n",
      "\"service\"\t1518\n",
      "\"servicer\"\t1944\n",
      "\"servicing\"\t36767\n",
      "\"out\"\t1242\n",
      "\"overlimit\"\t127\n",
      "\"owed\"\t11848\n",
      "\"pay\"\t3821\n",
      "\"payment\"\t92\n",
      "\"payments\"\t3226\n",
      "\"payoff\"\t1155\n",
      "\"plans\"\t350\n",
      "\"practices\"\t1003\n",
      "\"privacy\"\t240\n",
      "\"problems\"\t9484\n",
      "\"process\"\t5505\n",
      "\"theft\"\t3276\n",
      "\"threatening\"\t2505\n",
      "\"to\"\t8401\n",
      "\"transaction\"\t1485\n",
      "\"transfer\"\t597\n",
      "\"unable\"\t8178\n",
      "\"underwriting\"\t2774\n",
      "\"unsolicited\"\t640\n",
      "\"use\"\t1477\n",
      "\"using\"\t2422\n",
      "\"verification\"\t5214\n",
      "\"decrease\"\t1149\n",
      "\"delay\"\t243\n",
      "\"delinquent\"\t1061\n",
      "\"deposits\"\t10555\n",
      "\"determination\"\t1490\n",
      "\"did\"\t139\n",
      "\"didn\"\t925\n",
      "\"disclosure\"\t5214\n",
      "\"disclosures\"\t64\n",
      "\"dispute\"\t904\n",
      "\"disputes\"\t6938\n",
      "\"was\"\t274\n",
      "\"when\"\t4095\n",
      "\"with\"\t1944\n",
      "\"withdrawals\"\t10555\n",
      "\"workout\"\t350\n",
      "\"wrong\"\t169\n",
      "\"you\"\t3821\n",
      "\"your\"\t3844\n",
      "Removing temp directory /tmp/mrjob3_2B.root.20180219.124143.270621...\n"
     ]
    }
   ],
   "source": [
    "!python mrjob3_2B.py \\\n",
    "    -r local Consumer_Complaints.csv \\\n",
    "    --jobconf numReduceTasks=4 \\\n",
    "    --cmdenv PATH=/opt/anaconda/bin:$PATH \\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(u'a', 3503)\n",
      "(u'account', 20681)\n",
      "(u'acct', 163)\n",
      "(u'action', 2505)\n",
      "(u'advance', 240)\n",
      "(u'advertising', 1193)\n",
      "(u'amount', 98)\n",
      "(u'amt', 71)\n",
      "(u'an', 2505)\n",
      "(u'and', 16448)\n",
      "(u'application', 8868)\n",
      "(u'applied', 139)\n",
      "(u'apply', 118)\n",
      "(u'apr', 3431)\n",
      "(u'arbitration', 168)\n",
      "(u'are', 3821)\n",
      "(u'atm', 2422)\n",
      "(u'attempts', 11848)\n",
      "(u'available', 274)\n",
      "(u'balance', 597)\n",
      "(u'bank', 202)\n",
      "(u'bankruptcy', 222)\n",
      "(u'being', 5663)\n",
      "(u'billing', 8158)\n",
      "(u'by', 5663)\n",
      "(u'can', 1999)\n",
      "(u'cancelling', 2795)\n",
      "(u'card', 4405)\n",
      "(u'cash', 240)\n",
      "(u'caused', 5663)\n",
      "(u'changes', 350)\n",
      "(u'charged', 976)\n",
      "(u'charges', 131)\n",
      "(u'checks', 75)\n",
      "(u'closing', 2795)\n",
      "(u'club', 12545)\n",
      "(u'collect', 11848)\n",
      "(u'collection', 1907)\n",
      "(u'communication', 6920)\n",
      "(u'company', 4858)\n",
      "(u'cont', 11848)\n",
      "(u'contact', 3053)\n",
      "(u'convenience', 75)\n",
      "(u'costs', 4350)\n",
      "(u'credit', 55251)\n",
      "(u'credited', 92)\n",
      "(u'customer', 2734)\n",
      "(u'd', 11848)\n",
      "(u'day', 71)\n",
      "(u'dealing', 1944)\n",
      "(u'debit', 2422)\n",
      "(u'debt', 19309)\n",
      "(u'decision', 2774)\n",
      "(u'decrease', 1149)\n",
      "(u'delay', 243)\n",
      "(u'delinquent', 1061)\n",
      "(u'deposits', 10555)\n",
      "(u'determination', 1490)\n",
      "(u'did', 139)\n",
      "(u'didn', 925)\n",
      "(u'disclosure', 5214)\n",
      "(u'disclosures', 64)\n",
      "(u'dispute', 904)\n",
      "(u'disputes', 6938)\n",
      "(u'embezzlement', 3276)\n",
      "(u'expect', 807)\n",
      "(u'false', 2508)\n",
      "(u'fee', 3198)\n",
      "(u'fees', 807)\n",
      "(u'for', 929)\n",
      "(u'forbearance', 350)\n",
      "(u'fraud', 3842)\n",
      "(u'funds', 5663)\n",
      "(u'get', 4357)\n",
      "(u'getting', 291)\n",
      "(u'health', 12545)\n",
      "(u'i', 925)\n",
      "(u'identity', 4729)\n",
      "(u'illegal', 2505)\n",
      "(u'improper', 4309)\n",
      "(u'incorrect', 29133)\n",
      "(u'increase', 1149)\n",
      "(u'info', 2896)\n",
      "(u'information', 29069)\n",
      "(u'interest', 4238)\n",
      "(u'investigation', 4858)\n",
      "(u'issuance', 640)\n",
      "(u'issue', 1099)\n",
      "(u'issues', 538)\n",
      "(u'late', 1797)\n",
      "(u'lease', 6337)\n",
      "(u'lender', 2165)\n",
      "(u'line', 1732)\n",
      "(u'loan', 119630)\n",
      "(u'low', 5663)\n",
      "(u'making', 3226)\n",
      "(u'managing', 5006)\n",
      "(u'marketing', 1193)\n",
      "(u'missing', 64)\n",
      "(u'modification', 70487)\n",
      "(u'money', 413)\n",
      "(u'monitoring', 1453)\n",
      "(u'my', 10731)\n",
      "(u'not', 12353)\n",
      "(u'of', 10885)\n",
      "(u'on', 29069)\n",
      "(u'opening', 16205)\n",
      "(u'or', 22533)\n",
      "(u'other', 7886)\n",
      "(u'out', 1242)\n",
      "(u'overlimit', 127)\n",
      "(u'owed', 11848)\n",
      "(u'pay', 3821)\n",
      "(u'payment', 92)\n",
      "(u'payments', 3226)\n",
      "(u'payoff', 1155)\n",
      "(u'plans', 350)\n",
      "(u'practices', 1003)\n",
      "(u'privacy', 240)\n",
      "(u'problems', 9484)\n",
      "(u'process', 5505)\n",
      "(u'processing', 243)\n",
      "(u'promised', 274)\n",
      "(u'protection', 4139)\n",
      "(u'rate', 3431)\n",
      "(u'receive', 139)\n",
      "(u'received', 216)\n",
      "(u'receiving', 3226)\n",
      "(u'relations', 1367)\n",
      "(u'repay', 1647)\n",
      "(u'repaying', 3844)\n",
      "(u'report', 34903)\n",
      "(u'reporting', 6559)\n",
      "(u'representation', 2508)\n",
      "(u'rewards', 1002)\n",
      "(u's', 4858)\n",
      "(u'sale', 139)\n",
      "(u'scam', 566)\n",
      "(u'score', 4357)\n",
      "(u'service', 1518)\n",
      "(u'servicer', 1944)\n",
      "(u'servicing', 36767)\n",
      "(u'settlement', 4350)\n",
      "(u'sharing', 2832)\n",
      "(u'shopping', 672)\n",
      "(u'statement', 1220)\n",
      "(u'statements', 2508)\n",
      "(u'stop', 131)\n",
      "(u't', 2924)\n",
      "(u'tactics', 6920)\n",
      "(u'taking', 3747)\n",
      "(u'terms', 350)\n",
      "(u'the', 6248)\n",
      "(u'theft', 3276)\n",
      "(u'threatening', 2505)\n",
      "(u'to', 8401)\n",
      "(u'transaction', 1485)\n",
      "(u'transfer', 597)\n",
      "(u'unable', 8178)\n",
      "(u'underwriting', 2774)\n",
      "(u'unsolicited', 640)\n",
      "(u'use', 1477)\n",
      "(u'using', 2422)\n",
      "(u'verification', 5214)\n",
      "(u'was', 274)\n",
      "(u'when', 4095)\n",
      "(u'with', 1944)\n",
      "(u'withdrawals', 10555)\n",
      "(u'workout', 350)\n",
      "(u'wrong', 169)\n",
      "(u'you', 3821)\n",
      "(u'your', 3844)\n"
     ]
    }
   ],
   "source": [
    "from mrjob3_2B import MR32B\n",
    "mr_job = MR32B(args=['-r',\n",
    "                    'hadoop',\n",
    "                    'Consumer_Complaints.csv',\n",
    "                    '--cmdenv',\n",
    "                    'PATH=/opt/anaconda/bin:$PATH', \n",
    "                      ])\n",
    "with mr_job.make_runner() as runner: \n",
    "    runner.run()\n",
    "    # stream_output: get access of the output \n",
    "    for line in runner.stream_output():\n",
    "        print mr_job.parse_output_line(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.C SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%writefile mapper3.2.C.py\n",
    "#!/usr/bin/env python\n",
    "# START STUDENT CODE HW32CMAPPER\n",
    "\n",
    "import re\n",
    "import sys\n",
    "sys.stderr.write(\"reporter:counter:Mapper Counters,Calls,1\\n\")\n",
    "\n",
    "# read from standard input\n",
    "for line in sys.stdin:\n",
    "    issue = line.strip().split(',')[3]\n",
    "    words = re.findall(r'[a-z]+', issue.lower())\n",
    "    for word in words:\n",
    "        print '%s\\t%s' % (word, 1)\n",
    "        sys.stderr.write(\"reporter:counter:Issue,word,1\\n\")\n",
    "\n",
    "# END STUDENT CODE HW32CMAPPER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%writefile combiner3.2.C.py\n",
    "#!/usr/bin/env python\n",
    "# START STUDENT CODE HW32CCOMBINER\n",
    "\n",
    "import sys\n",
    "sys.stderr.write(\"reporter:counter:Combiner Counters,Calls,1\\n\")\n",
    "\n",
    "# initialize trackers\n",
    "cur_issue = None\n",
    "cur_count = 0\n",
    "\n",
    "# read input key-value pairs from standard input\n",
    "for line in sys.stdin:\n",
    "    # split product \\ t into key and value\n",
    "    key, value = line.split('\\t')\n",
    "    if key == cur_issue: \n",
    "        cur_count += int(value)\n",
    "    # OR emit current total and start a new tally \n",
    "    else: \n",
    "        if cur_issue:\n",
    "            print '%s\\t%s' % (cur_issue, cur_count)\n",
    "        cur_issue, cur_count  = key, int(value)\n",
    "\n",
    "# don't forget the last record! \n",
    "print '%s\\t%s' % (cur_issue, cur_count)\n",
    "    \n",
    "\n",
    "# END STUDENT CODE HW32CCOMBINER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%writefile reducer3.2.C.py\n",
    "#!/usr/bin/env python\n",
    "# START STUDENT CODE HW32CREDUCER\n",
    "\n",
    "\n",
    "import sys\n",
    "sys.stderr.write(\"reporter:counter:Reducer Counters,Calls,1\\n\")\n",
    "\n",
    "# initialize trackers\n",
    "cur_issue = None\n",
    "cur_count = 0\n",
    "\n",
    "# read input key-value pairs from standard input\n",
    "for line in sys.stdin:\n",
    "    # split product \\ t into key and value\n",
    "    key, value = line.split('\\t')\n",
    "    if key == cur_issue: \n",
    "        cur_count += int(value)\n",
    "    # OR emit current total and start a new tally \n",
    "    else: \n",
    "        if cur_issue:\n",
    "            print '%s\\t%s' % (cur_issue, cur_count)\n",
    "        cur_issue, cur_count  = key, int(value)\n",
    "\n",
    "# don't forget the last record! \n",
    "print '%s\\t%s' % (cur_issue, cur_count)\n",
    "    \n",
    "\n",
    "# END STUDENT CODE HW32CREDUCER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!hdfs dfs -rm -r {HDFS_DIR}/complaints-output3c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Hadoop command\n",
    "# START STUDENT CODE HW32CHADOOP\n",
    "\n",
    "!hadoop jar {JAR_FILE} \\\n",
    "  -files mapper3.2.C.py,reducer3.2.C.py,combiner3.2.C.py \\\n",
    "  -mapper mapper3.2.C.py \\\n",
    "  -combiner combiner3.2.C.py \\\n",
    "  -reducer reducer3.2.C.py \\\n",
    "  -input {HDFS_DIR}/Consumer_Complaints.csv \\\n",
    "  -output {HDFS_DIR}/complaints-output3c \\\n",
    "  -numReduceTasks 1\n",
    "\n",
    "\n",
    "#   -D mapreduce.job.maps=2 \\   -numReduceTasks 4\n",
    "\n",
    "\n",
    "# END STUDENT CODE HW32CHADOOP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 3.2.C OUTPUT/ANSWER\n",
    "!hdfs dfs -ls {HDFS_DIR}/complaints-output3c\n",
    "!hdfs dfs -cat {HDFS_DIR}/complaints-output3c/part-0000*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# INSERT SCREENSHOT OF JOB TRACKER UI COUNTERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%writefile frequencies_mapper3.2.C.py\n",
    "#!/usr/bin/env python\n",
    "# START STUDENT CODE HW32CFREQMAPPER\n",
    "\n",
    "import re\n",
    "import sys\n",
    "sys.stderr.write(\"reporter:counter:Mapper Counters,Calls,1\\n\")\n",
    "\n",
    "total = 0 \n",
    "\n",
    "# read from standard input\n",
    "for line in sys.stdin:\n",
    "    key, value = line.strip().split()\n",
    "    total += int(value)\n",
    "    print '%s\\t%s' % (key, value)\n",
    "\n",
    "print '%s\\t%s' % ('!total', total)\n",
    "\n",
    "# END STUDENT CODE HW32CFREQMAPPER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!chmod a+x frequencies_mapper3.2.C.py\n",
    "!hdfs dfs -cat {HDFS_DIR}/complaints-output3c/part-0000* | ./frequencies_mapper3.2.C.py > test.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%writefile frequencies_reducer3.2.C.py\n",
    "#!/usr/bin/env python\n",
    "# START STUDENT CODE HW32CFREQREDUCER\n",
    "\n",
    "import sys\n",
    "sys.stderr.write(\"reporter:counter:Reducer Counters,Calls,1\\n\")\n",
    "\n",
    "total = 0\n",
    "\n",
    "# read from standard input\n",
    "for line in sys.stdin:\n",
    "    # parse input\n",
    "    line = line.strip()\n",
    "    word, count = line.split('\\t')\n",
    "    \n",
    "    if word == '!total':\n",
    "        total += int(count)\n",
    "\n",
    "    else:\n",
    "        print \"%s\\t%s\\t%s\" % (count, float(count)/float(total), word)\n",
    "    \n",
    "\n",
    "# END STUDENT CODE HW32CFREQREDUCER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!chmod a+x frequencies_reducer3.2.C.py\n",
    "#!hdfs dfs -cat {HDFS_DIR}/complaints-output4/part-0000* | ./frequencies_reducer3.2.C.py\n",
    "#!cat test.data | sort -k2,2nr |  ./frequencies_reducer3.2.C.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!hdfs dfs -rm -r {HDFS_DIR}/complaints-output4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Hadoop command\n",
    "# START STUDENT CODE HW32CFREQHADOOP\n",
    "\n",
    "!hadoop jar {JAR_FILE} \\\n",
    "  -D stream.num.map.output.key.fields=3 \\\n",
    "  -D mapred.output.key.comparator.class=org.apache.hadoop.mapred.lib.KeyFieldBasedComparator \\\n",
    "  -D mapred.text.key.comparator.options=\"-k2,2nr -k1,1\" \\\n",
    "  -files frequencies_mapper3.2.C.py,frequencies_reducer3.2.C.py \\\n",
    "  -mapper frequencies_mapper3.2.C.py \\\n",
    "  -reducer frequencies_reducer3.2.C.py \\\n",
    "  -input {HDFS_DIR}/complaints-output3c/part-00000 \\\n",
    "  -output {HDFS_DIR}/complaints-output4 \\\n",
    "  -numReduceTasks 1\n",
    "\n",
    "  #-D stream.num.map.output.key.fields=2 \\\n",
    "  #-D mapred.output.key.comparator.class=org.apache.hadoop.mapred.lib.KeyFieldBasedComparator \\\n",
    "  #-D mapred.text.key.comparator.options=\"-k1,1nr -k2,2\" \\\n",
    "\n",
    "# END STUDENT CODE HW32CFREQHADOOP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 3.2.C OUTPUT/ANSWER\n",
    "print 'Top 50\\n'\n",
    "!hdfs dfs -cat {HDFS_DIR}/complaints-output4/part-0000* | head -n 50\n",
    "print 'Botto 10\\n'\n",
    "!hdfs dfs -cat {HDFS_DIR}/complaints-output4/part-0000* | tail -n 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 3.2.C OUTPUT/ANSWER\n",
    "!hdfs dfs -cat {HDFS_DIR}/complaints-output4/part-0000* | head -n 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile frequencies_mapper3.2.C.py\n",
    "#!/usr/bin/env python\n",
    "# START STUDENT CODE HW32CFREQMAPPER\n",
    "\n",
    "import re\n",
    "import sys\n",
    "sys.stderr.write(\"reporter:counter:Mapper Counters,Calls,1\\n\")\n",
    "\n",
    "total = 0 \n",
    "\n",
    "# read from standard input\n",
    "for line in sys.stdin:\n",
    "    key, value = line.strip().split()\n",
    "    total += int(value)\n",
    "    print '%s\\t%s' % (value, key)\n",
    "\n",
    "print '%s\\t%s' % (total, 'zzzz')\n",
    "\n",
    "# END STUDENT CODE HW32CFREQMAPPER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%writefile frequencies_reducer3.2.C.py\n",
    "#!/usr/bin/env python\n",
    "# START STUDENT CODE HW32CFREQREDUCER\n",
    "\n",
    "import sys\n",
    "sys.stderr.write(\"reporter:counter:Reducer Counters,Calls,1\\n\")\n",
    "\n",
    "# initialize trackers\n",
    "cur_word = None\n",
    "total_counts = 0\n",
    "start, stop = 0,50\n",
    "\n",
    "# read from standard input\n",
    "for line in sys.stdin:\n",
    "    # parse input\n",
    "    count, word = line.split()\n",
    "    \n",
    "    if word == 'zzzz':\n",
    "        total_counts += int(count)\n",
    "        print total_counts\n",
    "\n",
    "    # Now process rest\n",
    "    \n",
    "    if (word != 'zzzz') and cur_word:\n",
    "        if start <= stop:\n",
    "            print \"%s\\t%s\\t%.4f\" % (word, count, float(total_counts))\n",
    "            start += 1\n",
    "            \n",
    "    cur_word = word\n",
    "    \n",
    "\n",
    "# END STUDENT CODE HW32CFREQREDUCER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!hdfs dfs -rm -r {HDFS_DIR}/complaints-output4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Hadoop command\n",
    "# START STUDENT CODE HW32CFREQHADOOP\n",
    "\n",
    "!hadoop jar {JAR_FILE} \\\n",
    "  -D stream.num.map.output.key.fields=2 \\\n",
    "  -D mapred.output.key.comparator.class=org.apache.hadoop.mapred.lib.KeyFieldBasedComparator \\\n",
    "  -D mapred.text.key.comparator.options=\"-k1,1n -k2,2\" \\\n",
    "  -files frequencies_mapper3.2.C.py,frequencies_reducer3.2.C.py \\\n",
    "  -mapper frequencies_mapper3.2.C.py \\\n",
    "  -reducer /bin/cat \\\n",
    "  -input {HDFS_DIR}/complaints-output3c/part-00000 \\\n",
    "  -output {HDFS_DIR}/complaints-output4 \\\n",
    "  -numReduceTasks 1\n",
    "\n",
    "# END STUDENT CODE HW32CFREQHADOOP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 3.2.C OUTPUT/ANSWER\n",
    "!hdfs dfs -cat {HDFS_DIR}/complaints-output4/part-0000*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mrjob3_2C.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mrjob3_2C.py\n",
    "\n",
    "import sys\n",
    "import re\n",
    "import json\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "from mrjob.protocol import RawProtocol\n",
    "\n",
    "WORD_RE = re.compile(r\"[\\w']+\")\n",
    "  \n",
    "class MR32C(MRJob):\n",
    "\n",
    "    SORT_VALUES = True\n",
    "    #OUTPUT_PROTOCOL = RawProtocol\n",
    "\n",
    "    def steps(self):\n",
    "        \n",
    "        JOBCONF_STEP1 = {\n",
    "                'mapreduce.job.output.key.comparator.class': 'org.apache.hadoop.mapred.lib.KeyFieldBasedComparator',\n",
    "                'stream.num.map.output.key.fields':'2',\n",
    "                'stream.map.output.field.separator':'\\t',\n",
    "                'mapreduce.partition.keycomparator.options': '-k1',\n",
    "                'mapreduce.job.reduces': '1', \n",
    "                }  \n",
    "        \n",
    "        JOBCONF_STEP2 = {\n",
    "                'mapreduce.job.output.key.comparator.class': 'org.apache.hadoop.mapred.lib.KeyFieldBasedComparator',\n",
    "                'stream.num.map.output.key.fields':'2',\n",
    "                'stream.map.output.field.separator':'\\t',\n",
    "                'mapreduce.partition.keycomparator.options': '-k1nr',\n",
    "                'mapreduce.job.reduces': '1', \n",
    "                }  \n",
    "        \n",
    "        return [\n",
    "        MRStep(jobconf=JOBCONF_STEP1, \n",
    "               mapper=self.mapper,\n",
    "               combiner=self.combiner,\n",
    "               reducer_init=self.reducer_init,\n",
    "               reducer=self.reducer,\n",
    "              ),\n",
    "        MRStep(jobconf=JOBCONF_STEP2, \n",
    "              mapper=self.mapper2,)     \n",
    "        ]\n",
    "\n",
    "\n",
    "    def mapper(self, _, line):\n",
    "        issue = line.strip().split(',')[3]\n",
    "        words = WORD_RE.findall(issue.lower())\n",
    "        for word in words:\n",
    "            yield '!TOTAL', 1\n",
    "            yield word, 1\n",
    "            \n",
    "    def combiner(self, word, counts):\n",
    "        yield word, sum(counts)\n",
    "   \n",
    "    def reducer_init(self):\n",
    "        self.total = 0\n",
    "\n",
    "    def reducer(self, word, counts):\n",
    "        if word == '!TOTAL':\n",
    "            self.total = sum(counts)\n",
    "        partial = sum(counts)\n",
    "        yield word, (partial, float(partial)/self.total)\n",
    "        \n",
    "    def mapper2(self, word, counts):\n",
    "        if word != '!TOTAL':\n",
    "            yield (counts[0], (counts[1], word))\n",
    "           \n",
    "                    \n",
    "if __name__ == '__main__':\n",
    "    MR32C.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm: `/user/root/HW3/complaints-output3c': No such file or directory\n",
      "No configs found; falling back on auto-configuration\n",
      "No configs specified for hadoop runner\n",
      "Looking for hadoop binary in $PATH...\n",
      "Found hadoop binary: /usr/bin/hadoop\n",
      "Using Hadoop version 2.6.0\n",
      "Looking for Hadoop streaming jar in /home/hadoop/contrib...\n",
      "Looking for Hadoop streaming jar in /usr/lib/hadoop-mapreduce...\n",
      "Found Hadoop streaming jar: /usr/lib/hadoop-mapreduce/hadoop-streaming.jar\n",
      "Creating temp directory /tmp/mrjob3_2C.root.20180225.073917.521777\n",
      "Copying local files to hdfs:///user/root/tmp/mrjob/mrjob3_2C.root.20180225.073917.521777/files/...\n",
      "Running step 1 of 2...\n",
      "  packageJobJar: [] [/usr/jars/hadoop-streaming-2.6.0-cdh5.7.0.jar] /tmp/streamjob6081262181590616641.jar tmpDir=null\n",
      "  Connecting to ResourceManager at /0.0.0.0:8032\n",
      "  Connecting to ResourceManager at /0.0.0.0:8032\n",
      "  Total input paths to process : 1\n",
      "  number of splits:2\n",
      "  Submitting tokens for job: job_1519482645842_0029\n",
      "  Submitted application application_1519482645842_0029\n",
      "  The url to track the job: http://docker.w261:8088/proxy/application_1519482645842_0029/\n",
      "  Running job: job_1519482645842_0029\n",
      "  Job job_1519482645842_0029 running in uber mode : false\n",
      "   map 0% reduce 0%\n",
      "   map 50% reduce 0%\n",
      "   map 100% reduce 0%\n",
      "   map 100% reduce 100%\n",
      "  Job job_1519482645842_0029 completed successfully\n",
      "  Output directory: hdfs:///user/root/tmp/mrjob/mrjob3_2C.root.20180225.073917.521777/step-output/0000\n",
      "Counters: 49\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=50910582\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=4965\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=5100\n",
      "\t\tFILE: Number of bytes written=368474\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=50910924\n",
      "\t\tHDFS: Number of bytes written=4965\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of read operations=9\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tData-local map tasks=2\n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tTotal megabyte-seconds taken by all map tasks=12654592\n",
      "\t\tTotal megabyte-seconds taken by all reduce tasks=2478080\n",
      "\t\tTotal time spent by all map tasks (ms)=12358\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=12358\n",
      "\t\tTotal time spent by all reduce tasks (ms)=2420\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=2420\n",
      "\t\tTotal vcore-seconds taken by all map tasks=12358\n",
      "\t\tTotal vcore-seconds taken by all reduce tasks=2420\n",
      "\tMap-Reduce Framework\n",
      "\t\tCPU time spent (ms)=8650\n",
      "\t\tCombine input records=1960966\n",
      "\t\tCombine output records=315\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tGC time elapsed (ms)=134\n",
      "\t\tInput split bytes=342\n",
      "\t\tMap input records=312913\n",
      "\t\tMap output bytes=23979754\n",
      "\t\tMap output materialized bytes=5106\n",
      "\t\tMap output records=1960966\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tPhysical memory (bytes) snapshot=984793088\n",
      "\t\tReduce input groups=170\n",
      "\t\tReduce input records=315\n",
      "\t\tReduce output records=170\n",
      "\t\tReduce shuffle bytes=5106\n",
      "\t\tShuffled Maps =2\n",
      "\t\tSpilled Records=630\n",
      "\t\tTotal committed heap usage (bytes)=1617952768\n",
      "\t\tVirtual memory (bytes) snapshot=4241928192\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "Running step 2 of 2...\n",
      "  packageJobJar: [] [/usr/jars/hadoop-streaming-2.6.0-cdh5.7.0.jar] /tmp/streamjob2856891791238800695.jar tmpDir=null\n",
      "  Connecting to ResourceManager at /0.0.0.0:8032\n",
      "  Connecting to ResourceManager at /0.0.0.0:8032\n",
      "  Total input paths to process : 1\n",
      "  number of splits:2\n",
      "  Submitting tokens for job: job_1519482645842_0030\n",
      "  Submitted application application_1519482645842_0030\n",
      "  The url to track the job: http://docker.w261:8088/proxy/application_1519482645842_0030/\n",
      "  Running job: job_1519482645842_0030\n",
      "  Job job_1519482645842_0030 running in uber mode : false\n",
      "   map 0% reduce 0%\n",
      "   map 50% reduce 0%\n",
      "   map 100% reduce 0%\n",
      "   map 100% reduce 100%\n",
      "  Job job_1519482645842_0030 completed successfully\n",
      "  Output directory: hdfs:///user/root/tmp/mrjob/mrjob3_2C.root.20180225.073917.521777/output\n",
      "Counters: 49\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=7448\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=5117\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=5461\n",
      "\t\tFILE: Number of bytes written=365917\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=7786\n",
      "\t\tHDFS: Number of bytes written=5117\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of read operations=9\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tData-local map tasks=2\n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tTotal megabyte-seconds taken by all map tasks=4700160\n",
      "\t\tTotal megabyte-seconds taken by all reduce tasks=2342912\n",
      "\t\tTotal time spent by all map tasks (ms)=4590\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=4590\n",
      "\t\tTotal time spent by all reduce tasks (ms)=2288\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=2288\n",
      "\t\tTotal vcore-seconds taken by all map tasks=4590\n",
      "\t\tTotal vcore-seconds taken by all reduce tasks=2288\n",
      "\tMap-Reduce Framework\n",
      "\t\tCPU time spent (ms)=1470\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tGC time elapsed (ms)=41\n",
      "\t\tInput split bytes=338\n",
      "\t\tMap input records=170\n",
      "\t\tMap output bytes=5117\n",
      "\t\tMap output materialized bytes=5467\n",
      "\t\tMap output records=169\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tPhysical memory (bytes) snapshot=917184512\n",
      "\t\tReduce input groups=169\n",
      "\t\tReduce input records=169\n",
      "\t\tReduce output records=169\n",
      "\t\tReduce shuffle bytes=5467\n",
      "\t\tShuffled Maps =2\n",
      "\t\tSpilled Records=338\n",
      "\t\tTotal committed heap usage (bytes)=1513095168\n",
      "\t\tVirtual memory (bytes) snapshot=4234387456\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "Streaming final output from hdfs:///user/root/tmp/mrjob/mrjob3_2C.root.20180225.073917.521777/output...\n",
      "119630\t[0.1220112944,\"loan\"]\t\n",
      "70487\t[0.0718900787,\"modification\"]\t\n",
      "55251\t[0.0563507985,\"credit\"]\t\n",
      "36767\t[0.0374988654,\"servicing\"]\t\n",
      "34903\t[0.0355977615,\"report\"]\t\n",
      "29133\t[0.0297129068,\"incorrect\"]\t\n",
      "29069\t[0.0296476329,\"information\"]\t\n",
      "29069\t[0.0296476329,\"on\"]\t\n",
      "22533\t[0.0229815305,\"or\"]\t\n",
      "20681\t[0.0210926656,\"account\"]\t\n",
      "19309\t[0.0196933552,\"debt\"]\t\n",
      "16448\t[0.0167754056,\"and\"]\t\n",
      "16205\t[0.0165275686,\"opening\"]\t\n",
      "12545\t[0.0127947144,\"health\"]\t\n",
      "12545\t[0.0127947144,\"club\"]\t\n",
      "12353\t[0.0125988926,\"not\"]\t\n",
      "11848\t[0.0120838403,\"owed\"]\t\n",
      "11848\t[0.0120838403,\"cont'd\"]\t\n",
      "11848\t[0.0120838403,\"attempts\"]\t\n",
      "11848\t[0.0120838403,\"collect\"]\t\n",
      "10885\t[0.0111016713,\"of\"]\t\n",
      "10731\t[0.0109446059,\"my\"]\t\n",
      "10555\t[0.0107651025,\"withdrawals\"]\t\n",
      "10555\t[0.0107651025,\"deposits\"]\t\n",
      "9484\t[0.0096727837,\"problems\"]\t\n",
      "8868\t[0.0090445219,\"application\"]\t\n",
      "8401\t[0.0085682261,\"to\"]\t\n",
      "8178\t[0.0083407871,\"unable\"]\t\n",
      "8158\t[0.008320389,\"billing\"]\t\n",
      "7886\t[0.0080429747,\"other\"]\t\n",
      "6938\t[0.0070761043,\"disputes\"]\t\n",
      "6920\t[0.007057746,\"communication\"]\t\n",
      "6920\t[0.007057746,\"tactics\"]\t\n",
      "6559\t[0.0066895601,\"reporting\"]\t\n",
      "6337\t[0.0064631411,\"lease\"]\t\n",
      "6248\t[0.0063723695,\"the\"]\t\n",
      "5663\t[0.0057757248,\"low\"]\t\n",
      "5663\t[0.0057757248,\"being\"]\t\n",
      "5663\t[0.0057757248,\"funds\"]\t\n",
      "5663\t[0.0057757248,\"by\"]\t\n",
      "5663\t[0.0057757248,\"caused\"]\t\n",
      "5505\t[0.0056145798,\"process\"]\t\n",
      "5214\t[0.0053177873,\"verification\"]\t\n",
      "5214\t[0.0053177873,\"disclosure\"]\t\n",
      "5006\t[0.0051056469,\"managing\"]\t\n",
      "4858\t[0.0049547009,\"company's\"]\t\n",
      "4858\t[0.0049547009,\"investigation\"]\t\n",
      "4729\t[0.0048231331,\"identity\"]\t\n",
      "4405\t[0.0044926837,\"card\"]\t\n",
      "4357\t[0.0044437282,\"get\"]\t\n",
      "4357\t[0.0044437282,\"score\"]\t\n",
      "4350\t[0.0044365889,\"settlement\"]\t\n",
      "4350\t[0.0044365889,\"costs\"]\t\n",
      "4309\t[0.0043947728,\"improper\"]\t\n",
      "4238\t[0.0043223595,\"interest\"]\t\n",
      "4139\t[0.0042213888,\"protection\"]\t\n",
      "4095\t[0.004176513,\"when\"]\t\n",
      "3844\t[0.0039205167,\"your\"]\t\n",
      "3844\t[0.0039205167,\"repaying\"]\t\n",
      "3842\t[0.0039184769,\"fraud\"]\t\n",
      "3821\t[0.0038970589,\"are\"]\t\n",
      "3821\t[0.0038970589,\"pay\"]\t\n",
      "3821\t[0.0038970589,\"you\"]\t\n",
      "3747\t[0.0038215859,\"taking\"]\t\n",
      "3503\t[0.003572729,\"a\"]\t\n",
      "3431\t[0.0034992958,\"apr\"]\t\n",
      "3431\t[0.0034992958,\"rate\"]\t\n",
      "3276\t[0.0033412104,\"theft\"]\t\n",
      "3276\t[0.0033412104,\"embezzlement\"]\t\n",
      "3226\t[0.0032902151,\"receiving\"]\t\n",
      "3226\t[0.0032902151,\"payments\"]\t\n",
      "3226\t[0.0032902151,\"making\"]\t\n",
      "3198\t[0.0032616578,\"fee\"]\t\n",
      "3053\t[0.0031137715,\"contact\"]\t\n",
      "2896\t[0.0029536463,\"info\"]\t\n",
      "2832\t[0.0028883724,\"sharing\"]\t\n",
      "2795\t[0.0028506359,\"closing\"]\t\n",
      "2795\t[0.0028506359,\"cancelling\"]\t\n",
      "2774\t[0.0028292178,\"decision\"]\t\n",
      "2774\t[0.0028292178,\"underwriting\"]\t\n",
      "2734\t[0.0027884216,\"customer\"]\t\n",
      "2508\t[0.002557923,\"false\"]\t\n",
      "2508\t[0.002557923,\"statements\"]\t\n",
      "2508\t[0.002557923,\"representation\"]\t\n",
      "2505\t[0.0025548633,\"threatening\"]\t\n",
      "2505\t[0.0025548633,\"an\"]\t\n",
      "2505\t[0.0025548633,\"action\"]\t\n",
      "2505\t[0.0025548633,\"illegal\"]\t\n",
      "2422\t[0.0024702111,\"atm\"]\t\n",
      "2422\t[0.0024702111,\"debit\"]\t\n",
      "2422\t[0.0024702111,\"using\"]\t\n",
      "2165\t[0.0022080954,\"lender\"]\t\n",
      "1999\t[0.0020387911,\"can't\"]\t\n",
      "1944\t[0.0019826963,\"dealing\"]\t\n",
      "1944\t[0.0019826963,\"with\"]\t\n",
      "1944\t[0.0019826963,\"servicer\"]\t\n",
      "1907\t[0.0019449598,\"collection\"]\t\n",
      "1797\t[0.0018327702,\"late\"]\t\n",
      "1732\t[0.0017664763,\"line\"]\t\n",
      "1647\t[0.0016797844,\"repay\"]\t\n",
      "1518\t[0.0015482165,\"service\"]\t\n",
      "1490\t[0.0015196592,\"determination\"]\t\n",
      "1485\t[0.0015145597,\"transaction\"]\t\n",
      "1477\t[0.0015064004,\"use\"]\t\n",
      "1453\t[0.0014819227,\"monitoring\"]\t\n",
      "1367\t[0.0013942108,\"relations\"]\t\n",
      "1242\t[0.0012667226,\"out\"]\t\n",
      "1220\t[0.0012442847,\"statement\"]\t\n",
      "1193\t[0.0012167473,\"marketing\"]\t\n",
      "1193\t[0.0012167473,\"advertising\"]\t\n",
      "1155\t[0.0011779908,\"payoff\"]\t\n",
      "1149\t[0.0011718714,\"decrease\"]\t\n",
      "1149\t[0.0011718714,\"increase\"]\t\n",
      "1099\t[0.0011208761,\"issue\"]\t\n",
      "1061\t[0.0010821197,\"delinquent\"]\t\n",
      "1003\t[0.0010229652,\"practices\"]\t\n",
      "1002\t[0.0010219453,\"rewards\"]\t\n",
      "976\t[0.0009954278,\"charged\"]\t\n",
      "929\t[0.0009474922,\"for\"]\t\n",
      "925\t[0.0009434126,\"i\"]\t\n",
      "925\t[0.0009434126,\"didn't\"]\t\n",
      "904\t[0.0009219946,\"dispute\"]\t\n",
      "807\t[0.0008230637,\"expect\"]\t\n",
      "807\t[0.0008230637,\"fees\"]\t\n",
      "672\t[0.0006853765,\"shopping\"]\t\n",
      "640\t[0.0006527395,\"issuance\"]\t\n",
      "640\t[0.0006527395,\"unsolicited\"]\t\n",
      "597\t[0.0006088836,\"transfer\"]\t\n",
      "597\t[0.0006088836,\"balance\"]\t\n",
      "566\t[0.0005772665,\"scam\"]\t\n",
      "538\t[0.0005487092,\"issues\"]\t\n",
      "413\t[0.000421221,\"money\"]\t\n",
      "350\t[0.0003569669,\"terms\"]\t\n",
      "350\t[0.0003569669,\"plans\"]\t\n",
      "350\t[0.0003569669,\"workout\"]\t\n",
      "350\t[0.0003569669,\"changes\"]\t\n",
      "350\t[0.0003569669,\"forbearance\"]\t\n",
      "291\t[0.0002967925,\"getting\"]\t\n",
      "274\t[0.0002794541,\"available\"]\t\n",
      "274\t[0.0002794541,\"was\"]\t\n",
      "274\t[0.0002794541,\"promised\"]\t\n",
      "243\t[0.000247837,\"processing\"]\t\n",
      "243\t[0.000247837,\"delay\"]\t\n",
      "240\t[0.0002447773,\"advance\"]\t\n",
      "240\t[0.0002447773,\"cash\"]\t\n",
      "240\t[0.0002447773,\"privacy\"]\t\n",
      "222\t[0.000226419,\"bankruptcy\"]\t\n",
      "216\t[0.0002202996,\"received\"]\t\n",
      "202\t[0.0002060209,\"bank\"]\t\n",
      "169\t[0.000172364,\"wrong\"]\t\n",
      "168\t[0.0001713441,\"arbitration\"]\t\n",
      "163\t[0.0001662446,\"acct\"]\t\n",
      "139\t[0.0001417669,\"applied\"]\t\n",
      "139\t[0.0001417669,\"did\"]\t\n",
      "139\t[0.0001417669,\"sale\"]\t\n",
      "139\t[0.0001417669,\"receive\"]\t\n",
      "131\t[0.0001336076,\"stop\"]\t\n",
      "131\t[0.0001336076,\"charges\"]\t\n",
      "127\t[0.000129528,\"overlimit\"]\t\n",
      "118\t[0.0001203488,\"apply\"]\t\n",
      "98\t[0.0000999507,\"amount\"]\t\n",
      "92\t[0.0000938313,\"credited\"]\t\n",
      "92\t[0.0000938313,\"payment\"]\t\n",
      "75\t[0.0000764929,\"convenience\"]\t\n",
      "75\t[0.0000764929,\"checks\"]\t\n",
      "71\t[0.0000724133,\"amt\"]\t\n",
      "71\t[0.0000724133,\"day\"]\t\n",
      "64\t[0.000065274,\"disclosures\"]\t\n",
      "64\t[0.000065274,\"missing\"]\t\n",
      "Removing HDFS temp directory hdfs:///user/root/tmp/mrjob/mrjob3_2C.root.20180225.073917.521777...\n",
      "Removing temp directory /tmp/mrjob3_2C.root.20180225.073917.521777...\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -rm -r {HDFS_DIR}/complaints-output3c\n",
    "\n",
    "!python mrjob3_2C.py \\\n",
    "    -r hadoop Consumer_Complaints.csv \\\n",
    "    --cmdenv PATH=/opt/anaconda/bin:$PATH \\\n",
    "    ##--output-dir {HDFS_DIR}/complaints-output3c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(119630, [0.1220112944, u'loan'])\n",
      "(70487, [0.0718900787, u'modification'])\n",
      "(55251, [0.0563507985, u'credit'])\n",
      "(36767, [0.037498865400000005, u'servicing'])\n",
      "(34903, [0.0355977615, u'report'])\n",
      "(29133, [0.0297129068, u'incorrect'])\n",
      "(29069, [0.029647632900000002, u'information'])\n",
      "(29069, [0.029647632900000002, u'on'])\n",
      "(22533, [0.0229815305, u'or'])\n",
      "(20681, [0.0210926656, u'account'])\n",
      "(19309, [0.019693355200000002, u'debt'])\n",
      "(16448, [0.0167754056, u'and'])\n",
      "(16205, [0.0165275686, u'opening'])\n",
      "(12545, [0.0127947144, u'health'])\n",
      "(12545, [0.0127947144, u'club'])\n",
      "(12353, [0.0125988926, u'not'])\n",
      "(11848, [0.0120838403, u'owed'])\n",
      "(11848, [0.0120838403, u\"cont'd\"])\n",
      "(11848, [0.0120838403, u'attempts'])\n",
      "(11848, [0.0120838403, u'collect'])\n",
      "(10885, [0.011101671300000001, u'of'])\n",
      "(10731, [0.010944605900000001, u'my'])\n",
      "(10555, [0.0107651025, u'withdrawals'])\n",
      "(10555, [0.0107651025, u'deposits'])\n",
      "(9484, [0.0096727837, u'problems'])\n",
      "(8868, [0.0090445219, u'application'])\n",
      "(8401, [0.0085682261, u'to'])\n",
      "(8178, [0.0083407871, u'unable'])\n",
      "(8158, [0.008320389000000001, u'billing'])\n",
      "(7886, [0.0080429747, u'other'])\n",
      "(6938, [0.0070761043, u'disputes'])\n",
      "(6920, [0.007057746, u'communication'])\n",
      "(6920, [0.007057746, u'tactics'])\n",
      "(6559, [0.0066895601, u'reporting'])\n",
      "(6337, [0.0064631411, u'lease'])\n",
      "(6248, [0.0063723695, u'the'])\n",
      "(5663, [0.005775724800000001, u'low'])\n",
      "(5663, [0.005775724800000001, u'being'])\n",
      "(5663, [0.005775724800000001, u'funds'])\n",
      "(5663, [0.005775724800000001, u'by'])\n",
      "(5663, [0.005775724800000001, u'caused'])\n",
      "(5505, [0.0056145798, u'process'])\n",
      "(5214, [0.005317787300000001, u'verification'])\n",
      "(5214, [0.005317787300000001, u'disclosure'])\n",
      "(5006, [0.0051056469, u'managing'])\n",
      "(4858, [0.0049547009, u\"company's\"])\n",
      "(4858, [0.0049547009, u'investigation'])\n",
      "(4729, [0.0048231331, u'identity'])\n",
      "(4405, [0.004492683700000001, u'card'])\n",
      "(4357, [0.0044437282, u'get'])\n",
      "(4357, [0.0044437282, u'score'])\n",
      "(4350, [0.0044365889, u'settlement'])\n",
      "(4350, [0.0044365889, u'costs'])\n",
      "(4309, [0.0043947728, u'improper'])\n",
      "(4238, [0.0043223595, u'interest'])\n",
      "(4139, [0.0042213888, u'protection'])\n",
      "(4095, [0.004176513000000001, u'when'])\n",
      "(3844, [0.0039205167, u'your'])\n",
      "(3844, [0.0039205167, u'repaying'])\n",
      "(3842, [0.0039184769000000005, u'fraud'])\n",
      "(3821, [0.0038970589, u'are'])\n",
      "(3821, [0.0038970589, u'pay'])\n",
      "(3821, [0.0038970589, u'you'])\n",
      "(3747, [0.0038215859, u'taking'])\n",
      "(3503, [0.003572729, u'a'])\n",
      "(3431, [0.0034992958, u'apr'])\n",
      "(3431, [0.0034992958, u'rate'])\n",
      "(3276, [0.0033412104, u'theft'])\n",
      "(3276, [0.0033412104, u'embezzlement'])\n",
      "(3226, [0.0032902151, u'receiving'])\n",
      "(3226, [0.0032902151, u'payments'])\n",
      "(3226, [0.0032902151, u'making'])\n",
      "(3198, [0.0032616578, u'fee'])\n",
      "(3053, [0.0031137715, u'contact'])\n",
      "(2896, [0.0029536463, u'info'])\n",
      "(2832, [0.0028883724000000003, u'sharing'])\n",
      "(2795, [0.0028506359, u'closing'])\n",
      "(2795, [0.0028506359, u'cancelling'])\n",
      "(2774, [0.0028292178, u'decision'])\n",
      "(2774, [0.0028292178, u'underwriting'])\n",
      "(2734, [0.0027884216, u'customer'])\n",
      "(2508, [0.002557923, u'false'])\n",
      "(2508, [0.002557923, u'statements'])\n",
      "(2508, [0.002557923, u'representation'])\n",
      "(2505, [0.0025548633, u'threatening'])\n",
      "(2505, [0.0025548633, u'an'])\n",
      "(2505, [0.0025548633, u'action'])\n",
      "(2505, [0.0025548633, u'illegal'])\n",
      "(2422, [0.0024702111, u'atm'])\n",
      "(2422, [0.0024702111, u'debit'])\n",
      "(2422, [0.0024702111, u'using'])\n",
      "(2165, [0.0022080954, u'lender'])\n",
      "(1999, [0.0020387911, u\"can't\"])\n",
      "(1944, [0.0019826963, u'dealing'])\n",
      "(1944, [0.0019826963, u'with'])\n",
      "(1944, [0.0019826963, u'servicer'])\n",
      "(1907, [0.0019449598, u'collection'])\n",
      "(1797, [0.0018327702000000001, u'late'])\n",
      "(1732, [0.0017664763, u'line'])\n",
      "(1647, [0.0016797844, u'repay'])\n",
      "(1518, [0.0015482165, u'service'])\n",
      "(1490, [0.0015196592, u'determination'])\n",
      "(1485, [0.0015145597, u'transaction'])\n",
      "(1477, [0.0015064004, u'use'])\n",
      "(1453, [0.0014819227000000001, u'monitoring'])\n",
      "(1367, [0.0013942108, u'relations'])\n",
      "(1242, [0.0012667226, u'out'])\n",
      "(1220, [0.0012442847, u'statement'])\n",
      "(1193, [0.0012167473000000001, u'marketing'])\n",
      "(1193, [0.0012167473000000001, u'advertising'])\n",
      "(1155, [0.0011779908000000001, u'payoff'])\n",
      "(1149, [0.0011718714, u'decrease'])\n",
      "(1149, [0.0011718714, u'increase'])\n",
      "(1099, [0.0011208761, u'issue'])\n",
      "(1061, [0.0010821197000000001, u'delinquent'])\n",
      "(1003, [0.0010229652, u'practices'])\n",
      "(1002, [0.0010219453000000001, u'rewards'])\n",
      "(976, [0.0009954278, u'charged'])\n",
      "(929, [0.0009474922, u'for'])\n",
      "(925, [0.0009434126, u'i'])\n",
      "(925, [0.0009434126, u\"didn't\"])\n",
      "(904, [0.0009219946000000001, u'dispute'])\n",
      "(807, [0.0008230637, u'expect'])\n",
      "(807, [0.0008230637, u'fees'])\n",
      "(672, [0.0006853765, u'shopping'])\n",
      "(640, [0.0006527395, u'issuance'])\n",
      "(640, [0.0006527395, u'unsolicited'])\n",
      "(597, [0.0006088836, u'transfer'])\n",
      "(597, [0.0006088836, u'balance'])\n",
      "(566, [0.0005772665000000001, u'scam'])\n",
      "(538, [0.0005487092, u'issues'])\n",
      "(413, [0.00042122100000000005, u'money'])\n",
      "(350, [0.0003569669, u'terms'])\n",
      "(350, [0.0003569669, u'plans'])\n",
      "(350, [0.0003569669, u'workout'])\n",
      "(350, [0.0003569669, u'changes'])\n",
      "(350, [0.0003569669, u'forbearance'])\n",
      "(291, [0.0002967925, u'getting'])\n",
      "(274, [0.00027945410000000003, u'available'])\n",
      "(274, [0.00027945410000000003, u'was'])\n",
      "(274, [0.00027945410000000003, u'promised'])\n",
      "(243, [0.000247837, u'processing'])\n",
      "(243, [0.000247837, u'delay'])\n",
      "(240, [0.0002447773, u'advance'])\n",
      "(240, [0.0002447773, u'cash'])\n",
      "(240, [0.0002447773, u'privacy'])\n",
      "(222, [0.000226419, u'bankruptcy'])\n",
      "(216, [0.00022029960000000001, u'received'])\n",
      "(202, [0.00020602090000000002, u'bank'])\n",
      "(169, [0.00017236400000000002, u'wrong'])\n",
      "(168, [0.0001713441, u'arbitration'])\n",
      "(163, [0.0001662446, u'acct'])\n",
      "(139, [0.0001417669, u'applied'])\n",
      "(139, [0.0001417669, u'did'])\n",
      "(139, [0.0001417669, u'sale'])\n",
      "(139, [0.0001417669, u'receive'])\n",
      "(131, [0.0001336076, u'stop'])\n",
      "(131, [0.0001336076, u'charges'])\n",
      "(127, [0.000129528, u'overlimit'])\n",
      "(118, [0.00012034880000000001, u'apply'])\n",
      "(98, [9.99507e-05, u'amount'])\n",
      "(92, [9.38313e-05, u'credited'])\n",
      "(92, [9.38313e-05, u'payment'])\n",
      "(75, [7.649290000000001e-05, u'convenience'])\n",
      "(75, [7.649290000000001e-05, u'checks'])\n",
      "(71, [7.241330000000001e-05, u'amt'])\n",
      "(71, [7.241330000000001e-05, u'day'])\n",
      "(64, [6.5274e-05, u'disclosures'])\n",
      "(64, [6.5274e-05, u'missing'])\n"
     ]
    }
   ],
   "source": [
    "from mrjob3_2C import MR32C\n",
    "mr_job = MR32C(args=['-r',\n",
    "                    'hadoop',\n",
    "                    'Consumer_Complaints.csv',\n",
    "                    '--cmdenv',\n",
    "                    'PATH=/opt/anaconda/bin:$PATH', \n",
    "                      ])\n",
    "with mr_job.make_runner() as runner: \n",
    "    runner.run()\n",
    "    # stream_output: get access of the output \n",
    "    for line in runner.stream_output():\n",
    "        print mr_job.parse_output_line(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"3.2.1\"></a>\n",
    "### 3.2.1  \n",
    "Using **2 reducers**: What are the top **50 most frequent terms** in your word count analysis? \n",
    "\n",
    "Present the top 50 terms and their frequency and their relative frequency. Present the top 50 terms and their frequency and their relative frequency. If there are ties please sort the tokens in alphanumeric/string order. Present bottom 10 tokens (least frequent items). Please **use a combiner.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### START STUDENT CODE HW321 (INSERT CELLS BELOW AS NEEDED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "119630\t[0.1220112944,\"loan\"]\t\n",
      "70487\t[0.0718900787,\"modification\"]\t\n",
      "55251\t[0.0563507985,\"credit\"]\t\n",
      "36767\t[0.0374988654,\"servicing\"]\t\n",
      "34903\t[0.0355977615,\"report\"]\t\n",
      "29133\t[0.0297129068,\"incorrect\"]\t\n",
      "29069\t[0.0296476329,\"information\"]\t\n",
      "29069\t[0.0296476329,\"on\"]\t\n",
      "22533\t[0.0229815305,\"or\"]\t\n",
      "20681\t[0.0210926656,\"account\"]\t\n"
     ]
    }
   ],
   "source": [
    "# 3.2.C OUTPUT/ANSWER\n",
    "!hdfs dfs -cat {HDFS_DIR}/complaints-output3c/part-0000* | head -n 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXsAAAD8CAYAAACW/ATfAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4wLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvpW3flQAAEShJREFUeJzt3W2MXNV9x/Hv1htDIE1tmIKMbdWm\nMrTkoYUuyIQ2ojjhqQhTCSSjqGyBympCUxKIwJQXqC8iQZ5IUVISFxxMRXkoIbGFoJQSKlQJTJaH\nYPPg2JhiLzbYIwJJQU3i8O+Lc1aerHbs2bl3dnd6vh/pau49c2bOuXt2f3P23jszAxGBJOn/t9+Y\n7g5IknrPsJekAhj2klQAw16SCmDYS1IBDHtJKoBhL0kF6CTs1wC7gU3jyj8LbAaeB77UUn4NsDXf\nd0YNfZQkVTTYQZ3bgG8At7eU/SmwHPgo8HPgiFx+HLAC+BBwFPAfwDHAr+rpriSpG52E/WPAonFl\nnwauJwU9pJk/pBeAu3L5K6QZ/knA4/trYM+ePfHqq6921mNJEgBDQ0NN4Lc7qdtJ2E/kGOBPgC8C\n/wt8AfghMB94oqXeaC6byMq88M4773DiiSd22RVJKlNEdDxL7jbsB4G5wFLgROAe4GhgYKL+tHmO\n1Xmh2Wz6AT2S1EPdXo0zCtxHCvIngfeARi5f2FJvAbCzSgclSdV1G/bfB07L68cAs4EmsJ50gvYg\nYDGwhPRiIEmaRp0cxrkTOJV9M/frSJdjriFdjvkLYJg0y3+edEjnBWAvcBleiSNJ025gJnye/cjI\nSHiCVpImJyKeAoY6qes7aCWpAIa9JBXAsJekAhj2klSAbt9UNWN8deN+P4mhp678yMnT1rYkTYYz\ne0kqgGEvSQUw7CWpAIa9JBXAsJekAhj2klQAw16SCmDYS1IBDHtJKoBhL0kFMOwlqQCGvSQVwLCX\npAJ0EvZrgN2k75sd7wuk755t5O0B4CZgK/AccEINfZQkVdRJ2N8GnDlB+ULgk8D2lrKzgCV5WQnc\nXLF/kqQadBL2jwFvTlB+I3AVaWY/Zjlwey57ApgDzKvYR0lSRd0esz8XeA340bjy+cCOlu3RXDaR\nlcAIMNJoNNpUkSTVoZtvqjoEuBY4fYL7BiYoiwnKAFbnhWaz2a6OJKkG3YT97wKL2TerXwA8DZxE\nmskvbKm7ANhZpYOSpOq6OYyzETgCWJSXUdJVN68D64GLSDP8pcDbwK4a+ilJqqCTsL8TeBw4lhTs\nl+6n7gPANtKll/8EfKZqByVJ1XVyGOfCA9y/qGU9gMu67o0kqSd8B60kFcCwl6QCGPaSVADDXpIK\nYNhLUgEMe0kqgGEvSQUw7CWpAIa9JBXAsJekAhj2klQAw16SCmDYS1IBDHtJKoBhL0kFMOwlqQCG\nvSQVwLCXpAJ0EvZrgN3AppayLwMvAc8B3wPmtNx3Dek7aDcDZ9TTTUlSFZ2E/W3AmePKHgY+DHwU\n+DEp4AGOA1YAH8qP+UdgVh0dlSR1r5Owfwx4c1zZvwN78/oTwIK8vhy4C/g58Apphn9S9W5Kkqqo\n45j9JcCDeX0+sKPlvtFcNpGVwAgw0mg0auiGJKmdwYqPv5Y0w78jbw9MUCfaPHZ1Xmg2m+3qSJJq\nUCXsh4FzgGXsC/RRYGFLnQXAzgptSJJq0O1hnDOBq4FzgXdbyteTTtAeBCwGlgBPVumgJKm6Tmb2\ndwKnAg3SzP060tU3B5GuyoF0kvavgeeBe4AXSId3LgN+VWuPJUmT1knYXzhB2a37qf/FvEiSZgjf\nQStJBTDsJakAhr0kFcCwl6QCGPaSVADDXpIKYNhLUgEMe0kqgGEvSQUw7CWpAIa9JBXAsJekAhj2\nklQAw16SCmDYS1IBDHtJKoBhL0kFMOwlqQCdhP0aYDewqaXsMNL3z27Jt3Nz+QBwE7AVeA44obae\nSpK61knY3wacOa5sFfAIsCTfrsrlZ+WyJcBK4OZaeilJqqSTsH8MeHNc2XJgbV5fC5zXUn47EMAT\nwBxgXvVuSpKq6PaY/ZHArry+Czgir88HdrTUG81lE1kJjAAjjUajy25IkjoxWPPzDUxQFm3qrs4L\nzWazXR1JUg26ndm/wb7DM/NIJ3AhzeQXttRbAOzssg1JUk26Dfv1wHBeHwbWtZRfRJrhLwXeZt/h\nHknSNOnkMM6dwKlAgzRzvw64HrgHuBTYDlyQ6z4AnE269PJd4OJ6uytJ6kYnYX9hm/JlE5QFcFn3\n3ZEk9YLvoJWkAhj2klQAw16SCmDYS1IBDHtJKoBhL0kFMOwlqQCGvSQVwLCXpAIY9pJUAMNekgpg\n2EtSAQx7SSqAYS9JBTDsJakAhr0kFcCwl6QCGPaSVICqYf954HlgE+m7ag8GFgMbgC3A3cDsim1I\nkiqqEvbzgb8FhoAPA7OAFcANwI3AEuAnpC8llyRNo6oz+0Hg/fn2EGAXcBpwb75/LXBexTYkSRVV\nCfvXgK8A20kh/zbwFPAWsDfXGSX9ByBJmkZVwn4usJx0jP4o4FDgrAnqRZvHrwRGgJFGo1GhG5Kk\nAxms8NhPAK8Ae/L2fcDHgDn5efcCC4CdbR6/Oi80m812LwiSpBpUmdlvB5aSjtUPAMuAF4BHgfNz\nnWFgXZUOSpKqqxL2G0gnYp8GNubnWg1cDVwBbAUOB26t2EdJUkVVDuMAXJeXVtuAkyo+rySpRr6D\nVpIKYNhLUgEMe0kqgGEvSQUw7CWpAIa9JBXAsJekAhj2klQAw16SCmDYS1IBDHtJKoBhL0kFMOwl\nqQCGvSQVwLCXpAIY9pJUAMNekgpg2EtSAaqG/RzS99C+BLwInAwcBjwMbMm3cyu2IUmqqGrY/wPw\nb8DvAX9ACvxVwCPAkny7qmIbkqSKqoT9B4GPA7fm7V8AbwHLgbW5bC1wXoU2JEk1qBL2RwN7gO8A\nzwC3AIcCRwK7cp1dwBFVOihJqq5K2A8CJwA3A8cD7zC5QzYrgRFgpNFoVOiGJOlAqoT9aF425O17\nSeH/BjAvl80Ddrd5/GpgCBhqNpsVuiFJOpAqYf86sAM4Nm8vA14A1gPDuWwYWFehDUlSDQYrPv6z\nwB3AbGAbcDHpBeQe4FJgO3BBxTYkSRVVDftnSYdixltW8XklSTXyHbSSVADDXpIKYNhLUgEMe0kq\ngGEvSQUw7CWpAIa9JBXAsJekAhj2klQAw16SCmDYS1IBDHtJKoBhL0kFMOwlqQCGvSQVwLCXpAIY\n9pJUAMNekgpQR9jPAp4B7s/bi4ENwBbgbtL300qSplEdYX858GLL9g3AjcAS4CekLx6XJE2jqmG/\nAPgz4Ja8PQCcBtybt9cC51VsQ5JUUdWw/zpwFfBe3j4ceAvYm7dHgfkV25AkVVQl7M8BdgNPtZQN\nTFAv2jx+JTACjDQajQrdkCQdyGCFx54CnAucDRwMfJA005+Tn3cv6TDPzjaPX50Xms1muxcESVIN\nqszsryGF+SJgBfAD4FPAo8D5uc4wsK5CG5KkGvTiOvurgSuAraRj+Lf2oA1J0iRUOYzT6j/zArAN\nOKmm55Uk1cB30EpSAQx7SSqAYS9JBTDsJakAhr0kFcCwl6QCGPaSVADDXpIKYNhLUgEMe0kqgGEv\nSQUw7CWpAIa9JBXAsJekAhj2klQAw16SCmDYS1IBDHtJKkCVsF9I+nLxF4Hngctz+WHAw8CWfDu3\nSgclSdVVCfu9wJXA7wNLgcuA44BVwCPAkny7qmIfJUkVVQn7XcDTef1npBn+fGA5sDaXrwXOq9CG\nJKkGdR2zXwQcD2wAjiS9EJBvj6ipDUlSlwZreI4PAN8FPgf8dBKPW5kXGo1GDd2QJLVTdWb/PlLQ\n3wHcl8veAObl9XnA7jaPXQ0MAUPNZrNiNyRJ+1Ml7AeAW0nH6r/WUr4eGM7rw8C6Cm1IkmpQ5TDO\nKcBfABuBZ3PZ3wHXA/cAlwLbgQuqdFCSVF2VsP8v0ux+IssqPK8kqWa+g1aSCmDYS1IBDHtJKoBh\nL0kFMOwlqQCGvSQVwLCXpAIY9pJUAMNekgpg2EtSAQx7SSpAHZ9nX6yvbnx8Wtq98iMnT0u7kvqX\nM3tJKoBhL0kFMOwlqQCGvSQVwBO0fWi6TgyDJ4elfuXMXpIK0MuwPxPYDGwFVvWwHUnSAfQq7GcB\n3wTOAo4DLsy3kqRp0Ktj9ieRZvTb8vZdwHLghR61pykynecLSuP5kalTwnmwXs3s5wM7WrZHc5kk\naRr0amY/MEFZjNtemReGhob+JyI2d9lWA2h2+diZxn2ZmaZlX66I8X8ytXBcZpg8zt3uy+90WrFX\nYT8KLGzZXgDsHFdndV6qGgGGaniemcB9mZncl5nJfZmEXh3G+SGwBFgMzAZWAOt71JYk6QB6NbPf\nC/wN8BDpypw1wPM9akuSdAC9fAftA3nptToOBc0U7svM5L7MTO7LJAxEb04CSZJmED8uQZIK0O9h\nPxM/kmEh8CjwIuk8xeW5/DDgYWBLvp2byweAm0j78BxwQstzDef6W/L6mD8CNubH3MTEl7rWaRbw\nDHB/3l4MbMj9upt0Eh7goLy9Nd+/qOU5rsnlm4EzWsqncgznAPcCL5HG52T6d1w+T/r92gTcCRxM\n/4zLGmB37vuYqRiHdm3UvS9fJv2OPQd8j/R7N2ayP+9uxnRiEdGvy6yIeDkijo6I2RHxo4g4bgb0\na15EnJDXfzMifpz79aWIWJXLV0XEDXn97Ih4MCIGImJpRGzI5YdFxLZ8Ozevz833PRkRJ+fHPBgR\nZ/V4n66IiH+JiPvz9j0RsSKvfysiPp3XP5O3yfffndePy+NzUEQszuM2axrGcG1E/FVenx0Rc/p0\nXOZHxCsR8f6W8fjLPhqXj0f6G9nUUjYV49Cujbr35fSIGMzrN7S0083Pe7Jj2nbp1R/VVCwnR8RD\nLdvX5GW6+zV+WRcRn4yIzZFeCMi3m/P6tyPiwpb6Y/UuzPcxrt68iHippXx8vbqXBRHxSEScFins\nByKiGft+mVvH4aG8Tb6/meuPH5uxelM5hh+MFJAD48r7cVzmR8SOSEE3mMfljD4bl0Xx6wE5FePQ\nro2696V1+fOIuKPNz/FAP+9u/tba9rOfD+P0w0cyLAKOJ/2bdSSwK5fvAo7I6+32Y3/loxOU98rX\ngauA9/L24cBbpMtrx7ff2ue9wNu5/mT3sReOBvYA3yEdkroFOJT+HJfXgK8A20l9fht4iv4clzFT\nMQ7t2uilS4AH8/pk96Wbv7W2+jnsO/lIhun0AeC7wOeAn+6nXrv9mGx5L5xDOh75VEvZ/tqfyfsy\nSDreezPpBfgd9n8seibvy1zSBwsuBo4ivWidtZ/2Z/K+HEg/9/1aUhDfkbfr3JdJ72c/h30nH8kw\nXd5HCvo7gPty2RvAvLw+jxSi0H4/9le+YILyXjgFOBf4b9Inl55GmunPYd97NFrbb+3zIPBbwJtM\nfh97YTQvG/L2vaTw78dx+QTwCuk/lV+Sfsc+Rn+Oy5ipGId2bfTCMGmy9Cn2hfBk96XJ5Me0rX4O\n+5n6kQwDwK2kqz2+1lK+nn1XDAwD61rKL8qPW0r6d2wX6d3Hp5NmcXPz+kP5vp/lugP5sWPPVbdr\nSL9gi0g/3x+QfnkfBc5vsy9j+3h+rh+5fAXpCoLFpHF7kqkdw9dJ//Yem7eXkT5yux/HZXtu55Dc\n1ti+9OO4jJmKcWjXRt3OBK4mTZTebSmf7M87mPyYtlfzSZepXs6OdLXLyxFx7QzoDxHxx5E8FxHP\n5uXsiDg80onOLfn2sFx/ICK+mfdhY0QMtTzXJRGxNS8Xt5QPRToh9HJEfONAJ2ZqWk6NfVfjHB3p\nioetEfGvka4uICIOzttb8/1Htzz+2tzfzfHrV6lM5Rj+YUSM5LH5fqSrN/p1XP4+0onITRHxz3kM\n+mVc7oyIXRHxy4gYjYhLp2gc2rVR975sjXQCfezv/1st9Sf78+5mTCdcfAetJBWgnw/jSJI6ZNhL\nUgEMe0kqgGEvSQUw7CWpAIa9JBXAsJekAhj2klSA/wNyHD2KvPLjIwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f3c8fbb1c50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5801.67455621\n",
      "2505.0\n",
      "119630\n"
     ]
    }
   ],
   "source": [
    "#import subprocess\n",
    "p = subprocess.Popen(['hdfs', 'dfs','-cat', '/user/root/HW3/complaints-output3c/part-00000'],\n",
    "                    stdout=subprocess.PIPE)\n",
    "counts = []\n",
    "for line in p.stdout.readlines():\n",
    "    line = line.strip()\n",
    "    cols = line.split('\\t')\n",
    "    counts.append(int(cols[0]))\n",
    "    \n",
    "plt.hist(counts)\n",
    "plt.show()\n",
    "\n",
    "print np.mean(np.array(counts))\n",
    "print np.median(np.array(counts))\n",
    "print np.max(np.array(counts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper3.2.1.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper3.2.1.py\n",
    "#!/usr/bin/env python\n",
    "# START STUDENT CODE HW32CFREQMAPPER\n",
    "\n",
    "import sys\n",
    "sys.stderr.write(\"reporter:counter:Mapper Counters,Calls,1\\n\")\n",
    "\n",
    "total = 0 \n",
    "counts_median = 2508\n",
    "\n",
    "\n",
    "# read from standard input\n",
    "for line in sys.stdin:\n",
    "    key, value = line.strip().split()\n",
    "    total += int(value)\n",
    "    \n",
    "    partitionkey = 'b'\n",
    "    if int(value) <= counts_median:\n",
    "        partitionkey = 'a'\n",
    "        \n",
    "    print '%s\\t%s\\t%s' % (partitionkey, value, key)\n",
    "\n",
    "print '%s\\t%s\\t%s' % ('a', total, '!total')\n",
    "print '%s\\t%s\\t%s' % ('b', total, '!total')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!chmod a+x mapper3.2.1.py\n",
    "#!hdfs dfs -cat {HDFS_DIR}/complaints-output3c/part-0000* | ./mapper3.2.1.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer3.2.1.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer3.2.1.py\n",
    "#!/usr/bin/env python\n",
    "# START STUDENT CODE HW32CFREQREDUCER\n",
    "\n",
    "from __future__ import division\n",
    "import sys\n",
    "sys.stderr.write(\"reporter:counter:Reducer Counters,Calls,1\\n\")\n",
    "\n",
    "# initialize trackers\n",
    "total = 0\n",
    "\n",
    "# read from standard input\n",
    "for line in sys.stdin:\n",
    "    line = line.strip()\n",
    "    partitionkey, value, key = line.split('\\t')\n",
    "    \n",
    "    if key == '!total':\n",
    "        total += int(value)\n",
    "    else:\n",
    "        print \"%s\\t%s\\t%s\" % (value, int(value)/total, key)\n",
    "    \n",
    "\n",
    "# END STUDENT CODE HW32CFREQREDUCER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted /user/root/HW3/complaints-output5\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -rm -r {HDFS_DIR}/complaints-output5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "packageJobJar: [] [/usr/jars/hadoop-streaming-2.6.0-cdh5.7.0.jar] /tmp/streamjob4983457666485984138.jar tmpDir=null\n",
      "18/02/06 14:38:55 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "18/02/06 14:38:55 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "18/02/06 14:38:56 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "18/02/06 14:38:56 INFO mapreduce.JobSubmitter: number of splits:2\n",
      "18/02/06 14:38:56 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1517924926507_0011\n",
      "18/02/06 14:38:56 INFO impl.YarnClientImpl: Submitted application application_1517924926507_0011\n",
      "18/02/06 14:38:56 INFO mapreduce.Job: The url to track the job: http://docker.w261:8088/proxy/application_1517924926507_0011/\n",
      "18/02/06 14:38:56 INFO mapreduce.Job: Running job: job_1517924926507_0011\n",
      "18/02/06 14:39:02 INFO mapreduce.Job: Job job_1517924926507_0011 running in uber mode : false\n",
      "18/02/06 14:39:02 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "18/02/06 14:39:06 INFO mapreduce.Job:  map 50% reduce 0%\n",
      "18/02/06 14:39:07 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "18/02/06 14:39:11 INFO mapreduce.Job:  map 100% reduce 50%\n",
      "18/02/06 14:39:12 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "18/02/06 14:39:13 INFO mapreduce.Job: Job job_1517924926507_0011 completed successfully\n",
      "18/02/06 14:39:13 INFO mapreduce.Job: Counters: 51\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=3053\n",
      "\t\tFILE: Number of bytes written=477956\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=3414\n",
      "\t\tHDFS: Number of bytes written=5042\n",
      "\t\tHDFS: Number of read operations=12\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=4\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=2\n",
      "\t\tData-local map tasks=2\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=5125\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=5916\n",
      "\t\tTotal time spent by all map tasks (ms)=5125\n",
      "\t\tTotal time spent by all reduce tasks (ms)=5916\n",
      "\t\tTotal vcore-seconds taken by all map tasks=5125\n",
      "\t\tTotal vcore-seconds taken by all reduce tasks=5916\n",
      "\t\tTotal megabyte-seconds taken by all map tasks=5248000\n",
      "\t\tTotal megabyte-seconds taken by all reduce tasks=6057984\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=172\n",
      "\t\tMap output records=176\n",
      "\t\tMap output bytes=2689\n",
      "\t\tMap output materialized bytes=3065\n",
      "\t\tInput split bytes=256\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=176\n",
      "\t\tReduce shuffle bytes=3065\n",
      "\t\tReduce input records=176\n",
      "\t\tReduce output records=172\n",
      "\t\tSpilled Records=352\n",
      "\t\tShuffled Maps =4\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=4\n",
      "\t\tGC time elapsed (ms)=66\n",
      "\t\tCPU time spent (ms)=2740\n",
      "\t\tPhysical memory (bytes) snapshot=1160818688\n",
      "\t\tVirtual memory (bytes) snapshot=5564035072\n",
      "\t\tTotal committed heap usage (bytes)=2011168768\n",
      "\tMapper Counters\n",
      "\t\tCalls=2\n",
      "\tReducer Counters\n",
      "\t\tCalls=2\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=3158\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=5042\n",
      "18/02/06 14:39:13 INFO streaming.StreamJob: Output directory: /user/root/HW3/complaints-output5\n"
     ]
    }
   ],
   "source": [
    "\n",
    "!hadoop jar {JAR_FILE} \\\n",
    "  -D mapreduce.job.output.key.comparator.class=org.apache.hadoop.mapred.lib.KeyFieldBasedComparator \\\n",
    "  -D stream.map.output.field.separator=\"\\t\" \\\n",
    "  -D mapreduce.partition.keypartitioner.options=\"-k1,1\" \\\n",
    "  -D mapreduce.partition.keycomparator.options=\"-k2,2nr -k3,3\" \\\n",
    "  -D mapreduce.job.reduces=2 \\\n",
    "  -files mapper3.2.1.py,reducer3.2.1.py \\\n",
    "  -mapper mapper3.2.1.py \\\n",
    "  -reducer reducer3.2.1.py \\\n",
    "  -input {HDFS_DIR}/complaints-output3c/part-00000 \\\n",
    "  -output {HDFS_DIR}/complaints-output5 \\\n",
    "  -partitioner org.apache.hadoop.mapred.lib.KeyFieldBasedPartitioner \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3 items\n",
      "-rw-r--r--   1 root supergroup          0 2018-02-06 14:39 /user/root/HW3/complaints-output5/_SUCCESS\n",
      "-rw-r--r--   1 root supergroup       2439 2018-02-06 14:39 /user/root/HW3/complaints-output5/part-00000\n",
      "-rw-r--r--   1 root supergroup       2603 2018-02-06 14:39 /user/root/HW3/complaints-output5/part-00001\n",
      "\n",
      "------------Top 50 words-------------\n",
      "\n",
      "Count   Frequency       Word\n",
      "\n",
      "-------------------------------------\n",
      "\n",
      "119630\t0.119616483337\tloan\n",
      "70487\t0.0704790358689\tmodification\n",
      "55251\t0.0552447573424\tcredit\n",
      "36767\t0.0367628457984\tservicing\n",
      "34903\t0.0348990564066\treport\n",
      "29133\t0.029129708343\tincorrect\n",
      "29069\t0.0290657155741\tinformation\n",
      "29069\t0.0290657155741\ton\n",
      "22533\t0.0225304540587\tor\n",
      "20681\t0.020678663311\taccount\n",
      "19309\t0.0193068183295\tdebt\n",
      "16448\t0.016446141586\tand\n",
      "16205\t0.0162031690419\topening\n",
      "12545\t0.0125435825752\tclub\n",
      "12545\t0.0125435825752\thealth\n",
      "12353\t0.0123516042687\tnot\n",
      "11848\t0.0118466613273\tattempts\n",
      "11848\t0.0118466613273\tcollect\n",
      "11848\t0.0118466613273\tcont\n",
      "11848\t0.0118466613273\td\n",
      "11848\t0.0118466613273\towed\n",
      "10885\t0.010883770134\tof\n",
      "10731\t0.010729787534\tmy\n",
      "10555\t0.0105538074198\tdeposits\n",
      "10555\t0.0105538074198\twithdrawals\n",
      "9484\t0.00948292842909\tproblems\n",
      "8868\t0.00886699802922\tapplication\n",
      "8401\t0.00840005079426\tto\n",
      "8178\t0.00817707599041\tunable\n",
      "8158\t0.00815707825016\tbilling\n",
      "7886\t0.00788510898268\tother\n",
      "6938\t0.00693721609458\tdisputes\n",
      "6920\t0.00691921812835\tcommunication\n",
      "6920\t0.00691921812835\ttactics\n",
      "6559\t0.00655825891674\treporting\n",
      "6337\t0.00633628399991\tlease\n",
      "6248\t0.00624729405577\tthe\n",
      "5663\t0.0056623601533\tbeing\n",
      "5663\t0.0056623601533\tby\n",
      "5663\t0.0056623601533\tcaused\n",
      "5663\t0.0056623601533\tfunds\n",
      "5663\t0.0056623601533\tlow\n",
      "5505\t0.00550437800529\tprocess\n",
      "5214\t0.00521341088457\tdisclosure\n",
      "5214\t0.00521341088457\tverification\n",
      "5006\t0.00500543438591\tmanaging\n",
      "4858\t0.00485745110802\tcompany\n",
      "4858\t0.00485745110802\tinvestigation\n",
      "4858\t0.00485745110802\ts\n",
      "4729\t0.00472846568338\tidentity\n",
      "\n",
      "----------Bottom 10 words------------\n",
      "\n",
      "118\t0.000117986667507\tapply\n",
      "98\t9.79889272512e-05\tamount\n",
      "92\t9.19896051746e-05\tcredited\n",
      "92\t9.19896051746e-05\tpayment\n",
      "75\t7.49915259576e-05\tchecks\n",
      "75\t7.49915259576e-05\tconvenience\n",
      "71\t7.09919779065e-05\tamt\n",
      "71\t7.09919779065e-05\tday\n",
      "64\t6.39927688171e-05\tdisclosures\n",
      "64\t6.39927688171e-05\tmissing\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -ls {HDFS_DIR}/complaints-output5\n",
    "print \"\\n------------Top 50 words-------------\\n\"\n",
    "print \"Count   Frequency       Word\"\n",
    "print \"\\n-------------------------------------\\n\"\n",
    "!hdfs dfs -cat {HDFS_DIR}/complaints-output5/part-00000 | head -n 50\n",
    "print \"\\n----------Bottom 10 words------------\\n\"\n",
    "!hdfs dfs -cat {HDFS_DIR}/complaints-output5/part-00001 | tail -n 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### END STUDENT CODE HW321"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mrjob3_2C1.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mrjob3_2C1.py\n",
    "\n",
    "import sys\n",
    "import re\n",
    "import json\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "from mrjob.protocol import RawProtocol\n",
    "from mrjob.protocol import JSONProtocol\n",
    "\n",
    "WORD_RE = re.compile(r\"[\\w']+\")\n",
    "  \n",
    "class MR32C1(MRJob):\n",
    "\n",
    "    SORT_VALUES = True\n",
    "    #INTERNAL_PROTOCOL = RawProtocol\n",
    "    #OUTPUT_PROTOCOL = RawProtocol\n",
    "\n",
    "    def steps(self):\n",
    "        \n",
    "        JOBCONF_STEP1 = {\n",
    "                'mapreduce.job.output.key.comparator.class': 'org.apache.hadoop.mapred.lib.KeyFieldBasedComparator',\n",
    "                'stream.num.map.output.key.fields':2,\n",
    "                'stream.map.output.field.separator':'\\t',\n",
    "                'mapreduce.partition.keycomparator.options': '-k1,1',\n",
    "                }  \n",
    "        \n",
    "        JOBCONF_STEP2 = {\n",
    "                'mapreduce.job.output.key.comparator.class': 'org.apache.hadoop.mapred.lib.KeyFieldBasedComparator',\n",
    "                'stream.num.map.output.key.fields':3,\n",
    "                'stream.map.output.field.separator':'\\t',\n",
    "                'mapreduce.partition.keycomparator.options': '-k1,1',\n",
    "                'mapreduce.job.reduces':2,\n",
    "                'partitioner':'org.apache.hadoop.mapred.lib.KeyFieldBasedPartitioner'    \n",
    "                }  \n",
    "        \n",
    "        \n",
    "        \n",
    "        return [\n",
    "        MRStep(jobconf=JOBCONF_STEP1,\n",
    "               mapper=self.mapper,\n",
    "               combiner=self.combiner,\n",
    "               reducer=self.reducer,\n",
    "              ),\n",
    "        MRStep(jobconf=JOBCONF_STEP2, \n",
    "              mapper=self.mapper2,\n",
    "              #reducer_init=self.reducer2_init,\n",
    "              #reducer=self.reducer2,\n",
    "              ),\n",
    "            \n",
    "            \n",
    "            \n",
    "        ]\n",
    "\n",
    "\n",
    "    def mapper(self, _, line):\n",
    "        issue = line.strip().split(',')[3]\n",
    "        words = WORD_RE.findall(issue.lower())\n",
    "        for word in words:\n",
    "            yield '!Total', 1 \n",
    "            yield word, 1\n",
    "            \n",
    "    def combiner(self, word, counts):\n",
    "        yield word, sum(counts)\n",
    "        \n",
    "    def reducer(self, word, counts):\n",
    "        yield word, sum(counts)\n",
    "        \n",
    "    def mapper2(self, word, counts):\n",
    "        if word == '!Total':\n",
    "            yield 'a', '!Total'+'\\t'+str(counts)\n",
    "            yield 'b', '!Total'+'\\t'+str(counts)\n",
    "        \n",
    "        if int(counts) > 2500:\n",
    "            yield 'a', word+'\\t'+str(counts)\n",
    "        else:\n",
    "            yield 'b', word+'\\t'+str(counts)\n",
    "   \n",
    "    def reducer2_init(self):\n",
    "        self.total = 0\n",
    "\n",
    "    def reducer2(self, key, value):\n",
    "        for v in value:\n",
    "            k, val = v.split()\n",
    "            if k == '!Total':\n",
    "                self.total = int(val)\n",
    "            else:\n",
    "                yield k, (key, int(val), self.total)\n",
    "                \n",
    "                    \n",
    "if __name__ == '__main__':\n",
    "    MR32C1.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm: `{HDFS_DIR}/complaints-output3c1': No such file or directory\n",
      "No configs found; falling back on auto-configuration\n",
      "No configs specified for hadoop runner\n",
      "Looking for hadoop binary in $PATH...\n",
      "Found hadoop binary: /usr/bin/hadoop\n",
      "Using Hadoop version 2.6.0\n",
      "Looking for Hadoop streaming jar in /home/hadoop/contrib...\n",
      "Looking for Hadoop streaming jar in /usr/lib/hadoop-mapreduce...\n",
      "Found Hadoop streaming jar: /usr/lib/hadoop-mapreduce/hadoop-streaming.jar\n",
      "Creating temp directory /tmp/mrjob3_2C1.root.20180225.150132.004055\n",
      "Copying local files to hdfs:///user/root/tmp/mrjob/mrjob3_2C1.root.20180225.150132.004055/files/...\n",
      "Running step 1 of 2...\n",
      "  packageJobJar: [] [/usr/jars/hadoop-streaming-2.6.0-cdh5.7.0.jar] /tmp/streamjob3443014629834083642.jar tmpDir=null\n",
      "  Connecting to ResourceManager at /0.0.0.0:8032\n",
      "  Connecting to ResourceManager at /0.0.0.0:8032\n",
      "  Total input paths to process : 1\n",
      "  number of splits:2\n",
      "  Submitting tokens for job: job_1519482645842_0039\n",
      "  Submitted application application_1519482645842_0039\n",
      "  The url to track the job: http://docker.w261:8088/proxy/application_1519482645842_0039/\n",
      "  Running job: job_1519482645842_0039\n",
      "  Job job_1519482645842_0039 running in uber mode : false\n",
      "   map 0% reduce 0%\n",
      "   map 50% reduce 0%\n",
      "   map 100% reduce 0%\n",
      "   map 100% reduce 100%\n",
      "  Job job_1519482645842_0039 completed successfully\n",
      "  Output directory: hdfs:///user/root/tmp/mrjob/mrjob3_2C1.root.20180225.150132.004055/step-output/0000\n",
      "Counters: 49\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=50910582\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=2445\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=5100\n",
      "\t\tFILE: Number of bytes written=368531\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=50910926\n",
      "\t\tHDFS: Number of bytes written=2445\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of read operations=9\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tData-local map tasks=2\n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tTotal megabyte-seconds taken by all map tasks=12716032\n",
      "\t\tTotal megabyte-seconds taken by all reduce tasks=2461696\n",
      "\t\tTotal time spent by all map tasks (ms)=12418\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=12418\n",
      "\t\tTotal time spent by all reduce tasks (ms)=2404\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=2404\n",
      "\t\tTotal vcore-seconds taken by all map tasks=12418\n",
      "\t\tTotal vcore-seconds taken by all reduce tasks=2404\n",
      "\tMap-Reduce Framework\n",
      "\t\tCPU time spent (ms)=8560\n",
      "\t\tCombine input records=1960966\n",
      "\t\tCombine output records=315\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tGC time elapsed (ms)=131\n",
      "\t\tInput split bytes=344\n",
      "\t\tMap input records=312913\n",
      "\t\tMap output bytes=23979754\n",
      "\t\tMap output materialized bytes=5106\n",
      "\t\tMap output records=1960966\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tPhysical memory (bytes) snapshot=956211200\n",
      "\t\tReduce input groups=170\n",
      "\t\tReduce input records=315\n",
      "\t\tReduce output records=170\n",
      "\t\tReduce shuffle bytes=5106\n",
      "\t\tShuffled Maps =2\n",
      "\t\tSpilled Records=630\n",
      "\t\tTotal committed heap usage (bytes)=1617428480\n",
      "\t\tVirtual memory (bytes) snapshot=4227371008\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "Running step 2 of 2...\n",
      "  packageJobJar: [] [/usr/jars/hadoop-streaming-2.6.0-cdh5.7.0.jar] /tmp/streamjob947093094923266795.jar tmpDir=null\n",
      "  Connecting to ResourceManager at /0.0.0.0:8032\n",
      "  Connecting to ResourceManager at /0.0.0.0:8032\n",
      "  Total input paths to process : 1\n",
      "  number of splits:2\n",
      "  Submitting tokens for job: job_1519482645842_0040\n",
      "  Submitted application application_1519482645842_0040\n",
      "  The url to track the job: http://docker.w261:8088/proxy/application_1519482645842_0040/\n",
      "  Running job: job_1519482645842_0040\n",
      "  Job job_1519482645842_0040 running in uber mode : false\n",
      "   map 0% reduce 0%\n",
      "   map 100% reduce 0%\n",
      "   map 100% reduce 50%\n",
      "   map 100% reduce 100%\n",
      "  Job job_1519482645842_0040 completed successfully\n",
      "  Output directory: hdfs:///user/root/tmp/mrjob/mrjob3_2C1.root.20180225.150132.004055/output\n",
      "Counters: 49\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=3668\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=3509\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=3865\n",
      "\t\tFILE: Number of bytes written=481812\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=4008\n",
      "\t\tHDFS: Number of bytes written=3509\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of read operations=12\n",
      "\t\tHDFS: Number of write operations=4\n",
      "\tJob Counters \n",
      "\t\tData-local map tasks=2\n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=2\n",
      "\t\tTotal megabyte-seconds taken by all map tasks=4933632\n",
      "\t\tTotal megabyte-seconds taken by all reduce tasks=4831232\n",
      "\t\tTotal time spent by all map tasks (ms)=4818\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=4818\n",
      "\t\tTotal time spent by all reduce tasks (ms)=4718\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=4718\n",
      "\t\tTotal vcore-seconds taken by all map tasks=4818\n",
      "\t\tTotal vcore-seconds taken by all reduce tasks=4718\n",
      "\tMap-Reduce Framework\n",
      "\t\tCPU time spent (ms)=2180\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tGC time elapsed (ms)=52\n",
      "\t\tInput split bytes=340\n",
      "\t\tMap input records=170\n",
      "\t\tMap output bytes=3509\n",
      "\t\tMap output materialized bytes=3877\n",
      "\t\tMap output records=172\n",
      "\t\tMerged Map outputs=4\n",
      "\t\tPhysical memory (bytes) snapshot=1180880896\n",
      "\t\tReduce input groups=171\n",
      "\t\tReduce input records=172\n",
      "\t\tReduce output records=172\n",
      "\t\tReduce shuffle bytes=3877\n",
      "\t\tShuffled Maps =4\n",
      "\t\tSpilled Records=344\n",
      "\t\tTotal committed heap usage (bytes)=2017460224\n",
      "\t\tVirtual memory (bytes) snapshot=5677879296\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "Streaming final output from hdfs:///user/root/tmp/mrjob/mrjob3_2C1.root.20180225.150132.004055/output...\n",
      "\"b\"\t\"acct\\t163\"\t\n",
      "\"b\"\t\"dispute\\t904\"\t\n",
      "\"b\"\t\"disclosures\\t64\"\t\n",
      "\"b\"\t\"for\\t929\"\t\n",
      "\"b\"\t\"!Total\\t980483\"\t\n",
      "\"b\"\t\"didn't\\t925\"\t\n",
      "\"b\"\t\"collection\\t1907\"\t\n",
      "\"b\"\t\"did\\t139\"\t\n",
      "\"b\"\t\"determination\\t1490\"\t\n",
      "\"b\"\t\"fees\\t807\"\t\n",
      "\"b\"\t\"checks\\t75\"\t\n",
      "\"b\"\t\"charges\\t131\"\t\n",
      "\"b\"\t\"charged\\t976\"\t\n",
      "\"b\"\t\"changes\\t350\"\t\n",
      "\"b\"\t\"delinquent\\t1061\"\t\n",
      "\"b\"\t\"cash\\t240\"\t\n",
      "\"b\"\t\"delay\\t243\"\t\n",
      "\"b\"\t\"decrease\\t1149\"\t\n",
      "\"b\"\t\"can't\\t1999\"\t\n",
      "\"b\"\t\"i\\t925\"\t\n",
      "\"b\"\t\"increase\\t1149\"\t\n",
      "\"b\"\t\"debit\\t2422\"\t\n",
      "\"b\"\t\"bankruptcy\\t222\"\t\n",
      "\"b\"\t\"bank\\t202\"\t\n",
      "\"b\"\t\"balance\\t597\"\t\n",
      "\"b\"\t\"available\\t274\"\t\n",
      "\"b\"\t\"dealing\\t1944\"\t\n",
      "\"b\"\t\"atm\\t2422\"\t\n",
      "\"b\"\t\"day\\t71\"\t\n",
      "\"b\"\t\"arbitration\\t168\"\t\n",
      "\"b\"\t\"expect\\t807\"\t\n",
      "\"b\"\t\"apply\\t118\"\t\n",
      "\"b\"\t\"applied\\t139\"\t\n",
      "\"b\"\t\"credited\\t92\"\t\n",
      "\"b\"\t\"getting\\t291\"\t\n",
      "\"b\"\t\"forbearance\\t350\"\t\n",
      "\"b\"\t\"amt\\t71\"\t\n",
      "\"b\"\t\"amount\\t98\"\t\n",
      "\"b\"\t\"advertising\\t1193\"\t\n",
      "\"b\"\t\"advance\\t240\"\t\n",
      "\"b\"\t\"convenience\\t75\"\t\n",
      "\"b\"\t\"issue\\t1099\"\t\n",
      "\"b\"\t\"issuance\\t640\"\t\n",
      "\"b\"\t\"received\\t216\"\t\n",
      "\"b\"\t\"receive\\t139\"\t\n",
      "\"b\"\t\"with\\t1944\"\t\n",
      "\"b\"\t\"stop\\t131\"\t\n",
      "\"b\"\t\"promised\\t274\"\t\n",
      "\"b\"\t\"processing\\t243\"\t\n",
      "\"b\"\t\"workout\\t350\"\t\n",
      "\"b\"\t\"statement\\t1220\"\t\n",
      "\"b\"\t\"privacy\\t240\"\t\n",
      "\"b\"\t\"practices\\t1003\"\t\n",
      "\"b\"\t\"plans\\t350\"\t\n",
      "\"b\"\t\"payoff\\t1155\"\t\n",
      "\"b\"\t\"shopping\\t672\"\t\n",
      "\"b\"\t\"payment\\t92\"\t\n",
      "\"b\"\t\"transfer\\t597\"\t\n",
      "\"b\"\t\"transaction\\t1485\"\t\n",
      "\"b\"\t\"overlimit\\t127\"\t\n",
      "\"b\"\t\"out\\t1242\"\t\n",
      "\"b\"\t\"was\\t274\"\t\n",
      "\"b\"\t\"servicer\\t1944\"\t\n",
      "\"b\"\t\"service\\t1518\"\t\n",
      "\"b\"\t\"wrong\\t169\"\t\n",
      "\"b\"\t\"scam\\t566\"\t\n",
      "\"b\"\t\"sale\\t139\"\t\n",
      "\"b\"\t\"rewards\\t1002\"\t\n",
      "\"b\"\t\"monitoring\\t1453\"\t\n",
      "\"b\"\t\"money\\t413\"\t\n",
      "\"b\"\t\"using\\t2422\"\t\n",
      "\"b\"\t\"missing\\t64\"\t\n",
      "\"b\"\t\"marketing\\t1193\"\t\n",
      "\"b\"\t\"use\\t1477\"\t\n",
      "\"b\"\t\"terms\\t350\"\t\n",
      "\"b\"\t\"unsolicited\\t640\"\t\n",
      "\"b\"\t\"repay\\t1647\"\t\n",
      "\"b\"\t\"line\\t1732\"\t\n",
      "\"b\"\t\"lender\\t2165\"\t\n",
      "\"b\"\t\"relations\\t1367\"\t\n",
      "\"b\"\t\"late\\t1797\"\t\n",
      "\"b\"\t\"issues\\t538\"\t\n",
      "\"a\"\t\"your\\t3844\"\t\n",
      "\"a\"\t\"you\\t3821\"\t\n",
      "\"a\"\t\"withdrawals\\t10555\"\t\n",
      "\"a\"\t\"when\\t4095\"\t\n",
      "\"a\"\t\"verification\\t5214\"\t\n",
      "\"a\"\t\"underwriting\\t2774\"\t\n",
      "\"a\"\t\"unable\\t8178\"\t\n",
      "\"a\"\t\"to\\t8401\"\t\n",
      "\"a\"\t\"threatening\\t2505\"\t\n",
      "\"a\"\t\"theft\\t3276\"\t\n",
      "\"a\"\t\"the\\t6248\"\t\n",
      "\"a\"\t\"taking\\t3747\"\t\n",
      "\"a\"\t\"tactics\\t6920\"\t\n",
      "\"a\"\t\"statements\\t2508\"\t\n",
      "\"a\"\t\"sharing\\t2832\"\t\n",
      "\"a\"\t\"settlement\\t4350\"\t\n",
      "\"a\"\t\"servicing\\t36767\"\t\n",
      "\"a\"\t\"score\\t4357\"\t\n",
      "\"a\"\t\"representation\\t2508\"\t\n",
      "\"a\"\t\"reporting\\t6559\"\t\n",
      "\"a\"\t\"report\\t34903\"\t\n",
      "\"a\"\t\"repaying\\t3844\"\t\n",
      "\"a\"\t\"rate\\t3431\"\t\n",
      "\"a\"\t\"protection\\t4139\"\t\n",
      "\"a\"\t\"process\\t5505\"\t\n",
      "\"a\"\t\"problems\\t9484\"\t\n",
      "\"a\"\t\"payments\\t3226\"\t\n",
      "\"a\"\t\"pay\\t3821\"\t\n",
      "\"a\"\t\"owed\\t11848\"\t\n",
      "\"a\"\t\"other\\t7886\"\t\n",
      "\"a\"\t\"or\\t22533\"\t\n",
      "\"a\"\t\"opening\\t16205\"\t\n",
      "\"a\"\t\"on\\t29069\"\t\n",
      "\"a\"\t\"of\\t10885\"\t\n",
      "\"a\"\t\"not\\t12353\"\t\n",
      "\"a\"\t\"my\\t10731\"\t\n",
      "\"a\"\t\"modification\\t70487\"\t\n",
      "\"a\"\t\"managing\\t5006\"\t\n",
      "\"a\"\t\"making\\t3226\"\t\n",
      "\"a\"\t\"low\\t5663\"\t\n",
      "\"a\"\t\"loan\\t119630\"\t\n",
      "\"a\"\t\"lease\\t6337\"\t\n",
      "\"a\"\t\"receiving\\t3226\"\t\n",
      "\"a\"\t\"investigation\\t4858\"\t\n",
      "\"a\"\t\"interest\\t4238\"\t\n",
      "\"a\"\t\"information\\t29069\"\t\n",
      "\"a\"\t\"info\\t2896\"\t\n",
      "\"a\"\t\"incorrect\\t29133\"\t\n",
      "\"a\"\t\"improper\\t4309\"\t\n",
      "\"a\"\t\"illegal\\t2505\"\t\n",
      "\"a\"\t\"identity\\t4729\"\t\n",
      "\"a\"\t\"health\\t12545\"\t\n",
      "\"a\"\t\"get\\t4357\"\t\n",
      "\"a\"\t\"funds\\t5663\"\t\n",
      "\"a\"\t\"fraud\\t3842\"\t\n",
      "\"a\"\t\"fee\\t3198\"\t\n",
      "\"a\"\t\"false\\t2508\"\t\n",
      "\"a\"\t\"embezzlement\\t3276\"\t\n",
      "\"a\"\t\"disputes\\t6938\"\t\n",
      "\"a\"\t\"disclosure\\t5214\"\t\n",
      "\"a\"\t\"deposits\\t10555\"\t\n",
      "\"a\"\t\"decision\\t2774\"\t\n",
      "\"a\"\t\"debt\\t19309\"\t\n",
      "\"a\"\t\"customer\\t2734\"\t\n",
      "\"a\"\t\"credit\\t55251\"\t\n",
      "\"a\"\t\"costs\\t4350\"\t\n",
      "\"a\"\t\"contact\\t3053\"\t\n",
      "\"a\"\t\"cont'd\\t11848\"\t\n",
      "\"a\"\t\"company's\\t4858\"\t\n",
      "\"a\"\t\"communication\\t6920\"\t\n",
      "\"a\"\t\"collect\\t11848\"\t\n",
      "\"a\"\t\"club\\t12545\"\t\n",
      "\"a\"\t\"closing\\t2795\"\t\n",
      "\"a\"\t\"caused\\t5663\"\t\n",
      "\"a\"\t\"card\\t4405\"\t\n",
      "\"a\"\t\"cancelling\\t2795\"\t\n",
      "\"a\"\t\"by\\t5663\"\t\n",
      "\"a\"\t\"billing\\t8158\"\t\n",
      "\"a\"\t\"being\\t5663\"\t\n",
      "\"a\"\t\"attempts\\t11848\"\t\n",
      "\"a\"\t\"are\\t3821\"\t\n",
      "\"a\"\t\"apr\\t3431\"\t\n",
      "\"a\"\t\"application\\t8868\"\t\n",
      "\"a\"\t\"and\\t16448\"\t\n",
      "\"a\"\t\"an\\t2505\"\t\n",
      "\"a\"\t\"action\\t2505\"\t\n",
      "\"a\"\t\"account\\t20681\"\t\n",
      "\"a\"\t\"a\\t3503\"\t\n",
      "\"a\"\t\"!Total\\t980483\"\t\n",
      "\"a\"\t\"!Total\\t980483\"\t\n",
      "Removing HDFS temp directory hdfs:///user/root/tmp/mrjob/mrjob3_2C1.root.20180225.150132.004055...\n",
      "Removing temp directory /tmp/mrjob3_2C1.root.20180225.150132.004055...\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -rm -r {HDFS_DIR}/complaints-output3c1\n",
    "\n",
    "!python mrjob3_2C1.py \\\n",
    "    -r hadoop Consumer_Complaints.csv \\\n",
    "    --cmdenv PATH=/opt/anaconda/bin:$PATH \\\n",
    "    ##--output-dir {HDFS_DIR}/complaints-output3c1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hdfs dfs -rm -r {HDFS_DIR}/complaints-output3c1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting MRJob_unorderedTotalOrderSort.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile MRJob_unorderedTotalOrderSort.py\n",
    "\n",
    "import mrjob\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "from mrjob.protocol import RawValueProtocol\n",
    "from mrjob.protocol import RawProtocol\n",
    "from operator import itemgetter\n",
    "import numpy as np\n",
    "\n",
    "class MRJob_unorderedTotalOrderSort(MRJob):\n",
    "    \n",
    "    # Allows values to be treated as keys, so they can be used for sorting:\n",
    "    MRJob.SORT_VALUES = True \n",
    "    \n",
    "    # The protocols are critical. It will not work without these:\n",
    "    INTERNAL_PROTOCOL = RawProtocol\n",
    "    OUTPUT_PROTOCOL = RawProtocol\n",
    " \n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super(MRJob_unorderedTotalOrderSort, self).__init__(*args, **kwargs)\n",
    "        self.NUM_REDUCERS = 3\n",
    "    \n",
    "    \n",
    "    def mapper(self, _, line):\n",
    "        line = line.strip()\n",
    "        key,value = line.split('\\t')\n",
    "        if int(key) > 20:\n",
    "            yield \"A\",key+\"\\t\"+value\n",
    "        elif int(key) > 10:\n",
    "            yield \"B\",key+\"\\t\"+value\n",
    "        else:\n",
    "            yield \"C\",key+\"\\t\"+value\n",
    "            \n",
    "    def reducer(self,key,value):\n",
    "        for v in value:\n",
    "            yield key, v\n",
    "\n",
    "    \n",
    "    def steps(self):\n",
    "        \n",
    "        JOBCONF_STEP1 = {\n",
    "            'stream.num.map.output.key.fields':3,\n",
    "            'stream.map.output.field.separator':\"\\t\",\n",
    "            'mapreduce.partition.keypartitioner.options':'-k1,1',\n",
    "            'mapreduce.job.output.key.comparator.class': 'org.apache.hadoop.mapred.lib.KeyFieldBasedComparator',\n",
    "            'mapreduce.partition.keycomparator.options':'-k1,1 -k2,2nr -k3,3',\n",
    "            'mapred.reduce.tasks': self.NUM_REDUCERS,\n",
    "            'partitioner':'org.apache.hadoop.mapred.lib.KeyFieldBasedPartitioner'\n",
    "        }\n",
    "        return [MRStep(jobconf=JOBCONF_STEP1,\n",
    "                    mapper=self.mapper,\n",
    "                    reducer=self.reducer)\n",
    "                ]\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    MRJob_unorderedTotalOrderSort.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW3.3. Shopping Cart Analysis\n",
    "Product Recommendations: \n",
    ">The action or practice of selling additional products or services to existing customers is called cross-selling. Giving a product recommendation is one example of cross-selling that are frequently used by online retailers. One simple method to give product recommendations is to recommend products that are frequently browsed together by the customers.\n",
    "\t\n",
    "For this homework question you will perform some exploratory data analysis on the online browsing behavior dataset `ProductPurchaseData.txt` in preparation for performing product recommendations later. Use Hadoop MapReduce for your EDA. Consider answering the following questions: \n",
    "* How many unique items are available from this supplier?\n",
    "* What is the largest basket?\n",
    "* What are the top 50 most frequently purchased items? their frequency? their relative frequency? (break ties by sorting in alphabetical order\n",
    "\n",
    "NOTE: Use a single reducer for all jobs in this section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### START STUDENT CODE HW33 (INSERT CELLS BELOW AS NEEDED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FRO11987 ELE17451 ELE89019 SNA90258 GRO99222 \n",
      "GRO99222 GRO12298 FRO12685 ELE91550 SNA11465 ELE26917 ELE52966 FRO90334 SNA30755 ELE17451 FRO84225 SNA80192 \n",
      "ELE17451 GRO73461 DAI22896 SNA99873 FRO86643 \n",
      "ELE17451 ELE37798 FRO86643 GRO56989 ELE23393 SNA11465 \n",
      "ELE17451 SNA69641 FRO86643 FRO78087 SNA11465 GRO39357 ELE28573 ELE11375 DAI54444 \n",
      "ELE17451 GRO73461 DAI22896 SNA99873 FRO18919 DAI50921 SNA80192 GRO75578 \n",
      "ELE17451 ELE59935 FRO18919 ELE23393 SNA80192 SNA85662 SNA91554 DAI22177 \n",
      "ELE17451 SNA69641 FRO18919 SNA90258 ELE28573 ELE11375 DAI14125 FRO78087 \n",
      "ELE17451 GRO73461 DAI22896 SNA80192 SNA85662 SNA90258 DAI46755 FRO81176 ELE66810 DAI49199 DAI91535 GRO94758 ELE94711 DAI22177 \n",
      "ELE17451 SNA69641 DAI91535 GRO94758 GRO99222 FRO76833 FRO81176 SNA80192 DAI54690 ELE37798 GRO56989 \n",
      "cat: write error: Broken pipe\n"
     ]
    }
   ],
   "source": [
    "!cat ProductPurchaseData.txt | head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting productWordCountMapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile productWordCountMapper.py\n",
    "#!/usr/bin/env python\n",
    "import sys\n",
    "import re\n",
    "sys.stderr.write(\"reporter:counter:Mapper Counters,Calls,1\\n\")\n",
    "\n",
    "max_len = 0\n",
    "for line in sys.stdin:\n",
    "    line = line.strip()\n",
    "    words = line.split()\n",
    "    if len(words) > max_len:\n",
    "        max_len = len(words)\n",
    "    for word in words:\n",
    "        sys.stderr.write(\"reporter:counter:Mapper Counters,Total,1\\n\")\n",
    "        print '%s\\t%d' % (word, 1)\n",
    "print \"!MAX_LENGTH\\t%s\" % (max_len)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting productWordCountReducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile productWordCountReducer.py\n",
    "#!/usr/bin/env python\n",
    "from operator import itemgetter\n",
    "import sys\n",
    "cur_key = None\n",
    "cur_count = 0\n",
    "sys.stderr.write(\"reporter:counter:Reducer Counters,Calls,1\\n\")\n",
    "\n",
    "current_word = None\n",
    "current_count = 0\n",
    "word = None\n",
    "max_len = 0\n",
    "# input comes from STDIN\n",
    "for line in sys.stdin:\n",
    "    key, value = line.split(\"\\t\")\n",
    "    if key == \"!MAX_LENGTH\":\n",
    "        if int(value) > max_len:\n",
    "            max_len = int(value)\n",
    "    elif key == cur_key:\n",
    "        cur_count += int(value)\n",
    "    else:\n",
    "        if cur_key:\n",
    "            print '%s\\t%s' % (cur_key, cur_count)\n",
    "        cur_key = key\n",
    "        cur_count = int(value)\n",
    "\n",
    "print '%s\\t%s' % (cur_key, cur_count)\n",
    "\n",
    "print \"!MAX_LENGTH\\t%s\" % (max_len)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted /user/root/HW3/3.2.1/productWordCount\n",
      "packageJobJar: [] [/usr/jars/hadoop-streaming-2.6.0-cdh5.7.0.jar] /tmp/streamjob6056768237583984552.jar tmpDir=null\n",
      "18/02/07 05:57:34 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "18/02/07 05:57:34 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "18/02/07 05:57:35 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "18/02/07 05:57:35 INFO mapreduce.JobSubmitter: number of splits:2\n",
      "18/02/07 05:57:35 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1517924926507_0013\n",
      "18/02/07 05:57:35 INFO impl.YarnClientImpl: Submitted application application_1517924926507_0013\n",
      "18/02/07 05:57:35 INFO mapreduce.Job: The url to track the job: http://docker.w261:8088/proxy/application_1517924926507_0013/\n",
      "18/02/07 05:57:35 INFO mapreduce.Job: Running job: job_1517924926507_0013\n",
      "18/02/07 05:57:40 INFO mapreduce.Job: Job job_1517924926507_0013 running in uber mode : false\n",
      "18/02/07 05:57:40 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "18/02/07 05:57:46 INFO mapreduce.Job:  map 50% reduce 0%\n",
      "18/02/07 05:57:47 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "18/02/07 05:57:52 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "18/02/07 05:57:53 INFO mapreduce.Job: Job job_1517924926507_0013 completed successfully\n",
      "18/02/07 05:57:53 INFO mapreduce.Job: Counters: 52\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=4950752\n",
      "\t\tFILE: Number of bytes written=10253160\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=3462855\n",
      "\t\tHDFS: Number of bytes written=142673\n",
      "\t\tHDFS: Number of read operations=9\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tData-local map tasks=2\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=6475\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=2956\n",
      "\t\tTotal time spent by all map tasks (ms)=6475\n",
      "\t\tTotal time spent by all reduce tasks (ms)=2956\n",
      "\t\tTotal vcore-seconds taken by all map tasks=6475\n",
      "\t\tTotal vcore-seconds taken by all reduce tasks=2956\n",
      "\t\tTotal megabyte-seconds taken by all map tasks=6630400\n",
      "\t\tTotal megabyte-seconds taken by all reduce tasks=3026944\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=31101\n",
      "\t\tMap output records=380826\n",
      "\t\tMap output bytes=4189094\n",
      "\t\tMap output materialized bytes=4950758\n",
      "\t\tInput split bytes=242\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=12593\n",
      "\t\tReduce shuffle bytes=4950758\n",
      "\t\tReduce input records=380826\n",
      "\t\tReduce output records=12593\n",
      "\t\tSpilled Records=761652\n",
      "\t\tShuffled Maps =2\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tGC time elapsed (ms)=67\n",
      "\t\tCPU time spent (ms)=7170\n",
      "\t\tPhysical memory (bytes) snapshot=973307904\n",
      "\t\tVirtual memory (bytes) snapshot=4178219008\n",
      "\t\tTotal committed heap usage (bytes)=1508376576\n",
      "\tMapper Counters\n",
      "\t\tCalls=2\n",
      "\t\tTotal=380824\n",
      "\tReducer Counters\n",
      "\t\tCalls=1\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=3462613\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=142673\n",
      "18/02/07 05:57:53 INFO streaming.StreamJob: Output directory: /user/root/HW3/3.2.1/productWordCount\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -rm -r {HDFS_DIR}/3.2.1/productWordCount\n",
    "\n",
    "!hadoop jar {JAR_FILE} \\\n",
    "  -files productWordCountMapper.py,productWordCountReducer.py \\\n",
    "  -mapper productWordCountMapper.py \\\n",
    "  -reducer productWordCountReducer.py \\\n",
    "  -input {HDFS_DIR}/ProductPurchaseData.txt \\\n",
    "  -output {HDFS_DIR}/3.2.1/productWordCount "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SNA99814\t1\n",
      "SNA99861\t3\n",
      "SNA99870\t19\n",
      "SNA99873\t2083\n",
      "SNA99886\t44\n",
      "SNA99895\t2\n",
      "SNA99918\t3\n",
      "SNA99924\t16\n",
      "SNA99941\t1\n",
      "!MAX_LENGTH\t37\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -cat {HDFS_DIR}/3.2.1/productWordCount/* | tail -n 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting productFreqCountMap.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile productFreqCountMap.py\n",
    "#!/usr/bin/env python\n",
    "\n",
    "from __future__ import division\n",
    "import sys\n",
    "\n",
    "total = 0\n",
    "\n",
    "for line in sys.stdin:\n",
    "    line = line.strip()\n",
    "    key, value = line.split('\\t')\n",
    "    if key == '!MAX_LENGTH':\n",
    "        continue\n",
    "    else:\n",
    "        total += int(value)\n",
    "        print '%s\\t%s' % (key, value)\n",
    "\n",
    "print '!TOTAL\\t', total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting productFreqCountReduce.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile productFreqCountReduce.py\n",
    "#!/usr/bin/env python\n",
    "\n",
    "from __future__ import division\n",
    "import sys\n",
    "\n",
    "total = 0\n",
    "\n",
    "for line in sys.stdin:\n",
    "    line = line.strip()\n",
    "    key, value = line.split('\\t')\n",
    "    if key == '!TOTAL':\n",
    "        total += int(value)\n",
    "    else:\n",
    "        print '%s\\t%s\\t%s' % (key, value, int(value)/total)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted /user/root/HW3/3.2.1/productFreqCount\n",
      "packageJobJar: [] [/usr/jars/hadoop-streaming-2.6.0-cdh5.7.0.jar] /tmp/streamjob8753035782598735585.jar tmpDir=null\n",
      "18/02/07 06:15:04 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "18/02/07 06:15:04 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "18/02/07 06:15:05 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "18/02/07 06:15:05 INFO mapreduce.JobSubmitter: number of splits:2\n",
      "18/02/07 06:15:05 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1517924926507_0016\n",
      "18/02/07 06:15:05 INFO impl.YarnClientImpl: Submitted application application_1517924926507_0016\n",
      "18/02/07 06:15:05 INFO mapreduce.Job: The url to track the job: http://docker.w261:8088/proxy/application_1517924926507_0016/\n",
      "18/02/07 06:15:05 INFO mapreduce.Job: Running job: job_1517924926507_0016\n",
      "18/02/07 06:15:10 INFO mapreduce.Job: Job job_1517924926507_0016 running in uber mode : false\n",
      "18/02/07 06:15:10 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "18/02/07 06:15:15 INFO mapreduce.Job:  map 50% reduce 0%\n",
      "18/02/07 06:15:16 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "18/02/07 06:15:20 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "18/02/07 06:15:20 INFO mapreduce.Job: Job job_1517924926507_0016 completed successfully\n",
      "18/02/07 06:15:20 INFO mapreduce.Job: Counters: 49\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=180474\n",
      "\t\tFILE: Number of bytes written=714956\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=147031\n",
      "\t\tHDFS: Number of bytes written=368635\n",
      "\t\tHDFS: Number of read operations=9\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tData-local map tasks=2\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=5451\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=2569\n",
      "\t\tTotal time spent by all map tasks (ms)=5451\n",
      "\t\tTotal time spent by all reduce tasks (ms)=2569\n",
      "\t\tTotal vcore-seconds taken by all map tasks=5451\n",
      "\t\tTotal vcore-seconds taken by all reduce tasks=2569\n",
      "\t\tTotal megabyte-seconds taken by all map tasks=5581824\n",
      "\t\tTotal megabyte-seconds taken by all reduce tasks=2630656\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=12593\n",
      "\t\tMap output records=12594\n",
      "\t\tMap output bytes=155280\n",
      "\t\tMap output materialized bytes=180480\n",
      "\t\tInput split bytes=262\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=12594\n",
      "\t\tReduce shuffle bytes=180480\n",
      "\t\tReduce input records=12594\n",
      "\t\tReduce output records=12592\n",
      "\t\tSpilled Records=25188\n",
      "\t\tShuffled Maps =2\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tGC time elapsed (ms)=55\n",
      "\t\tCPU time spent (ms)=3310\n",
      "\t\tPhysical memory (bytes) snapshot=941879296\n",
      "\t\tVirtual memory (bytes) snapshot=4180439040\n",
      "\t\tTotal committed heap usage (bytes)=1508376576\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=146769\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=368635\n",
      "18/02/07 06:15:20 INFO streaming.StreamJob: Output directory: /user/root/HW3/3.2.1/productFreqCount\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -rm -r {HDFS_DIR}/3.2.1/productFreqCount\n",
    "\n",
    "!hadoop jar {JAR_FILE} \\\n",
    "  -D stream.num.map.output.key.field=3 \\\n",
    "  -D stream.map.output.field.separator=\"\\t\" \\\n",
    "  -D mapreduce.partition.keypartitioner.options=\"-k1,1\" \\\n",
    "  -D mapreduce.job.output.key.comparator.class=org.apache.hadoop.mapred.lib.KeyFieldBasedComparator \\\n",
    "  -D mapreduce.partition.keycomparator.options=\"-k2,2nr -k1,1\" \\\n",
    "  -files productFreqCountMap.py,productFreqCountReduce.py \\\n",
    "  -mapper productFreqCountMap.py \\\n",
    "  -reducer productFreqCountReduce.py \\\n",
    "  -input {HDFS_DIR}/3.2.1/productWordCount \\\n",
    "  -output {HDFS_DIR}/3.2.1/productFreqCount "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### END STUDENT CODE HW33"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DAI62779\t6667\t0.0175067747831\n",
      "FRO40251\t3881\t0.010191059387\n",
      "ELE17451\t3875\t0.0101753040775\n",
      "GRO73461\t3602\t0.00945843749344\n",
      "SNA80324\t3044\t0.00799319370628\n",
      "ELE32164\t2851\t0.0074863979161\n",
      "DAI75645\t2736\t0.00718442114993\n",
      "SNA45677\t2455\t0.0064465474865\n",
      "FRO31317\t2330\t0.0061183118711\n",
      "DAI85309\t2293\t0.00602115412894\n",
      "ELE26917\t2292\t0.00601852824402\n",
      "FRO80039\t2233\t0.00586360103355\n",
      "GRO21487\t2115\t0.00555374661261\n",
      "SNA99873\t2083\t0.00546971829507\n",
      "GRO59710\t2004\t0.00526227338613\n",
      "GRO71621\t1920\t0.00504169905258\n",
      "FRO85978\t1918\t0.00503644728273\n",
      "GRO30386\t1840\t0.00483162825872\n",
      "ELE74009\t1816\t0.00476860702057\n",
      "GRO56726\t1784\t0.00468457870302\n",
      "DAI63921\t1773\t0.00465569396887\n",
      "GRO46854\t1756\t0.00461105392517\n",
      "ELE66600\t1713\t0.00449814087347\n",
      "DAI83733\t1712\t0.00449551498855\n",
      "FRO32293\t1702\t0.00446925613932\n",
      "ELE66810\t1697\t0.0044561267147\n",
      "SNA55762\t1646\t0.00432220658362\n",
      "DAI22177\t1627\t0.00427231477008\n",
      "FRO78087\t1531\t0.00402022981745\n",
      "ELE99737\t1516\t0.0039808415436\n",
      "ELE34057\t1489\t0.00390994265067\n",
      "GRO94758\t1489\t0.00390994265067\n",
      "FRO35904\t1436\t0.00377077074974\n",
      "FRO53271\t1420\t0.00372875659097\n",
      "SNA93860\t1407\t0.00369462008697\n",
      "SNA90094\t1390\t0.00364998004327\n",
      "GRO38814\t1352\t0.00355019641619\n",
      "ELE56788\t1345\t0.00353181522173\n",
      "GRO61133\t1321\t0.00346879398357\n",
      "DAI88807\t1316\t0.00345566455896\n",
      "ELE74482\t1316\t0.00345566455896\n",
      "ELE59935\t1311\t0.00344253513434\n",
      "SNA96271\t1295\t0.00340052097557\n",
      "DAI43223\t1290\t0.00338739155095\n",
      "ELE91337\t1289\t0.00338476566603\n",
      "GRO15017\t1275\t0.0033480032771\n",
      "DAI31081\t1261\t0.00331124088818\n",
      "GRO81087\t1220\t0.00320357960633\n",
      "DAI22896\t1219\t0.0032009537214\n",
      "GRO85051\t1214\t0.00318782429679\n",
      "cat: Unable to write to output stream.\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -cat {HDFS_DIR}/3.2.1/productFreqCount/* | head -n 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12592\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -cat {HDFS_DIR}/3.2.1/productFreqCount/* | wc -l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"3.3.1\"></a>\n",
    "## HW3.3.1 OPTIONAL \n",
    "Using 2 reducers:  Report your findings such as number of unique products; largest basket; report the top 50 most frequently purchased items,  their frequency,  and their relative frequency (break ties by sorting the products alphabetical order) etc. using Hadoop Map-Reduce. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### START STUDENT CODE HW331 (INSERT CELLS BELOW AS NEEDED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### END STUDENT CODE HW331"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW3.4. (Computationally prohibitive but then again Hadoop can handle this) Pairs\n",
    "\n",
    "Suppose we want to recommend new products to the customer based on the products they\n",
    "have already browsed on the online website. Write a map-reduce program \n",
    "to find products which are frequently browsed together. Fix the support count (cooccurence count) to s = 100 \n",
    "(i.e. product pairs need to occur together at least 100 times to be considered frequent) \n",
    "and find pairs of items (sometimes referred to itemsets of size 2 in association rule mining) that have a support count of 100 or more.\n",
    "\n",
    "List the top 50 product pairs with corresponding support count (aka frequency), and support (aka relative frequency) in decreasing order of support  for frequent (100>count) itemsets of size 2. \n",
    "\n",
    "The global relative frequency (support) of an itemset of size 2, is the number of occurances of the item set, related to the total number of baskets in the data set. For instance, if the pair (\"beer\",\"diapers\") occurs 10 times, and there are a total of 20 baskets, then the relative frequency (support) for (\"beer\",\"diapers\") is $\\frac{10}{20}$ = 0.5.\n",
    "\n",
    "$$ support(A,B) = \\frac{count(A,B)}{N} $$\n",
    "\n",
    "\n",
    "Use the Pairs pattern (lecture 3)  to  extract these frequent itemsets of size 2. Feel free to use combiners if they bring value. Instrument your code with counters for count the number of times your mapper, combiner, and reducers are called.  \n",
    "\n",
    "Please output records of the following form for the top 50 pairs (itemsets of size 2): \n",
    "\n",
    "      item1, item2, support count, support\n",
    "\n",
    "\n",
    "\n",
    "Fix the ordering of the pairs lexicographically (left to right), and break ties in support (between pairs, if any exist) by taking the first ones in lexicographically increasing order. \n",
    "\n",
    "Report  the compute time for the Pairs job. Describe the computational setup used (E.g., single computer; dual core; linux, number of mappers, number of reducers)\n",
    "Instrument your mapper, combiner, and reducer to count how many times each is called using Counters and report these counts.\n",
    "\n",
    "\n",
    "***\n",
    "\n",
    "__OPTIONAL NOTES:__   \n",
    "\n",
    "Distinguishing between global relative frequency, and local relative frequency (AKA support vs confidence in association rule mining), the local relative frequency of a bigram (AKA itemset of size 2) $(w_i, w_j)$ relative to its first word $w_i$ is the number of occurrences of the pair $(w_i, w_j)$ related to the number of occurrences of the word $w_i$. For instance, if the word \"euro\" followed by the word \"crisis\" occurs 10 times and the word \"euro\" occurs 20 times in total, we say that the frequency of the pair (\"euro\",\"crisis\") is $\\frac{10}{20}$ = 0.5.\n",
    "\n",
    "$$ f(B|A) = \\frac{count(A,B)}{count(A)} = \\frac{count(A,B)}{\\sum_{B'} count(A,B')}$$\n",
    "\n",
    "This specific notion of relative frequency with respect to the first term in the pair is example of the concept of confidence from association rule mining.  \n",
    "Additional reference: https://www-users.cs.umn.edu/~kumar/dmbook/ch6.pdf pg. 3-4   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing pairsMapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile pairsMapper.py\n",
    "#!/usr/bin/env python\n",
    "from __future__ import division\n",
    "import sys\n",
    "import itertools\n",
    "\n",
    "############################################################\n",
    "# Using order inversion, we are able to count the total\n",
    "# number of baskets to send to the reducer for calculating\n",
    "# the relative frequencies.\n",
    "############################################################\n",
    "\n",
    "\n",
    "sys.stderr.write(\"reporter:counter:Mapper Counters,Calls,1\\n\")\n",
    "\n",
    "total = 0\n",
    "\n",
    "for line in sys.stdin:\n",
    "    line = line.strip()\n",
    "    words = line.split()\n",
    "    total += 1\n",
    "    for subset in itertools.combinations(sorted(set(words)), 2):\n",
    "        print '%s\\t%s' % (subset[0]+\" - \"+subset[1], 1)\n",
    "print \"%s\\t%s\" % (\"!TOTAL\", total)    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing pairsReducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile pairsReducer.py\n",
    "#!/usr/bin/env python\n",
    "from __future__ import division\n",
    "import sys\n",
    "\n",
    "sys.stderr.write(\"reporter:counter:Reducer Counters,Calls,1\\n\")\n",
    "\n",
    "total = 0\n",
    "cur_key = None\n",
    "cur_count = 0\n",
    "support_count = 100\n",
    "\n",
    "for line in sys.stdin:\n",
    "    line = line.strip()\n",
    "    key,value = line.split(\"\\t\")\n",
    "    if key == \"!TOTAL\":\n",
    "        total += int(value)\n",
    "    elif key == cur_key:\n",
    "        cur_count += int(value)\n",
    "    else:\n",
    "        if cur_key:\n",
    "            if cur_count >= support_count:\n",
    "                print '%s\\t%s\\t%s' % (cur_key,cur_count,str(cur_count/total))\n",
    "        cur_key = key\n",
    "        cur_count = int(value)\n",
    "\n",
    "if cur_count >= support_count:\n",
    "    print '%s\\t%s\\t%s' % (cur_key,cur_count,str(cur_count/total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted /user/root/HW3/3.4/output1\n",
      "packageJobJar: [] [/usr/jars/hadoop-streaming-2.6.0-cdh5.7.0.jar] /tmp/streamjob2289570488318103950.jar tmpDir=null\n",
      "18/02/07 06:33:45 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "18/02/07 06:33:45 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "18/02/07 06:33:46 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "18/02/07 06:33:46 INFO mapreduce.JobSubmitter: number of splits:2\n",
      "18/02/07 06:33:46 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1517924926507_0021\n",
      "18/02/07 06:33:46 INFO impl.YarnClientImpl: Submitted application application_1517924926507_0021\n",
      "18/02/07 06:33:46 INFO mapreduce.Job: The url to track the job: http://docker.w261:8088/proxy/application_1517924926507_0021/\n",
      "18/02/07 06:33:46 INFO mapreduce.Job: Running job: job_1517924926507_0021\n",
      "18/02/07 06:33:51 INFO mapreduce.Job: Job job_1517924926507_0021 running in uber mode : false\n",
      "18/02/07 06:33:51 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "18/02/07 06:33:59 INFO mapreduce.Job:  map 50% reduce 0%\n",
      "18/02/07 06:34:00 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "18/02/07 06:34:09 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "18/02/07 06:34:09 INFO mapreduce.Job: Job job_1517924926507_0021 completed successfully\n",
      "18/02/07 06:34:09 INFO mapreduce.Job: Counters: 52\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=60816372\n",
      "\t\tFILE: Number of bytes written=121984025\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=3462855\n",
      "\t\tHDFS: Number of bytes written=54433\n",
      "\t\tHDFS: Number of read operations=9\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tKilled map tasks=1\n",
      "\t\tLaunched map tasks=3\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tData-local map tasks=3\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=11712\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=7684\n",
      "\t\tTotal time spent by all map tasks (ms)=11712\n",
      "\t\tTotal time spent by all reduce tasks (ms)=7684\n",
      "\t\tTotal vcore-seconds taken by all map tasks=11712\n",
      "\t\tTotal vcore-seconds taken by all reduce tasks=7684\n",
      "\t\tTotal megabyte-seconds taken by all map tasks=11993088\n",
      "\t\tTotal megabyte-seconds taken by all reduce tasks=7868416\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=31101\n",
      "\t\tMap output records=2534016\n",
      "\t\tMap output bytes=55748334\n",
      "\t\tMap output materialized bytes=60816378\n",
      "\t\tInput split bytes=242\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=877096\n",
      "\t\tReduce shuffle bytes=60816378\n",
      "\t\tReduce input records=2534016\n",
      "\t\tReduce output records=1334\n",
      "\t\tSpilled Records=5068032\n",
      "\t\tShuffled Maps =2\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tGC time elapsed (ms)=79\n",
      "\t\tCPU time spent (ms)=13110\n",
      "\t\tPhysical memory (bytes) snapshot=1091538944\n",
      "\t\tVirtual memory (bytes) snapshot=4176850944\n",
      "\t\tTotal committed heap usage (bytes)=1604321280\n",
      "\tMapper Counters\n",
      "\t\tCalls=2\n",
      "\tReducer Counters\n",
      "\t\tCalls=1\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=3462613\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=54433\n",
      "18/02/07 06:34:09 INFO streaming.StreamJob: Output directory: /user/root/HW3/3.4/output1\n",
      "28.2559680939\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "start=time.time() \n",
    "\n",
    "!hdfs dfs -rm -r {HDFS_DIR}/3.4/output1\n",
    "\n",
    "!hadoop jar {JAR_FILE} \\\n",
    "  -files pairsMapper.py,pairsReducer.py \\\n",
    "  -mapper pairsMapper.py \\\n",
    "  -reducer pairsReducer.py \\\n",
    "  -input {HDFS_DIR}/ProductPurchaseData.txt \\\n",
    "  -output {HDFS_DIR}/3.4/output1\n",
    "\n",
    "end = time.time()\n",
    "countTime = end-start\n",
    "\n",
    "print countTime\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DAI16732 - FRO78087\t106\t0.00340825053857\n",
      "DAI18527 - SNA44451\t102\t0.0032796373107\n",
      "DAI22177 - DAI31081\t127\t0.00408346998489\n",
      "DAI22177 - DAI62779\t382\t0.0122825632616\n",
      "DAI22177 - DAI63921\t136\t0.0043728497476\n",
      "DAI22177 - DAI75645\t123\t0.00395485675702\n",
      "DAI22177 - DAI83733\t126\t0.00405131667792\n",
      "DAI22177 - DAI85309\t172\t0.00553036879843\n",
      "DAI22177 - ELE17451\t203\t0.00652712131443\n",
      "DAI22177 - ELE26917\t134\t0.00430854313366\n",
      "cat: Unable to write to output stream.\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -cat {HDFS_DIR}/3.4/output1/* | head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm: `/user/root/HW3/3.4/output2': No such file or directory\n",
      "packageJobJar: [] [/usr/jars/hadoop-streaming-2.6.0-cdh5.7.0.jar] /tmp/streamjob5484665887784653194.jar tmpDir=null\n",
      "18/02/07 06:35:08 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "18/02/07 06:35:08 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "18/02/07 06:35:08 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "18/02/07 06:35:09 INFO mapreduce.JobSubmitter: number of splits:2\n",
      "18/02/07 06:35:09 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1517924926507_0022\n",
      "18/02/07 06:35:09 INFO impl.YarnClientImpl: Submitted application application_1517924926507_0022\n",
      "18/02/07 06:35:09 INFO mapreduce.Job: The url to track the job: http://docker.w261:8088/proxy/application_1517924926507_0022/\n",
      "18/02/07 06:35:09 INFO mapreduce.Job: Running job: job_1517924926507_0022\n",
      "18/02/07 06:35:15 INFO mapreduce.Job: Job job_1517924926507_0022 running in uber mode : false\n",
      "18/02/07 06:35:15 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "18/02/07 06:35:19 INFO mapreduce.Job:  map 50% reduce 0%\n",
      "18/02/07 06:35:20 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "18/02/07 06:35:24 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "18/02/07 06:35:24 INFO mapreduce.Job: Job job_1517924926507_0022 completed successfully\n",
      "18/02/07 06:35:24 INFO mapreduce.Job: Counters: 49\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=58441\n",
      "\t\tFILE: Number of bytes written=466096\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=58769\n",
      "\t\tHDFS: Number of bytes written=55767\n",
      "\t\tHDFS: Number of read operations=9\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tData-local map tasks=2\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=5222\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=2481\n",
      "\t\tTotal time spent by all map tasks (ms)=5222\n",
      "\t\tTotal time spent by all reduce tasks (ms)=2481\n",
      "\t\tTotal vcore-seconds taken by all map tasks=5222\n",
      "\t\tTotal vcore-seconds taken by all reduce tasks=2481\n",
      "\t\tTotal megabyte-seconds taken by all map tasks=5347328\n",
      "\t\tTotal megabyte-seconds taken by all reduce tasks=2540544\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=1334\n",
      "\t\tMap output records=1334\n",
      "\t\tMap output bytes=55767\n",
      "\t\tMap output materialized bytes=58447\n",
      "\t\tInput split bytes=240\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=1334\n",
      "\t\tReduce shuffle bytes=58447\n",
      "\t\tReduce input records=1334\n",
      "\t\tReduce output records=1334\n",
      "\t\tSpilled Records=2668\n",
      "\t\tShuffled Maps =2\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tGC time elapsed (ms)=36\n",
      "\t\tCPU time spent (ms)=1890\n",
      "\t\tPhysical memory (bytes) snapshot=938115072\n",
      "\t\tVirtual memory (bytes) snapshot=4172402688\n",
      "\t\tTotal committed heap usage (bytes)=1508376576\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=58529\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=55767\n",
      "18/02/07 06:35:24 INFO streaming.StreamJob: Output directory: /user/root/HW3/3.4/output2\n",
      "20.3689661026\n"
     ]
    }
   ],
   "source": [
    "start=time.time() \n",
    "\n",
    "!hdfs dfs -rm -r {HDFS_DIR}/3.4/output2\n",
    "\n",
    "!hadoop jar {JAR_FILE} \\\n",
    "  -D stream.num.map.output.key.field=2 \\\n",
    "  -D stream.map.output.field.separator=\"\\t\" \\\n",
    "  -D mapreduce.partition.keypartitioner.options=\"-k1,1\" \\\n",
    "  -D mapreduce.job.output.key.comparator.class=org.apache.hadoop.mapred.lib.KeyFieldBasedComparator \\\n",
    "  -D mapreduce.partition.keycomparator.options=\"-k2,2nr -k1,1\" \\\n",
    "  -mapper /bin/cat \\\n",
    "  -reducer /bin/cat \\\n",
    "  -input {HDFS_DIR}/3.4/output1 \\\n",
    "  -output {HDFS_DIR}/3.4/output2\n",
    "\n",
    "end = time.time()\n",
    "sortTime = end-start\n",
    "    \n",
    "print sortTime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DAI62779 - ELE17451\t1592\t0.0511880646925\t\n",
      "FRO40251 - SNA80324\t1412\t0.0454004694383\t\n",
      "DAI75645 - FRO40251\t1254\t0.0403202469374\t\n",
      "FRO40251 - GRO85051\t1213\t0.0390019613517\t\n",
      "DAI62779 - GRO73461\t1139\t0.0366226166361\t\n",
      "DAI75645 - SNA80324\t1130\t0.0363332368734\t\n",
      "DAI62779 - FRO40251\t1070\t0.0344040384554\t\n",
      "DAI62779 - SNA80324\t923\t0.0296775023311\t\n",
      "DAI62779 - DAI85309\t918\t0.0295167357963\t\n",
      "ELE32164 - GRO59710\t911\t0.0292916626475\t\n",
      "DAI62779 - DAI75645\t882\t0.0283592167454\t\n",
      "FRO40251 - GRO73461\t882\t0.0283592167454\t\n",
      "DAI62779 - ELE92920\t877\t0.0281984502106\t\n",
      "FRO40251 - FRO92469\t835\t0.026848011318\t\n",
      "DAI62779 - ELE32164\t832\t0.0267515513971\t\n",
      "DAI75645 - GRO73461\t712\t0.0228931545609\t\n",
      "DAI43223 - ELE32164\t711\t0.022861001254\t\n",
      "DAI62779 - GRO30386\t709\t0.02279669464\t\n",
      "ELE17451 - FRO40251\t697\t0.0224108549564\t\n",
      "DAI85309 - ELE99737\t659\t0.0211890292917\t\n",
      "DAI62779 - ELE26917\t650\t0.020899649529\t\n",
      "GRO21487 - GRO73461\t631\t0.0202887366966\t\n",
      "DAI62779 - SNA45677\t604\t0.0194205974084\t\n",
      "ELE17451 - SNA80324\t597\t0.0191955242597\t\n",
      "DAI62779 - GRO71621\t595\t0.0191312176457\t\n",
      "DAI62779 - SNA55762\t593\t0.0190669110318\t\n",
      "DAI62779 - DAI83733\t586\t0.018841837883\t\n",
      "ELE17451 - GRO73461\t580\t0.0186489180412\t\n",
      "GRO73461 - SNA80324\t562\t0.0180701585158\t\n",
      "DAI62779 - GRO59710\t561\t0.0180380052088\t\n",
      "DAI62779 - FRO80039\t550\t0.0176843188322\t\n",
      "DAI75645 - ELE17451\t547\t0.0175878589113\t\n",
      "DAI62779 - SNA93860\t537\t0.0172663258416\t\n",
      "DAI55148 - DAI62779\t526\t0.016912639465\t\n",
      "DAI43223 - GRO59710\t512\t0.0164624931674\t\n",
      "ELE17451 - ELE32164\t511\t0.0164303398605\t\n",
      "DAI62779 - SNA18336\t506\t0.0162695733256\t\n",
      "ELE32164 - GRO73461\t486\t0.0156265071863\t\n",
      "DAI62779 - FRO78087\t482\t0.0154978939584\t\n",
      "DAI85309 - ELE17451\t482\t0.0154978939584\t\n",
      "DAI62779 - GRO94758\t479\t0.0154014340375\t\n",
      "DAI62779 - GRO21487\t471\t0.0151442075817\t\n",
      "GRO85051 - SNA80324\t471\t0.0151442075817\t\n",
      "ELE17451 - GRO30386\t468\t0.0150477476608\t\n",
      "FRO85978 - SNA95666\t463\t0.014886981126\t\n",
      "DAI62779 - FRO19221\t462\t0.014854827819\t\n",
      "DAI62779 - GRO46854\t461\t0.0148226745121\t\n",
      "DAI43223 - DAI62779\t459\t0.0147583678981\t\n",
      "ELE92920 - SNA18336\t455\t0.0146297546703\t\n",
      "DAI88079 - FRO40251\t446\t0.0143403749076\t\n",
      "cat: Unable to write to output stream.\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -cat {HDFS_DIR}/3.4/output2/* | head -n 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FRO40251 - GRO50832\t100\t0.00321533069676\t\n",
      "FRO40251 - GRO56989\t100\t0.00321533069676\t\n",
      "FRO78087 - GRO30386\t100\t0.00321533069676\t\n",
      "FRO78087 - GRO94758\t100\t0.00321533069676\t\n",
      "FRO80039 - GRO64900\t100\t0.00321533069676\t\n",
      "GRO38814 - SNA93860\t100\t0.00321533069676\t\n",
      "GRO46854 - SNA66583\t100\t0.00321533069676\t\n",
      "GRO59710 - SNA93860\t100\t0.00321533069676\t\n",
      "GRO64900 - SNA45677\t100\t0.00321533069676\t\n",
      "GRO73461 - GRO88511\t100\t0.00321533069676\t\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -cat {HDFS_DIR}/3.4/output2/* | tail -n 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### END STUDENT CODE HW34"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"3.5\"></a>\n",
    "## HW3.5: Stripes\n",
    "Repeat 3.4 using the stripes design pattern for finding cooccuring pairs.\n",
    "\n",
    "Report  the compute times for stripes job versus the Pairs job. Describe the computational setup used (E.g., single computer; dual core; linux, number of mappers, number of reducers)\n",
    "\n",
    "Instrument your mapper, combiner, and reducer to count how many times each is called using Counters and report these counts. Discuss the differences in these counts between the Pairs and Stripes jobs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### START STUDENT CODE HW35 (INSERT CELLS BELOW AS NEEDED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting stripesMapper_1.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile stripesMapper_1.py\n",
    "#!/usr/bin/env python\n",
    "from __future__ import division\n",
    "import sys\n",
    "import itertools\n",
    "import json\n",
    "\n",
    "############################################################\n",
    "# Using order inversion, we are able to count the total\n",
    "# number of baskets to send to the reducer for calculating\n",
    "# the relative frequencies.\n",
    "############################################################\n",
    "\n",
    "\n",
    "sys.stderr.write(\"reporter:counter:Mapper Counters,Calls,1\\n\")\n",
    "\n",
    "N_BASKETS = 0\n",
    "\n",
    "for line in sys.stdin:\n",
    "    line = line.strip()\n",
    "    words = line.split()\n",
    "    N_BASKETS += 1\n",
    "    H = {}\n",
    "    for subset in itertools.combinations(sorted(set(words)), 2):\n",
    "        if subset[0] not in H.keys():\n",
    "            H[subset[0]] = {}\n",
    "            H[subset[0]][subset[1]] = 1 \n",
    "        elif subset[1] not in H[subset[0]]:\n",
    "            H[subset[0]][subset[1]] = 1\n",
    "        else:\n",
    "            H[subset[0]][subset[1]] += 1 \n",
    "\n",
    "    for key in H.keys():\n",
    "        print \"%s\\t%s\" % (key, json.dumps(H[key]))\n",
    "            \n",
    "            \n",
    "print \"%s\\t%s\" % (\"!N_BASKETS\", N_BASKETS)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting stripesReducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile stripesReducer.py\n",
    "#!/usr/bin/env python\n",
    "from __future__ import division\n",
    "import sys\n",
    "import collections as cl\n",
    "import json\n",
    "\n",
    "sys.stderr.write(\"reporter:counter:Reducer Counters,Calls,1\\n\")\n",
    "\n",
    "N_BASKETS = 0\n",
    "cur_key = None\n",
    "cur_counter = cl.Counter()\n",
    "support_count = 100\n",
    "\n",
    "for line in sys.stdin:\n",
    "    line = line.strip()\n",
    "        \n",
    "    key,value = line.split(\"\\t\")\n",
    "    if key == \"!N_BASKETS\":\n",
    "        N_BASKETS += int(value)\n",
    "        \n",
    "    elif key == cur_key:\n",
    "        cur_counter.update(json.loads(value))\n",
    "    else:\n",
    "        if cur_key:\n",
    "            for k in cur_counter.keys():\n",
    "                print '%s\\t%s\\t%s' % (cur_key+\" - \"+k, cur_counter[k], str(cur_counter[k]/N_BASKETS))\n",
    "        cur_key = key\n",
    "        cur_counter = cl.Counter(json.loads(value))\n",
    "\n",
    "for k in cur_counter.keys():\n",
    "    print '%s\\t%s\\t%s' % (cur_key+\" - \"+k, cur_counter[k], str(cur_counter[k]/N_BASKETS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted /user/root/HW3/3.5/stripes1\n",
      "packageJobJar: [] [/usr/jars/hadoop-streaming-2.6.0-cdh5.7.0.jar] /tmp/streamjob4157279206657460218.jar tmpDir=null\n",
      "18/02/07 10:02:45 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "18/02/07 10:02:45 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "18/02/07 10:02:46 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "18/02/07 10:02:46 INFO mapreduce.JobSubmitter: number of splits:2\n",
      "18/02/07 10:02:46 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1517924926507_0033\n",
      "18/02/07 10:02:46 INFO impl.YarnClientImpl: Submitted application application_1517924926507_0033\n",
      "18/02/07 10:02:46 INFO mapreduce.Job: The url to track the job: http://docker.w261:8088/proxy/application_1517924926507_0033/\n",
      "18/02/07 10:02:46 INFO mapreduce.Job: Running job: job_1517924926507_0033\n",
      "18/02/07 10:02:52 INFO mapreduce.Job: Job job_1517924926507_0033 running in uber mode : false\n",
      "18/02/07 10:02:52 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "18/02/07 10:03:00 INFO mapreduce.Job:  map 50% reduce 0%\n",
      "18/02/07 10:03:01 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "18/02/07 10:03:11 INFO mapreduce.Job:  map 100% reduce 96%\n",
      "18/02/07 10:03:12 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "18/02/07 10:03:12 INFO mapreduce.Job: Job job_1517924926507_0033 completed successfully\n",
      "18/02/07 10:03:12 INFO mapreduce.Job: Counters: 51\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=42880784\n",
      "\t\tFILE: Number of bytes written=86115933\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=3462855\n",
      "\t\tHDFS: Number of bytes written=35081846\n",
      "\t\tHDFS: Number of read operations=9\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tData-local map tasks=2\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=11921\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=8963\n",
      "\t\tTotal time spent by all map tasks (ms)=11921\n",
      "\t\tTotal time spent by all reduce tasks (ms)=8963\n",
      "\t\tTotal vcore-seconds taken by all map tasks=11921\n",
      "\t\tTotal vcore-seconds taken by all reduce tasks=8963\n",
      "\t\tTotal megabyte-seconds taken by all map tasks=12207104\n",
      "\t\tTotal megabyte-seconds taken by all reduce tasks=9178112\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=31101\n",
      "\t\tMap output records=349722\n",
      "\t\tMap output bytes=42019250\n",
      "\t\tMap output materialized bytes=42880790\n",
      "\t\tInput split bytes=242\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=348160\n",
      "\t\tReduce shuffle bytes=42880790\n",
      "\t\tReduce input records=349722\n",
      "\t\tReduce output records=877095\n",
      "\t\tSpilled Records=699444\n",
      "\t\tShuffled Maps =2\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tGC time elapsed (ms)=127\n",
      "\t\tCPU time spent (ms)=15560\n",
      "\t\tPhysical memory (bytes) snapshot=1029402624\n",
      "\t\tVirtual memory (bytes) snapshot=4171120640\n",
      "\t\tTotal committed heap usage (bytes)=1620049920\n",
      "\tMapper Counters\n",
      "\t\tCalls=2\n",
      "\tReducer Counters\n",
      "\t\tCalls=1\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=3462613\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=35081846\n",
      "18/02/07 10:03:12 INFO streaming.StreamJob: Output directory: /user/root/HW3/3.5/stripes1\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "# Count\n",
    "start = time.time()\n",
    "\n",
    "!hdfs dfs -rm -r {HDFS_DIR}/3.5/stripes1\n",
    "\n",
    "!hadoop jar {JAR_FILE} \\\n",
    "    -D stream.num.map.output.key.field=3 \\\n",
    "    -D stream.map.output.field.separator=\"\\t\" \\\n",
    "    -D mapreduce.partition.keypartitioner.options=\"-k1,1\" \\\n",
    "    -D mapreduce.job.output.key.comparator.class=org.apache.hadoop.mapred.lib.KeyFieldBasedComparator \\\n",
    "    -D mapreduce.partition.keycomparator.options=\"-k1,1\" \\\n",
    "    -files stripesMapper_1.py,stripesReducer.py \\\n",
    "    -cmdenv PATH=/opt/anaconda/bin:$PATH \\\n",
    "    -mapper stripesMapper_1.py \\\n",
    "    -reducer stripesReducer.py \\\n",
    "    -input {HDFS_DIR}/ProductPurchaseData.txt -output {HDFS_DIR}/3.5/stripes1\n",
    "    \n",
    "end = time.time()\n",
    "countTime = end-start\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted /user/root/HW3/3.5/stripes2\n",
      "packageJobJar: [] [/usr/jars/hadoop-streaming-2.6.0-cdh5.7.0.jar] /tmp/streamjob8859313128735607306.jar tmpDir=null\n",
      "18/02/07 10:08:56 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "18/02/07 10:08:56 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "18/02/07 10:08:56 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "18/02/07 10:08:56 INFO mapreduce.JobSubmitter: number of splits:2\n",
      "18/02/07 10:08:57 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1517924926507_0035\n",
      "18/02/07 10:08:57 INFO impl.YarnClientImpl: Submitted application application_1517924926507_0035\n",
      "18/02/07 10:08:57 INFO mapreduce.Job: The url to track the job: http://docker.w261:8088/proxy/application_1517924926507_0035/\n",
      "18/02/07 10:08:57 INFO mapreduce.Job: Running job: job_1517924926507_0035\n",
      "18/02/07 10:09:02 INFO mapreduce.Job: Job job_1517924926507_0035 running in uber mode : false\n",
      "18/02/07 10:09:02 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "18/02/07 10:09:10 INFO mapreduce.Job:  map 50% reduce 0%\n",
      "18/02/07 10:09:12 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "18/02/07 10:09:17 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "18/02/07 10:09:17 INFO mapreduce.Job: Job job_1517924926507_0035 completed successfully\n",
      "18/02/07 10:09:17 INFO mapreduce.Job: Counters: 49\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=37713137\n",
      "\t\tFILE: Number of bytes written=75775500\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=35086184\n",
      "\t\tHDFS: Number of bytes written=35958941\n",
      "\t\tHDFS: Number of read operations=9\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tData-local map tasks=2\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=13240\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=4129\n",
      "\t\tTotal time spent by all map tasks (ms)=13240\n",
      "\t\tTotal time spent by all reduce tasks (ms)=4129\n",
      "\t\tTotal vcore-seconds taken by all map tasks=13240\n",
      "\t\tTotal vcore-seconds taken by all reduce tasks=4129\n",
      "\t\tTotal megabyte-seconds taken by all map tasks=13557760\n",
      "\t\tTotal megabyte-seconds taken by all reduce tasks=4228096\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=877095\n",
      "\t\tMap output records=877095\n",
      "\t\tMap output bytes=35958941\n",
      "\t\tMap output materialized bytes=37713143\n",
      "\t\tInput split bytes=242\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=877095\n",
      "\t\tReduce shuffle bytes=37713143\n",
      "\t\tReduce input records=877095\n",
      "\t\tReduce output records=877095\n",
      "\t\tSpilled Records=1754190\n",
      "\t\tShuffled Maps =2\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tGC time elapsed (ms)=190\n",
      "\t\tCPU time spent (ms)=16120\n",
      "\t\tPhysical memory (bytes) snapshot=1337901056\n",
      "\t\tVirtual memory (bytes) snapshot=4167864320\n",
      "\t\tTotal committed heap usage (bytes)=1825046528\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=35085942\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=35958941\n",
      "18/02/07 10:09:17 INFO streaming.StreamJob: Output directory: /user/root/HW3/3.5/stripes2\n"
     ]
    }
   ],
   "source": [
    "# Sort\n",
    "start = time.time()   \n",
    "\n",
    "!hdfs dfs -rm -r {HDFS_DIR}/3.5/stripes2\n",
    "\n",
    "!hadoop jar {JAR_FILE} \\\n",
    "  -D stream.num.map.output.key.field=2 \\\n",
    "  -D stream.map.output.field.separator=\"\\t\" \\\n",
    "  -D mapreduce.partition.keypartitioner.options=\"-k1,1\" \\\n",
    "  -D mapreduce.job.output.key.comparator.class=org.apache.hadoop.mapred.lib.KeyFieldBasedComparator \\\n",
    "  -D mapreduce.partition.keycomparator.options=\"-k2,2nr -k1,1\" \\\n",
    "  -mapper /bin/cat \\\n",
    "  -reducer /bin/cat \\\n",
    "  -input {HDFS_DIR}/3.5/stripes1/* \\\n",
    "  -output {HDFS_DIR}/3.5/stripes2\n",
    "\n",
    "end = time.time()\n",
    "sortTime = end-start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------  Time to count and sort stripes  --------\n",
      "\n",
      "Count time:\t 31.682747 sec\n",
      "Sort time:\t 25.170172 sec\n",
      "Total time:\t 56.852919 sec \n",
      "\n",
      "------------- Most frequent pairs --------------\n",
      "\n",
      "Pair                Support count     Support \n",
      "\n",
      "DAI62779 - ELE17451\t1592\t0.0511880646925\t\n",
      "FRO40251 - SNA80324\t1412\t0.0454004694383\t\n",
      "DAI75645 - FRO40251\t1254\t0.0403202469374\t\n",
      "FRO40251 - GRO85051\t1213\t0.0390019613517\t\n",
      "DAI62779 - GRO73461\t1139\t0.0366226166361\t\n",
      "DAI75645 - SNA80324\t1130\t0.0363332368734\t\n",
      "DAI62779 - FRO40251\t1070\t0.0344040384554\t\n",
      "DAI62779 - SNA80324\t923\t0.0296775023311\t\n",
      "DAI62779 - DAI85309\t918\t0.0295167357963\t\n",
      "ELE32164 - GRO59710\t911\t0.0292916626475\t\n",
      "DAI62779 - DAI75645\t882\t0.0283592167454\t\n",
      "FRO40251 - GRO73461\t882\t0.0283592167454\t\n",
      "DAI62779 - ELE92920\t877\t0.0281984502106\t\n",
      "FRO40251 - FRO92469\t835\t0.026848011318\t\n",
      "DAI62779 - ELE32164\t832\t0.0267515513971\t\n",
      "DAI75645 - GRO73461\t712\t0.0228931545609\t\n",
      "DAI43223 - ELE32164\t711\t0.022861001254\t\n",
      "DAI62779 - GRO30386\t709\t0.02279669464\t\n",
      "ELE17451 - FRO40251\t697\t0.0224108549564\t\n",
      "DAI85309 - ELE99737\t659\t0.0211890292917\t\n",
      "DAI62779 - ELE26917\t650\t0.020899649529\t\n",
      "GRO21487 - GRO73461\t631\t0.0202887366966\t\n",
      "DAI62779 - SNA45677\t604\t0.0194205974084\t\n",
      "ELE17451 - SNA80324\t597\t0.0191955242597\t\n",
      "DAI62779 - GRO71621\t595\t0.0191312176457\t\n",
      "DAI62779 - SNA55762\t593\t0.0190669110318\t\n",
      "DAI62779 - DAI83733\t586\t0.018841837883\t\n",
      "ELE17451 - GRO73461\t580\t0.0186489180412\t\n",
      "GRO73461 - SNA80324\t562\t0.0180701585158\t\n",
      "DAI62779 - GRO59710\t561\t0.0180380052088\t\n",
      "DAI62779 - FRO80039\t550\t0.0176843188322\t\n",
      "DAI75645 - ELE17451\t547\t0.0175878589113\t\n",
      "DAI62779 - SNA93860\t537\t0.0172663258416\t\n",
      "DAI55148 - DAI62779\t526\t0.016912639465\t\n",
      "DAI43223 - GRO59710\t512\t0.0164624931674\t\n",
      "ELE17451 - ELE32164\t511\t0.0164303398605\t\n",
      "DAI62779 - SNA18336\t506\t0.0162695733256\t\n",
      "ELE32164 - GRO73461\t486\t0.0156265071863\t\n",
      "DAI62779 - FRO78087\t482\t0.0154978939584\t\n",
      "DAI85309 - ELE17451\t482\t0.0154978939584\t\n",
      "DAI62779 - GRO94758\t479\t0.0154014340375\t\n",
      "DAI62779 - GRO21487\t471\t0.0151442075817\t\n",
      "GRO85051 - SNA80324\t471\t0.0151442075817\t\n",
      "ELE17451 - GRO30386\t468\t0.0150477476608\t\n",
      "FRO85978 - SNA95666\t463\t0.014886981126\t\n",
      "DAI62779 - FRO19221\t462\t0.014854827819\t\n",
      "DAI62779 - GRO46854\t461\t0.0148226745121\t\n",
      "DAI43223 - DAI62779\t459\t0.0147583678981\t\n",
      "ELE92920 - SNA18336\t455\t0.0146297546703\t\n",
      "DAI88079 - FRO40251\t446\t0.0143403749076\t\n",
      "cat: Unable to write to output stream.\n"
     ]
    }
   ],
   "source": [
    "print \"-------  Time to count and sort stripes  --------\\n\"\n",
    "print \"Count time:\\t %f sec\\nSort time:\\t %f sec\\nTotal time:\\t %f sec \" % (countTime,sortTime,countTime+sortTime)\n",
    "print \"\\n------------- Most frequent pairs --------------\\n\"\n",
    "print \"Pair                Support count     Support \\n\"\n",
    "!hdfs dfs -cat {HDFS_DIR}/3.5/stripes2/part-00000 | head -n 50\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### END STUDENT CODE HW35"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OPTIONAL\n",
    "QUESTIONS  BELOW THIS LINE ARE OPTIONAL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"3.6\"></a>\n",
    "## HW3.6 Computing Relative Frequencies on 100K WikiPedia pages (93Meg)\n",
    "\n",
    "Dataset description\n",
    "For this assignment you will explore a set of 100,000 Wikipedia documents:\n",
    "\n",
    "https://www.dropbox.com/s/n5lfbnztclo93ej/wikitext_100k.txt?dl=0\n",
    "s3://cs9223/wikitext_100k.txt, or\n",
    "https://s3.amazonaws.com/cs9223/wikitext_100k.txt\n",
    "Each line in this file consists of the plain text extracted from a Wikipedia document.\n",
    "\n",
    "Task\n",
    "Compute the relative frequencies of each word that occurs in the documents in wikitext_100k.txt and output the top 100 word pairs sorted by decreasing order of relative frequency.\n",
    "\n",
    "Recall that the relative frequency (RF) of word B given word A is defined as follows:\n",
    "\n",
    "   f(B|A) = Count(A, B) / Count (A)   =  Count(A, B) / sum_B'(Count (A, B')\n",
    "\n",
    "where count(A,B) is the number of times A and B co-occur within a window of two words (co-occurrence window size of two) in a document and count(A) the number of times A occurs with anything else. Intuitively, given a document collection, the relative frequency captures the proportion of time the word B appears in the same document as A. (See Section 3.3, in Data-Intensive Text Processing with MapReduce).\n",
    "\n",
    "In the async lecture you learned different approaches to do this, and in this assignment, you will implement them:\n",
    "\n",
    "a.\tWrite a mapreduce program which uses the Stripes approach and writes its output in a file named rfstripes.txt \n",
    "\n",
    "b.\tWrite a mapreduce program which uses the Pairs approach and writes its output in a file named rfpairs.txt\n",
    "\n",
    "c.\tCompare the performance of the two approaches and output the relative performance to a file named rfcomp.txt. Compute the relative performance as follows: (running time for Pairs/ running time for Stripes). Also include an analysis comparing the communication costs for the two approaches. Instrument your mapper and reduces for counters where necessary to aid with your analysis.\n",
    "\n",
    "NOTE: please limit your analysis to the top 100 word pairs sorted by decreasing order of relative frequency for each word (tokens with all alphabetical letters).\n",
    "\n",
    "Please include markdown cell named rf.txt that describes the following:\n",
    "\n",
    "the input/output format in each Hadoop task, i.e., the keys for the mappers and reducers\n",
    "the Hadoop cluster settings you used, i.e., number of mappers and reducers\n",
    "the running time for each approach: pairs and stripes\n",
    "\n",
    "You can write your program using Python or MrJob (with Hadoop streaming) and you should run it on AWS. It is a good idea to develop and test your program on a local machine  before deploying on AWS. Remember your notebook, needs to have all the commands you used to run each Mapreduce job (i.e., pairs and stripes) -- include the Hadoop streaming commands you used to run your jobs.\n",
    "\n",
    "In addition the All the following files should be compressed in one ZIP file and submitted. The ZIP file should contain:\n",
    "\n",
    "\n",
    "A.\tThe result files: rfstripes.txt, rfpairs.txt, rfcomp.txt\n",
    "\n",
    "Prior to working with Hadoop, the corpus should first be preprocessed as follows:\n",
    "perform tokenization (whitespace and all non-alphabetic characters) and stopword removal  using standard tools from the Lucene search engine. All tokens should  then be replaced\n",
    "with unique integers for a more efficient encoding. \n",
    "\n",
    "\n",
    "== Preliminary information for the remaing HW problems===\n",
    "\n",
    "Much of this homework beyond this point will focus on the Apriori algorithm for frequent itemset  mining and the additional step for extracting association rules from these frequent itemsets.\n",
    "Please acquaint yourself with the background information (below)\n",
    "before approaching the remaining  assignments.\n",
    "\n",
    "=== Apriori background information ===\n",
    "\n",
    "Some background material for the  Apriori algorithm is located at:\n",
    "\n",
    " - Slides in Live Session #3\n",
    " - https://en.wikipedia.org/wiki/Apriori_algorithm\n",
    " - https://www.dropbox.com/s/k2zm4otych279z2/Apriori-good-slides.pdf?dl=0\n",
    " - http://snap.stanford.edu/class/cs246-2014/slides/02-assocrules.pdf\n",
    "\n",
    "Association Rules are frequently used for Market Basket Analysis (MBA) by retailers to\n",
    "understand the purchase behavior of their customers. This information can be then used for\n",
    "many different purposes such as cross-selling and up-selling of products, sales promotions,\n",
    "loyalty programs, store design, discount plans and many others.\n",
    "Evaluation of item sets: Once you have found the frequent itemsets of a dataset, you need\n",
    "to choose a subset of them as your recommendations. Commonly used metrics for measuring\n",
    "significance and interest for selecting rules for recommendations are: confidence; lift; and conviction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"3.8\"></a>\n",
    "## HW3.8. Shopping Cart Analysis\n",
    "Product Recommendations: The action or practice of selling additional products or services \n",
    "to existing customers is called cross-selling. Giving product recommendation is \n",
    "one of the examples of cross-selling that are frequently used by online retailers. \n",
    "One simple method to give product recommendations is to recommend products that are frequently\n",
    "browsed together by the customers.\n",
    "\n",
    "Suppose we want to recommend new products to the customer based on the products they\n",
    "have already browsed on the online website. Write a program using the A-priori algorithm\n",
    "to find products which are frequently browsed together. Fix the support to s = 100 \n",
    "(i.e. product sets need to occur together at least 100 times to be considered frequent) \n",
    "and find itemsets of size 2 and 3.\n",
    "\n",
    "Then extract association rules from these frequent items. \n",
    "\n",
    "A rule is of the form: \n",
    "\n",
    "(item1, item5)  item2.\n",
    "\n",
    "List the top 10 discovered rules in descreasing order of confidence in the following format\n",
    " \n",
    "(item1, item5)  item2, supportCount ,support, confidence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"3.8.1\"></a>\n",
    "## HW3.8.1\n",
    "\n",
    "Benchmark your results using the pyFIM implementation of the Apriori algorithm\n",
    "(Apriori - Association Rule Induction / Frequent Item Set Mining implemented by Christian Borgelt). \n",
    "You can download pyFIM from here: \n",
    "\n",
    "http://www.borgelt.net/pyfim.html\n",
    "\n",
    "Comment on the results from both implementations (your Hadoop MapReduce of apriori versus pyFIM) \n",
    "in terms of results and execution times."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "END OF HOMEWORK\n",
    "==============="
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "512px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": false,
   "sideBar": false,
   "threshold": 4,
   "toc_cell": true,
   "toc_position": {
    "height": "40px",
    "left": "231px",
    "right": "20px",
    "top": "126px",
    "width": "372px"
   },
   "toc_section_display": "none",
   "toc_window_display": true,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
