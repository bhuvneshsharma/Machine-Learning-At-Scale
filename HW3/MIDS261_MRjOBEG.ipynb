{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Homework 3 and 4 - Applications Using MRJob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# general imports\n",
    "import os\n",
    "import re\n",
    "import sys\n",
    "import time\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# tell matplotlib not to open a new window\n",
    "%matplotlib inline\n",
    "\n",
    "# automatically reload modules \n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python Version: 2.7.14 \n",
      "HDFS filesystem running at: \n",
      "\t hdfs://quickstart.cloudera:8020\n"
     ]
    }
   ],
   "source": [
    "# print some configuration details for future replicability.\n",
    "print 'Python Version: %s' % (sys.version.split('|')[0])\n",
    "hdfs_conf = !hdfs getconf -confKey fs.defaultFS ### UNCOMMENT ON DOCKER\n",
    "#hdfs_conf = !hdfs getconf -confKey fs.default.name ### UNCOMMENT ON ALTISCALE\n",
    "print 'HDFS filesystem running at: \\n\\t %s' % (hdfs_conf[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "JAR_FILE = \"/usr/lib/hadoop-mapreduce/hadoop-streaming-2.6.0-cdh5.7.0.jar\"\n",
    "HDFS_DIR = \"/user/root/HW3\"\n",
    "HOME_DIR = \"/media/notebooks/SP18-1-maynard242\" # FILL IN HERE eg. /media/notebooks/w261-main/Assignments\n",
    "# save path for use in Hadoop jobs (-cmdenv PATH={PATH})\n",
    "from os import environ\n",
    "PATH  = environ['PATH']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#!hdfs dfs -mkdir HW3\n",
    "!hdfs dfs -ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting example1.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile example1.txt\n",
    "Unix,30\n",
    "Solaris,10\n",
    "Linux,25\n",
    "Linux,20\n",
    "HPUX,100\n",
    "AIX,25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting example2.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile example2.txt\n",
    "foo foo quux labs foo bar jimi quux jimi jimi\n",
    "foo  jimi jimi\n",
    "data mining is data science\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting WordCount.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile WordCount.py\n",
    "\n",
    "from mrjob.job import MRJob\n",
    "import re\n",
    " \n",
    "WORD_RE = re.compile(r\"[\\w']+\")\n",
    " \n",
    "class MRWordFreqCount(MRJob):\n",
    "    def mapper(self, _, line):\n",
    "        for word in WORD_RE.findall(line):\n",
    "            yield word.lower(), 1\n",
    "     \n",
    "    def combiner(self, word, counts):\n",
    "        yield word, sum(counts)\n",
    "\n",
    "    #hello, (1,1,1,1,1,1): using a combiner? NO and YEs\n",
    "    def reducer(self, word, counts):\n",
    "        yield word, sum(counts)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    MRWordFreqCount.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No configs found; falling back on auto-configuration\n",
      "No configs specified for hadoop runner\n",
      "Looking for hadoop binary in $PATH...\n",
      "Found hadoop binary: /usr/bin/hadoop\n",
      "Using Hadoop version 2.6.0\n",
      "Looking for Hadoop streaming jar in /home/hadoop/contrib...\n",
      "Looking for Hadoop streaming jar in /usr/lib/hadoop-mapreduce...\n",
      "Found Hadoop streaming jar: /usr/lib/hadoop-mapreduce/hadoop-streaming.jar\n",
      "Creating temp directory /tmp/WordCount.root.20180219.120016.451572\n",
      "Copying local files to hdfs:///user/root/tmp/mrjob/WordCount.root.20180219.120016.451572/files/...\n",
      "Running step 1 of 1...\n",
      "  packageJobJar: [] [/usr/jars/hadoop-streaming-2.6.0-cdh5.7.0.jar] /tmp/streamjob8325369025906361369.jar tmpDir=null\n",
      "  Connecting to ResourceManager at /0.0.0.0:8032\n",
      "  Connecting to ResourceManager at /0.0.0.0:8032\n",
      "  Total input paths to process : 1\n",
      "  number of splits:2\n",
      "  Submitting tokens for job: job_1519041572493_0001\n",
      "  Submitted application application_1519041572493_0001\n",
      "  The url to track the job: http://docker.w261:8088/proxy/application_1519041572493_0001/\n",
      "  Running job: job_1519041572493_0001\n",
      "  Job job_1519041572493_0001 running in uber mode : false\n",
      "   map 0% reduce 0%\n",
      "   map 50% reduce 0%\n",
      "   map 100% reduce 0%\n",
      "   map 100% reduce 100%\n",
      "  Job job_1519041572493_0001 completed successfully\n",
      "  Output directory: hdfs:///user/root/tmp/mrjob/WordCount.root.20180219.120016.451572/output\n",
      "Counters: 49\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=132\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=82\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=127\n",
      "\t\tFILE: Number of bytes written=355519\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=452\n",
      "\t\tHDFS: Number of bytes written=82\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of read operations=9\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tData-local map tasks=2\n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tTotal megabyte-seconds taken by all map tasks=5013504\n",
      "\t\tTotal megabyte-seconds taken by all reduce tasks=2598912\n",
      "\t\tTotal time spent by all map tasks (ms)=4896\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=4896\n",
      "\t\tTotal time spent by all reduce tasks (ms)=2538\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=2538\n",
      "\t\tTotal vcore-seconds taken by all map tasks=4896\n",
      "\t\tTotal vcore-seconds taken by all reduce tasks=2538\n",
      "\tMap-Reduce Framework\n",
      "\t\tCPU time spent (ms)=1790\n",
      "\t\tCombine input records=18\n",
      "\t\tCombine output records=11\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tGC time elapsed (ms)=36\n",
      "\t\tInput split bytes=320\n",
      "\t\tMap input records=3\n",
      "\t\tMap output bytes=160\n",
      "\t\tMap output materialized bytes=133\n",
      "\t\tMap output records=18\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tPhysical memory (bytes) snapshot=927424512\n",
      "\t\tReduce input groups=9\n",
      "\t\tReduce input records=11\n",
      "\t\tReduce output records=9\n",
      "\t\tReduce shuffle bytes=133\n",
      "\t\tShuffled Maps =2\n",
      "\t\tSpilled Records=22\n",
      "\t\tTotal committed heap usage (bytes)=1513095168\n",
      "\t\tVirtual memory (bytes) snapshot=4241608704\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "Streaming final output from hdfs:///user/root/tmp/mrjob/WordCount.root.20180219.120016.451572/output...\n",
      "\"bar\"\t1\n",
      "\"data\"\t2\n",
      "\"foo\"\t4\n",
      "\"is\"\t1\n",
      "\"jimi\"\t5\n",
      "\"labs\"\t1\n",
      "\"mining\"\t1\n",
      "\"quux\"\t2\n",
      "\"science\"\t1\n",
      "Removing HDFS temp directory hdfs:///user/root/tmp/mrjob/WordCount.root.20180219.120016.451572...\n",
      "Removing temp directory /tmp/WordCount.root.20180219.120016.451572...\n"
     ]
    }
   ],
   "source": [
    "!python WordCount.py -r hadoop --cmdenv PATH=/opt/anaconda/bin:$PATH example2.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(u'jimi', 5)\n",
      "(u'quux', 2)\n",
      "(u'science', 1)\n",
      "(u'foo', 4)\n",
      "(u'bar', 1)\n",
      "(u'data', 2)\n",
      "(u'is', 1)\n",
      "(u'labs', 1)\n",
      "(u'mining', 1)\n"
     ]
    }
   ],
   "source": [
    "from WordCount import MRWordFreqCount\n",
    "mr_job = MRWordFreqCount(args=['example2.txt'])\n",
    "with mr_job.make_runner() as runner: \n",
    "    runner.run()\n",
    "    # stream_output: get access of the output \n",
    "    for line in runner.stream_output():\n",
    "        print mr_job.parse_output_line(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting AnotherWordCount.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile AnotherWordCount.py\n",
    "\n",
    "from mrjob.job import MRJob\n",
    "\n",
    "class MRAnotherWordCount(MRJob):\n",
    "    \n",
    "    def mapper (self,_,line):\n",
    "        yield \"chars\", len(line)\n",
    "        yield \"words\", len(line.split())\n",
    "        yield 'lines', 1\n",
    "        \n",
    "    def reducer (self, key, values):\n",
    "        yield key, sum(values)\n",
    "        \n",
    "if __name__ == '__main__':\n",
    "    MRAnotherWordCount.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No configs found; falling back on auto-configuration\n",
      "No configs specified for inline runner\n",
      "Running step 1 of 1...\n",
      "Creating temp directory /tmp/AnotherWordCount.root.20180203.141743.462604\n",
      "Streaming final output from /tmp/AnotherWordCount.root.20180203.141743.462604/output...\n",
      "\"words\"\t18\n",
      "\"chars\"\t86\n",
      "\"lines\"\t3\n",
      "Removing temp directory /tmp/AnotherWordCount.root.20180203.141743.462604...\n"
     ]
    }
   ],
   "source": [
    "!python AnotherWordCount.py example2.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing AnotherWC3.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile AnotherWC3.py\n",
    "# Copyright 2009-2010 Yelp\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "# http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "\n",
    "\"\"\"An implementation of wc as an MRJob.\n",
    "This is meant as an example of why mapper_final is useful.\"\"\"\n",
    "from mrjob.job import MRJob\n",
    "\n",
    "\n",
    "class MRWordCountUtility(MRJob):\n",
    "\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super(MRWordCountUtility, self).__init__(*args, **kwargs)\n",
    "        self.chars = 0\n",
    "        self.words = 0\n",
    "        self.lines = 0\n",
    "\n",
    "    def mapper(self, _, line):\n",
    "        # Don't actually yield anything for each line. Instead, collect them\n",
    "        # and yield the sums when all lines have been processed. The results\n",
    "        # will be collected by the reducer.\n",
    "        self.chars += len(line) + 1  # +1 for newline\n",
    "        self.words += sum(1 for word in line.split() if word.strip())\n",
    "        self.lines += 1\n",
    "\n",
    "    def mapper_final(self):\n",
    "        yield('chars', self.chars)\n",
    "        yield('words', self.words)\n",
    "        yield('lines', self.lines)\n",
    "\n",
    "    def reducer(self, key, values):\n",
    "        yield(key, sum(values))\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    MRWordCountUtility.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No configs found; falling back on auto-configuration\n",
      "No configs specified for hadoop runner\n",
      "Looking for hadoop binary in $PATH...\n",
      "Found hadoop binary: /usr/bin/hadoop\n",
      "Using Hadoop version 2.6.0\n",
      "Looking for Hadoop streaming jar in /home/hadoop/contrib...\n",
      "Looking for Hadoop streaming jar in /usr/lib/hadoop-mapreduce...\n",
      "Found Hadoop streaming jar: /usr/lib/hadoop-mapreduce/hadoop-streaming.jar\n",
      "Creating temp directory /tmp/AnotherWC3.root.20180210.114605.598768\n",
      "Copying local files to hdfs:///user/root/tmp/mrjob/AnotherWC3.root.20180210.114605.598768/files/...\n",
      "Running step 1 of 1...\n",
      "  packageJobJar: [] [/usr/jars/hadoop-streaming-2.6.0-cdh5.7.0.jar] /tmp/streamjob6426766568329965202.jar tmpDir=null\n",
      "  Connecting to ResourceManager at /0.0.0.0:8032\n",
      "  Connecting to ResourceManager at /0.0.0.0:8032\n",
      "  Total input paths to process : 1\n",
      "  number of splits:2\n",
      "  Submitting tokens for job: job_1518262877936_0001\n",
      "  Submitted application application_1518262877936_0001\n",
      "  The url to track the job: http://docker.w261:8088/proxy/application_1518262877936_0001/\n",
      "  Running job: job_1518262877936_0001\n",
      "  Job job_1518262877936_0001 running in uber mode : false\n",
      "   map 0% reduce 0%\n",
      "   map 50% reduce 0%\n",
      "   map 100% reduce 0%\n",
      "   map 100% reduce 100%\n",
      "  Job job_1518262877936_0001 completed successfully\n",
      "  Output directory: hdfs:///user/root/tmp/mrjob/AnotherWC3.root.20180210.114605.598768/output\n",
      "Counters: 49\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=132\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=32\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=81\n",
      "\t\tFILE: Number of bytes written=354347\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=454\n",
      "\t\tHDFS: Number of bytes written=32\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of read operations=9\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tData-local map tasks=2\n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tTotal megabyte-seconds taken by all map tasks=4611072\n",
      "\t\tTotal megabyte-seconds taken by all reduce tasks=2557952\n",
      "\t\tTotal time spent by all map tasks (ms)=4503\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=4503\n",
      "\t\tTotal time spent by all reduce tasks (ms)=2498\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=2498\n",
      "\t\tTotal vcore-seconds taken by all map tasks=4503\n",
      "\t\tTotal vcore-seconds taken by all reduce tasks=2498\n",
      "\tMap-Reduce Framework\n",
      "\t\tCPU time spent (ms)=1470\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tGC time elapsed (ms)=43\n",
      "\t\tInput split bytes=322\n",
      "\t\tMap input records=3\n",
      "\t\tMap output bytes=63\n",
      "\t\tMap output materialized bytes=87\n",
      "\t\tMap output records=6\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tPhysical memory (bytes) snapshot=928542720\n",
      "\t\tReduce input groups=3\n",
      "\t\tReduce input records=6\n",
      "\t\tReduce output records=3\n",
      "\t\tReduce shuffle bytes=87\n",
      "\t\tShuffled Maps =2\n",
      "\t\tSpilled Records=12\n",
      "\t\tTotal committed heap usage (bytes)=1513095168\n",
      "\t\tVirtual memory (bytes) snapshot=4245602304\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "Streaming final output from hdfs:///user/root/tmp/mrjob/AnotherWC3.root.20180210.114605.598768/output...\n",
      "\"chars\"\t89\n",
      "\"lines\"\t3\n",
      "\"words\"\t18\n",
      "Removing HDFS temp directory hdfs:///user/root/tmp/mrjob/AnotherWC3.root.20180210.114605.598768...\n",
      "Removing temp directory /tmp/AnotherWC3.root.20180210.114605.598768...\n"
     ]
    }
   ],
   "source": [
    "!python AnotherWC3.py -r hadoop --cmdenv PATH=/opt/anaconda/bin:$PATH example2.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting AnotherWC2.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile AnotherWC2.py\n",
    "\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "import re\n",
    "\n",
    "WORD_RE = re.compile(r\"[\\w']+\")\n",
    "\n",
    "\n",
    "class MRMostUsedWord(MRJob):\n",
    "\n",
    "    def steps(self):\n",
    "        return [\n",
    "            MRStep(mapper=self.mapper_get_words,\n",
    "                   combiner=self.combiner_count_words,\n",
    "                   reducer=self.reducer_count_words),\n",
    "            MRStep(reducer=self.reducer_find_max_word)\n",
    "        ]\n",
    "\n",
    "    def mapper_get_words(self, _, line):\n",
    "        # yield each word in the line\n",
    "        for word in WORD_RE.findall(line):\n",
    "            self.increment_counter('group', 'counter_name', 1)\n",
    "            yield (word.lower(), 1)\n",
    "\n",
    "    def combiner_count_words(self, word, counts):\n",
    "        # optimization: sum the words we've seen so far\n",
    "        yield (word, sum(counts))\n",
    "\n",
    "    def reducer_count_words(self, word, counts):\n",
    "        # send all (num_occurrences, word) pairs to the same reducer.\n",
    "        # num_occurrences is so we can easily use Python's max() function.\n",
    "        yield None, (sum(counts), word)\n",
    "\n",
    "    # discard the key; it is just None\n",
    "    def reducer_find_max_word(self, _, word_count_pairs):\n",
    "        # each item of word_count_pairs is (count, word),\n",
    "        # so yielding one results in key=counts, value=word\n",
    "        yield max(word_count_pairs)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    MRMostUsedWord.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No configs found; falling back on auto-configuration\n",
      "No configs specified for inline runner\n",
      "Running step 1 of 1...\n",
      "Creating temp directory /tmp/WordCount.root.20180203.150000.135247\n",
      "Streaming final output from mrJobOutput...\n",
      "\"labs\"\t1\n",
      "\"jimi\"\t5\n",
      "\"foo\"\t4\n",
      "\"science\"\t1\n",
      "\"quux\"\t2\n",
      "\"bar\"\t1\n",
      "\"mining\"\t1\n",
      "\"is\"\t1\n",
      "\"data\"\t2\n",
      "Removing temp directory /tmp/WordCount.root.20180203.150000.135247...\n"
     ]
    }
   ],
   "source": [
    "!python WordCount.py example2.txt --output-dir mrJobOutput"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 44\n",
      "4 drwxr-xr-x 2 root root 4096 Feb  3 14:52 .\n",
      "4 drwxrwxr-x 4 1000 1000 4096 Feb  3 15:16 ..\n",
      "4 -rw-r--r-- 1 root root    8 Feb  3 15:22 part-00000\n",
      "4 -rw-r--r-- 1 root root    9 Feb  3 15:22 part-00001\n",
      "4 -rw-r--r-- 1 root root    8 Feb  3 15:22 part-00002\n",
      "4 -rw-r--r-- 1 root root    7 Feb  3 15:22 part-00003\n",
      "4 -rw-r--r-- 1 root root    9 Feb  3 15:22 part-00004\n",
      "4 -rw-r--r-- 1 root root    9 Feb  3 15:22 part-00005\n",
      "4 -rw-r--r-- 1 root root   11 Feb  3 15:22 part-00006\n",
      "4 -rw-r--r-- 1 root root    9 Feb  3 15:22 part-00007\n",
      "4 -rw-r--r-- 1 root root   12 Feb  3 15:22 part-00008\n",
      "0 -rw-r--r-- 1 root root    0 Feb  3 15:22 part-00009\n",
      "\"bar\"\t1\n",
      "\"data\"\t2\n",
      "\"foo\"\t4\n",
      "\"is\"\t1\n",
      "\"jimi\"\t5\n",
      "\"labs\"\t1\n",
      "\"mining\"\t1\n",
      "\"quux\"\t2\n",
      "\"science\"\t1\n"
     ]
    }
   ],
   "source": [
    "!ls -las mrJobOutput/\n",
    "!cat mrJobOutput/part-0000*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting WordCount2.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile WordCount2.py\n",
    "\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "import re\n",
    " \n",
    "WORD_RE = re.compile(r\"[\\w']+\")\n",
    " \n",
    "class MRWordFreqCount(MRJob):\n",
    "    SORT_VALUES = True\n",
    "    def mapper(self, _, line):\n",
    "        for word in WORD_RE.findall(line):\n",
    "            self.increment_counter('group', 'mapper', 1)\n",
    "            yield word.lower(), 1\n",
    "            \n",
    "    def jobconfqqqq(self):  #assume we had second job to sort the word counts in decreasing order of counts\n",
    "        orig_jobconf = super(MRWordFreqCount, self).jobconf()        \n",
    "    'mapred.reduce.tasks': '1',\n",
    "        }\n",
    "        combined_jobconf = orig_jobconf\n",
    "        combined_jobconf.update(custom_jobconf)\n",
    "        self.jobconf = combined_jobconf\n",
    "        return combined_jobconf      custom_jobconf = {  #key value pairs\n",
    "            'mapred.output.key.comparator.class': 'org.apache.hadoop.mapred.lib.KeyFieldBasedComparator',\n",
    "            'mapred.text.key.comparator.options': '-k2,2nr',\n",
    "          \n",
    "\n",
    "\n",
    "    def combiner(self, word, counts):\n",
    "        self.increment_counter('group', 'combiner', 1)\n",
    "        yield word, sum(counts)\n",
    "\n",
    "    def reducer(self, word, counts):\n",
    "        self.increment_counter('group', 'reducer', 1)\n",
    "        yield word, sum(counts)\n",
    "\n",
    "    def steps(self):\n",
    "        return [MRStep(\n",
    "                mapper = self.mapper, \n",
    "                combiner = self.combiner,\n",
    "                reducer = self.reducer,\n",
    "                #,\n",
    "#                jobconf = self.jobconfqqqq\n",
    " \n",
    "#            jobconf = {'mapred.output.key.comparator.class': 'org.apache.hadoop.mapred.lib.KeyFieldBasedComparator',\n",
    "#                       'mapred.text.key.comparator.options':'-k1r',\n",
    "#                       'mapred.reduce.tasks' : 1}   \n",
    "       \n",
    "        \n",
    "            )]\n",
    "     \n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    MRWordFreqCount.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No configs found; falling back on auto-configuration\n",
      "No configs specified for inline runner\n",
      "Running step 1 of 1...\n",
      "Creating temp directory /tmp/WordCount2.root.20180203.152437.411298\n",
      "\n",
      "Counters: 3\n",
      "\tgroup\n",
      "\t\tcombiner=11\n",
      "\t\tmapper=18\n",
      "\t\treducer=9\n",
      "\n",
      "Streaming final output from mrJobOutput...\n",
      "\"labs\"\t1\n",
      "\"jimi\"\t5\n",
      "\"foo\"\t4\n",
      "\"science\"\t1\n",
      "\"quux\"\t2\n",
      "\"bar\"\t1\n",
      "\"mining\"\t1\n",
      "\"is\"\t1\n",
      "\"data\"\t2\n",
      "Removing temp directory /tmp/WordCount2.root.20180203.152437.411298...\n"
     ]
    }
   ],
   "source": [
    "!python WordCount2.py --jobconf numReduceTasks=1 example2.txt --output-dir mrJobOutput"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 44\n",
      "4 drwxr-xr-x 2 root root 4096 Feb  3 14:52 .\n",
      "4 drwxrwxr-x 4 1000 1000 4096 Feb  3 15:22 ..\n",
      "4 -rw-r--r-- 1 root root    8 Feb  3 15:23 part-00000\n",
      "4 -rw-r--r-- 1 root root    9 Feb  3 15:23 part-00001\n",
      "4 -rw-r--r-- 1 root root    8 Feb  3 15:23 part-00002\n",
      "4 -rw-r--r-- 1 root root    7 Feb  3 15:23 part-00003\n",
      "4 -rw-r--r-- 1 root root    9 Feb  3 15:23 part-00004\n",
      "4 -rw-r--r-- 1 root root    9 Feb  3 15:23 part-00005\n",
      "4 -rw-r--r-- 1 root root   11 Feb  3 15:23 part-00006\n",
      "4 -rw-r--r-- 1 root root    9 Feb  3 15:23 part-00007\n",
      "4 -rw-r--r-- 1 root root   12 Feb  3 15:23 part-00008\n",
      "0 -rw-r--r-- 1 root root    0 Feb  3 15:23 part-00009\n"
     ]
    }
   ],
   "source": [
    "!ls -las mrJobOutput/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate Relative Frequency and Sort by TOP and BOTTOM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile WordCount3.3.py\n",
    "\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "import re\n",
    "\n",
    "WORD_RE = re.compile(r\"[\\w']+\")\n",
    "\n",
    "\n",
    "class MRWordCount33(MRJob):\n",
    "\n",
    "    def steps(self):\n",
    "        return [\n",
    "            MRStep(mapper=self.mapper_get_words,\n",
    "                   combiner=self.combiner_count_words,\n",
    "                   reducer=self.reducer_count_words),\n",
    "            MRStep(reducer=self.reducer_find_max_word)\n",
    "        ]\n",
    "\n",
    "    def mapper_get_words(self, _, line):\n",
    "        for word in WORD_RE.findall(line):\n",
    "            self.increment_counter('Process', 'Mapper', 1)\n",
    "            yield (word.lower(), 1)\n",
    "\n",
    "    def combiner_count_words(self, word, counts):\n",
    "        # optimization: sum the words we've seen so far\n",
    "        yield (word, sum(counts))\n",
    "\n",
    "    def reducer_count_words(self, word, counts):\n",
    "        # send all (num_occurrences, word) pairs to the same reducer.\n",
    "        # num_occurrences is so we can easily use Python's max() function.\n",
    "        yield None, (sum(counts), word)\n",
    "\n",
    "    # discard the key; it is just None\n",
    "    def reducer_find_max_word(self, _, word_count_pairs):\n",
    "        # each item of word_count_pairs is (count, word),\n",
    "        # so yielding one results in key=counts, value=word\n",
    "        yield max(word_count_pairs)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    MRMostUsedWord.run()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
