{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": "true"
   },
   "source": [
    "# Table of Contents\n",
    " <p><div class=\"lev1 toc-item\"><a href=\"#-MIDS---w261-Machine-Learning-At-Scale-\" data-toc-modified-id=\"-MIDS---w261-Machine-Learning-At-Scale--1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span> MIDS - w261 Machine Learning At Scale </a></div><div class=\"lev2 toc-item\"><a href=\"#Assignment---HW11\" data-toc-modified-id=\"Assignment---HW11-11\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Assignment - HW11</a></div><div class=\"lev3 toc-item\"><a href=\"#INSTRUCTIONS-for-SUBMISSION\" data-toc-modified-id=\"INSTRUCTIONS-for-SUBMISSION-111\"><span class=\"toc-item-num\">1.1.1&nbsp;&nbsp;</span>INSTRUCTIONS for SUBMISSION</a></div><div class=\"lev3 toc-item\"><a href=\"#USEFUL-REFERENCES\" data-toc-modified-id=\"USEFUL-REFERENCES-112\"><span class=\"toc-item-num\">1.1.2&nbsp;&nbsp;</span>USEFUL REFERENCES</a></div><div class=\"lev3 toc-item\"><a href=\"#CONFIGURATION\" data-toc-modified-id=\"CONFIGURATION-113\"><span class=\"toc-item-num\">1.1.3&nbsp;&nbsp;</span>CONFIGURATION</a></div><div class=\"lev3 toc-item\"><a href=\"#DATASETS\" data-toc-modified-id=\"DATASETS-114\"><span class=\"toc-item-num\">1.1.4&nbsp;&nbsp;</span>DATASETS</a></div><div class=\"lev1 toc-item\"><a href=\"#HW-Problems\" data-toc-modified-id=\"HW-Problems-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>HW Problems</a></div><div class=\"lev2 toc-item\"><a href=\"#HW11.0:-Broadcast-versus-Caching-in-Spark\" data-toc-modified-id=\"HW11.0:-Broadcast-versus-Caching-in-Spark-21\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>HW11.0: Broadcast versus Caching in Spark</a></div><div class=\"lev2 toc-item\"><a href=\"#HW11.1-Loss-Functions\" data-toc-modified-id=\"HW11.1-Loss-Functions-22\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>HW11.1 Loss Functions</a></div><div class=\"lev2 toc-item\"><a href=\"#HW11.2-Gradient-descent\" data-toc-modified-id=\"HW11.2-Gradient-descent-23\"><span class=\"toc-item-num\">2.3&nbsp;&nbsp;</span>HW11.2 Gradient descent</a></div><div class=\"lev2 toc-item\"><a href=\"#HW11.3-Logistic-Regression\" data-toc-modified-id=\"HW11.3-Logistic-Regression-24\"><span class=\"toc-item-num\">2.4&nbsp;&nbsp;</span>HW11.3 Logistic Regression</a></div><div class=\"lev2 toc-item\"><a href=\"#HW11.4-SVMs\" data-toc-modified-id=\"HW11.4-SVMs-25\"><span class=\"toc-item-num\">2.5&nbsp;&nbsp;</span>HW11.4 SVMs</a></div><div class=\"lev2 toc-item\"><a href=\"#HW11.5--[OPTIONAL]-Distributed-Perceptron-algorithm.\" data-toc-modified-id=\"HW11.5--[OPTIONAL]-Distributed-Perceptron-algorithm.-26\"><span class=\"toc-item-num\">2.6&nbsp;&nbsp;</span>HW11.5  [OPTIONAL] Distributed Perceptron algorithm.</a></div><div class=\"lev2 toc-item\"><a href=\"#HW11.6-[OPTIONAL:-consider-doing-this-in-a-group]--Evalution-of-perceptron-algorihtms-on-PennTreeBank-POS-corpus\" data-toc-modified-id=\"HW11.6-[OPTIONAL:-consider-doing-this-in-a-group]--Evalution-of-perceptron-algorihtms-on-PennTreeBank-POS-corpus-27\"><span class=\"toc-item-num\">2.7&nbsp;&nbsp;</span>HW11.6 [OPTIONAL: consider doing this in a group]  Evalution of perceptron algorihtms on PennTreeBank POS corpus</a></div><div class=\"lev2 toc-item\"><a href=\"#HW11.7-[OPTIONAL:-consider-doing-this-in-a-group]-Kernel-Adatron\" data-toc-modified-id=\"HW11.7-[OPTIONAL:-consider-doing-this-in-a-group]-Kernel-Adatron-28\"><span class=\"toc-item-num\">2.8&nbsp;&nbsp;</span>HW11.7 [OPTIONAL: consider doing this in a group] Kernel Adatron</a></div><div class=\"lev2 toc-item\"><a href=\"#HW11.8-[OPTIONAL]-Create-an-animation-of-gradient-descent-for-the-Perceptron-learning-or-for-the-logistic-regression\" data-toc-modified-id=\"HW11.8-[OPTIONAL]-Create-an-animation-of-gradient-descent-for-the-Perceptron-learning-or-for-the-logistic-regression-29\"><span class=\"toc-item-num\">2.9&nbsp;&nbsp;</span>HW11.8 [OPTIONAL] Create an animation of gradient descent for the Perceptron learning or for the logistic regression</a></div><div class=\"lev1 toc-item\"><a href=\"#---------ALTERNATIVE-HOWEWORK---------\" data-toc-modified-id=\"---------ALTERNATIVE-HOWEWORK----------3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>-------  ALTERNATIVE HOWEWORK --------</a></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> MIDS - w261 Machine Learning At Scale </h1>\n",
    "__Course Lead:__ Dr James G. Shanahan (__email__ Jimi via  James.Shanahan _AT_ gmail.com)\n",
    "\n",
    "<h2>Assignment - HW11</h2>\n",
    "\n",
    "---\n",
    "__Name:__  *Your Name Goes Here*   \n",
    "__Class:__ MIDS w261 (Section *Your Section Goes Here*, e.g., Fall 2016 Group 1)     \n",
    "__Email:__  *Your UC Berkeley Email Goes Here*@iSchool.Berkeley.edu     \n",
    "__StudentId__  123457    __End of StudentId__     \n",
    "__Week:__   11\n",
    "__NOTE:__ please replace `1234567` with your student id above      \n",
    "\n",
    "### INSTRUCTIONS for SUBMISSION\n",
    "\n",
    "This homework can be completed locally on your computer. __Please submit your notebook to your classroom github repository 24 hours prior to the next live session.__ \n",
    "\n",
    "\n",
    "### USEFUL REFERENCES\n",
    "\n",
    "* Karau, Holden, Konwinski, Andy, Wendell, Patrick, & Zaharia, Matei. (2015). Learning Spark: Lightning-fast big data analysis. Sebastopol, CA: Oâ€™Reilly Publishers.\n",
    "* Hastie, Trevor, Tibshirani, Robert, & Friedman, Jerome. (2009). The elements of statistical learning: Data mining, inference, and prediction (2nd ed.). Stanford, CA: Springer Science+Business Media. (Download for free [here](http://statweb.stanford.edu/~tibs/ElemStatLearn/printings/ESLII_print10.pdf))\n",
    "* https://www.dropbox.com/s/ngomebw1koujs2d/classificationISLBook-Logistic-Regression-LDA-NaiveBayes.pdf?dl=0\n",
    "\n",
    "\n",
    "### CONFIGURATION\n",
    "Before starting your homework run the following cells to confirm your setup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# general imports\n",
    "import os\n",
    "import re\n",
    "import sys\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# tell matplotlib not to open a new window\n",
    "%matplotlib inline\n",
    "\n",
    "# automatically reload modules \n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__[OPTIONAL]:__ Fix chrome formatting. _The cell below implements a quick hack based on [this stackoverflow thread](http://stackoverflow.com/questions/34277967/chrome-rendering-mathjax-equations-with-a-trailing-vertical-line) to fix [this known issue](https://github.com/mathjax/MathJax/issues/1300) with Mathjax formatting in Chrome (a rounding issue adds a border to the right of mathjax markup)._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "$('.math>span').css(\"border-left-color\",\"transparent\")"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%javascript\n",
    "$('.math>span').css(\"border-left-color\",\"transparent\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DATASETS\n",
    "The only data we'll use in the required portion of this assignment is some fake data that you will generate by running provided code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW Problems\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW11.0: Broadcast versus Caching in Spark \n",
    "\n",
    "\n",
    "a) __What is the difference between broadcasting and caching data in Spark? Give an example (in the context of machine learning) of each mechanism (at a highlevel). Feel free to cut and paste code examples from the lectures to support your answer.__\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Type your answer here!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b) __Review the following Spark-notebook-based implementation of KMeans and use the broadcast pattern to make this implementation more efficient. Please describe your changes in English first, implement, comment your code and highlight your changes (write all your code in this notebook):___\n",
    "\n",
    "* Notebook\n",
    "https://www.dropbox.com/s/41q9lgyqhy8ed5g/EM-Kmeans.ipynb?dl=0\n",
    "\n",
    "* Notebook via NBViewer\n",
    "http://nbviewer.ipython.org/urls/dl.dropbox.com/s/41q9lgyqhy8ed5g/EM-Kmeans.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Type your answer here!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# START STUDENT CODE 11.0\n",
    "# (ADD CELLS AS NEEDED)\n",
    "\n",
    "# END STUDENT CODE 11.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW11.1 Loss Functions\n",
    "\n",
    "a) __In the context of binary classification problems, does the linear SVM learning algorithm yield the same result as a L2 penalized logistic regesssion learning algorithm?__ In your reponse, please discuss the loss functions, and the learnt models, and separating surfaces between the two classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Type your answer here!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b) __In the context of binary classification problems, does the linear SVM learning algorithm yield the same result as a perceptron learning algorithm?__ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Type your answer here!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c) __[OPTIONAL]: Generate an artifical binary classification dataset with 2 input features and plot the learnt separating surface for both a linear SVM and for  logistic regression. Comment on the learnt surfaces.__ Please feel free to do this in Python (no need to use Spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# START STUDENT CODE 11.1.c\n",
    "# (ADD CELLS AS NEEDED)\n",
    "\n",
    "# END STUDENT CODE 11.1.c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW11.2 Gradient descent\n",
    "\n",
    "a) __In the context of logistic regression describe and define three flavors of penalized loss functions.  Are these all supported in Spark MLLib (include online references to support your answers)__?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Type your answer here!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b) __Describe probabilistic interpretations of the L1 and L2 priors for penalized logistic regression (HINT: see synchronous slides for week 11 for details)__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Type your answer here!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW11.3 Logistic Regression\n",
    "\n",
    "a) __Generate 2 sets of linearly separable data with 100 data points each using the data generation code provided below and plot each in separate plots. Call one the training set and the other the testing set.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# part a - provided code\n",
    "def generateData(n):\n",
    "    \"\"\" \n",
    "    Generates a 2D linearly separable dataset with n samples. \n",
    "    The third element of the sample is the label.\n",
    "    The fourth element is an integer {0,1,2} representing the\n",
    "    source of the hypothetical data point (See part d)\n",
    "    \"\"\"\n",
    "    xb = (np.random.randn(n)*2-1)/2-0.5\n",
    "    yb = (np.random.randn(n)*2-1)/2+0.5\n",
    "    xr = (np.random.randn(n)*2-1)/2+0.5\n",
    "    yr = (np.random.randn(n)*2-1)/2-0.5\n",
    "    source = np.random.choice(3,n)\n",
    "    inputs = []\n",
    "    for i in range(len(xb)):\n",
    "        inputs.append([xb[i], yb[i], 1, source[i]])\n",
    "        inputs.append([xr[i], yr[i], -1, source[i]])\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# START STUDENT CODE 11.3.a\n",
    "# (ADD CELLS AS NEEDED)\n",
    "\n",
    "# END STUDENT CODE 11.3.a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b) __Write your own data generation function (based on the one provided in part a) to generating non-linearly separable training and testing datasets (with approximately 10% of the data falling on the wrong side of the separating hyperplane. Plot the resulting datasets.__ NOTE: For the remainder of this problem please use the non-linearly separable training and testing datasets. Make sure that your simulated data have a 4th field representing the 'source' as does the example code -- we'll use this for weighted regression in part d similar to what we did in HW6."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# START STUDENT CODE 11.3.b\n",
    "# (ADD CELLS AS NEEDED)\n",
    "\n",
    "# END STUDENT CODE 11.3.b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c) __Using MLLib  train up a LASSO logistic regression model with the training dataset and evaluate with the testing set. What a good number of iterations for training the logistic regression model? Justify with plots and words.__ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# START STUDENT CODE 11.3.c\n",
    "# (ADD CELLS AS NEEDED)\n",
    "\n",
    "# END STUDENT CODE 11.3.c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__ HW11.3 Part C Discussion:__\n",
    "> Type your response here!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "d) __Derive and implement in Spark a weighted LASSO logistic regression using the weighting scheme from HW6. Implement a convergence test of your choice to check for termination within your training algorithm.__ \n",
    "* Evaluate your homegrown weighted  LASSO logistic regression on the test dataset. \n",
    "* Report misclassification error (1 - Accuracy) and how many iterations does it took to converge. \n",
    "* Does Spark MLLib have a weighted LASSO logistic regression implementation? If so use it and report your findings on the weighted training set and test set.\n",
    "\n",
    "__Weighting Scheme Reminder:__ The `source` field contains a $0$, $1$ or $2$ for each data point. For this demo, lets suppose that our data come from three sources which we trust to varying degrees (2 = most reliable, 0 = least reliable). We'll weight our linear regression to pay more attention to the more reliable data points. Use the following weighting scheme:\n",
    "* if source = 0, weight: 1\n",
    "* if source = 1, weight: 2\n",
    "* if source = 2, weight: 3  \n",
    "\n",
    "Effectively, we're saying that data points from the most reliable source are three times as important as data points from the least reliable source."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# START STUDENT CODE 11.3.d\n",
    "# (ADD CELLS AS NEEDED)\n",
    "\n",
    "# END STUDENT CODE 11.3.d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__ HW11.3 Part D Discussion:__\n",
    "> Type your response here!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW11.4 SVMs\n",
    "\n",
    "Use the non-linearly separable training and testing datasets from HW11.3 in this problem.\n",
    "\n",
    "a) __Using MLLib  train up a soft SVM model with the training dataset and evaluate with the testing set. What is a good number of iterations for training the SVM model? Justify with plots and words.__ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# START STUDENT CODE 11.4.a\n",
    "# (ADD CELLS AS NEEDED)\n",
    "\n",
    "# END STUDENT CODE 11.4.a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__ HW11.4 Part A Discussion:__\n",
    "> Type your response here!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b) __[Optional] Derive and Implement in Spark a weighted hard linear svm classification learning algorithm.__ \n",
    "* Feel free to use the following notebook as a starting point:  [SVM Notebook](http://nbviewer.jupyter.org/urls/dl.dropbox.com/s/dm2l73iznde7y4f/SVM-Notebook-Linear-Kernel-2015-06-19.ipynb).\n",
    "\n",
    "* Evaluate your homegrown weighted  linear svm classification learning algorithm on the  weighted training dataset and test dataset from HW11.3 (linearly separable dataset). Report misclassification error (1 - Accuracy) and how many iterations does it took to converge?  How many support vectors do you end up with?\n",
    "\n",
    "* Does Spark MLLib have a weighted soft SVM learner? If so use it and report your findings on the weighted training set and test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__ HW11.4 Part B Discussion:__\n",
    "> Type your response here!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# START STUDENT CODE 11.4.b\n",
    "# (ADD CELLS AS NEEDED)\n",
    "\n",
    "# END STUDENT CODE 11.4.b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c) __[Optional]Repeat HW11.4.b using a soft SVM and a nonlinearly separable datasets. Compare the error rates that you get here with the error rates you achieve using MLLib's soft SVM. Report the number of support vectors in both cases (may not be available the MLLib implementation).__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# START STUDENT CODE 11.4.c\n",
    "# (ADD CELLS AS NEEDED)\n",
    "\n",
    "# END STUDENT CODE 11.4.c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__ HW11.4 Part C Discussion:__\n",
    "> Type your response here!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW11.5  [OPTIONAL] Distributed Perceptron algorithm.\n",
    "[Back to Table of Contents](#TOC)\n",
    "\n",
    "Using the following papers as background:\n",
    "http://static.googleusercontent.com/external_content/untrusted_dlcp/research.google.com/en//pubs/archive/36266.pdf\n",
    "\n",
    "https://www.dropbox.com/s/a5pdcp0r8ptudgj/gesmundo-tomeh-eacl-2012.pdf?dl=0\n",
    "\n",
    "http://www.slideshare.net/matsubaray/distributed-perceptron \n",
    "\n",
    "Implement each of the following flavors of perceptron learning algorithm:\n",
    "\n",
    "1. Serial (All Data): This is the classifier returned if trained serially on all the available data.  On a single computer for example (Mistake driven)\n",
    "2. Serial (Sub Sampling): Shard the data, select one shard randomly and train serially. \n",
    "3. Parallel (Parameter Mix): Learn a perceptron locally on each shard: \n",
    "Once learning is complete combine each learnt percepton using a uniform weighting\n",
    "4. Parallel (Iterative Parameter Mix) as described in the above papers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# START STUDENT CODE 11.5\n",
    "# (ADD CELLS AS NEEDED)\n",
    "\n",
    "# END STUDENT CODE 11.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW11.6 [OPTIONAL: consider doing this in a group]  Evalution of perceptron algorihtms on PennTreeBank POS corpus\n",
    "[Back to Table of Contents](#TOC)\n",
    "\n",
    "Reproduce the experiments reported in the following paper:\n",
    "\n",
    "*Prediction with MapReduce - Andrea Gesmundo and  Nadi Tomeh*\n",
    "\n",
    "http://www.aclweb.org/anthology/E12-2020 \n",
    "\n",
    "These experiments focus on the prediction accuracy on a part-of-speech\n",
    "(POS) task using the PennTreeBank corpus. They use sections 0-18 of the Wall\n",
    "Street Journal for training, and sections 22-24 for testing.\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# START STUDENT CODE 11.6\n",
    "# (ADD CELLS AS NEEDED)\n",
    "\n",
    "# END STUDENT CODE 11.6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW11.7 [OPTIONAL: consider doing this in a group] Kernel Adatron \n",
    "\n",
    "Implement the Kernal Adatron in Spark (contact Jimi for details)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# START STUDENT CODE 11.7\n",
    "# (ADD CELLS AS NEEDED)\n",
    "\n",
    "# END STUDENT CODE 11.7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW11.8 [OPTIONAL] Create an animation of gradient descent for the Perceptron learning or for the logistic regression \n",
    "    \n",
    "Learning with the following 3 training examples. Present the progress in terms of the 2 dimensional input space in terms of a contour plot and also in terms of the 3D surface plot. See Live slides for an example.\n",
    "\n",
    "Here is a sample training dataset that can be used:\n",
    "```\n",
    "-2, 3, +1\n",
    "-1, -1, -1\n",
    "2, -3, 1\n",
    "```\n",
    "Please feel free to use \n",
    " + R (yes R!)\n",
    " + d3\n",
    " + https://plot.ly/python/\n",
    " + Matplotlib\n",
    "\n",
    "I am happy for folks to collaborate on HW11.8 also.\n",
    "\n",
    "It would be great to get the 3D surface and contours lines (with solution region and label normalized data) all in the same graph\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# START STUDENT CODE 11.8\n",
    "# (ADD CELLS AS NEEDED)\n",
    "\n",
    "# END STUDENT CODE 11.8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# -------  ALTERNATIVE HOWEWORK --------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Implement a scaleable softmax classifier (aka multinomial logistic regression) in Spark (regularized and non-regularized)\n",
    "- Run experiments on MNIST dataset and for CIFAR dataset\n",
    "- And compare to MLLib implementations (accuracy, CPU times)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "417px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": false,
   "sideBar": false,
   "threshold": 4,
   "toc_cell": true,
   "toc_section_display": "block",
   "toc_window_display": true,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
